v3.7.12 (XXXX-XX-XX)
--------------------

* Fixed DEVSUP-764 (SEARCH-7): inconsistent BM25 scoring for LEVENSHTEIN_MATCH
  function.

* Added 2 options to allow HTTP redirection customization for root ("/") call of
  HTTP API:

  `--http.permanently-redirect-root`: if true (default), use a permanent
  redirection (use HTTP 301 code), if false fall back to temporary redirection
  (use HTTP 302 code);
  `--http.redirect-root-to`: redirect of root URL to a specified path (redirects
  to "/_admin/aardvark/index.html" if not set (default)).

* Fixes BTS-417. In some cases an index did not consider both bounds (lower and
  upper) for a close range scan if both bounds are expressed using the same
  operator, e.g., `FILTER doc.beginDate >= lb AND ub >= doc.beginDate`.

* Fixes BTS-416. During shutdown, a Shard leader wrongly reported that it could
  not drop a shard follower instead of correctly indicating the shutdown as the
  reason.

* Fixed issue BTS-354: Assertion related to getCollection.

* Fixed BTS-403: Hot restores must also clear relevant `Current` keys. The
  overriding of the `Plan` entries needs to be reflected in `Current` to avoid
  conflicts in maintenance jobs.

* Fixed issue #14122: when the optimizer rule "inline-subqueries" is applied, it
  may rename some variables in the query. The variable renaming was however not
  carried out for traversal PRUNE conditions, so the PRUNE conditions could
  still refer to obsolete variables, which would make the query fail with errors
  such as

    Query: AQL: missing variable ... for node ... while planning registers

* Fixed a use after free bug in the connection pool.

* Fix DEVSUP-749: Fix potential deadlock when executing concurrent view/link DDL
  operations and index DDL operations on the same collection.

* Fixed bug in error reporting when a database create did not work, which lead
  to a busy loop reporting this error to the agency.

* Fixed the error response if the HTTP version is not 1.0 or 1.1 and if the
  Content-Length is too large (> 1 GB).


v3.7.11 (2021-04-25)
--------------------

* Fixed proper return value in sendRequestRetry if server is shutting down.

* Updated ArangoDB Starter to 0.15.0.

* Updated arangosync to 1.4.0.

* Guarded access only to ActionBase::_result.

* Fixed internal issue #798: In  rare case when remove request completely cleans
  just consolidated segment commit could be cancelled and documents removed from
  collection may be left dangling in the ArangoSearch index.
  Also fixes ES-810 and BTS-279.

* Retry if an ex-leader can no longer drop a follower because it is no longer
  leading.

* Make the time-to-live (TTL) value of a streaming cursor only count after the
  response has been sent to the client.

* Fix BTS-340: AQL expressions similar to `x < 3 || x` are no longer erroneously
  be reduced to `x < 3` by the optimizer rule remove-redundant-or.

* Fix BTS-374: thread race between ArangoSearch link unloading and storage
  engine WAL flushing.

* Fix crashes during arangorestore operations due to usage of wrong pointer
  value for updating user permissions.

* Fixed BTS-360 and ES-826: sporadic ArangoSearch error `Invalid RL encoding in
  'dense_fixed_offset_column_key'`.

* Add HTTP REST API endpoint POST `/_api/cursor/<cursor-id>` as a drop-in
  replacement for PUT `/_api/cursor/<cursor-id>`. The POST API is functionally
  equivalent to the existing PUT API. The benefit of using the POST API is that
  HTTP POST requests will not be considered as idempotent, so proxies may not
  retry them if they fail. This was the case with the existing PUT API, as HTTP
  PUT requests can be considered idempotent according to the HTTP specification.

  The POST API is not used internally by ArangoDB's own requests in this version.
  This means that compatibility to older versions of ArangoDB that do not
  provide the new API is ensured.

* Added option `--query.require-with` to make AQL in single server mode also
  require `WITH` clauses where the cluster would need them.
  The option is turned off by default, but can be turned on in single servers to
  remove this behavior difference between single servers and clusters, making
  later a transition from single server to cluster easier.

* Timely updates of rebootId / cluster membership of DB servers and coordinators
  in ClusterInfo. Fixes BTS-368 detected in chaos tests.

* Fix BTS-350, BTS-358: Fixed potential startup errors due to global replication
  applier being started before end of database recovery procedure.
  Also fixed potential shutdown errors due to global replication applier being
  shut down in parallel to a concurrent shut down attempt.

* Web UI - Added missing HTML escaping inside the file upload plugin used in the
  section of deploying a new Foxx application when uploading a zip file.

* Fix BTS-352: removed assertion for success of a RocksDB function and throw a
  proper exception instead.

* Allow to specify a fail-over LDAP server. Instead of "--ldap.OPTION" you need
  to specify "--ldap2.OPTION". Authentication / Authorization will first check
  the primary LDAP server. If this server cannot authenticate a user, it will
  try the secondary one. It is possible to specify a file containing all users
  that the primary (or secondary) LDAP server is handling by specifying the
  option "--ldap.responsible-for". This file must contain the usernames
  line-by-line.

* Fixed a problem in document batch operations, where errors from one shard were
  reported multiple times, if the shard is completely off line.

* Fix BTS-346: Improved handling of AQL query kill command in unlikely places,
  before the query starts to execute and after the query is done but the result
  is still being written. Now the cleanup of queries works more reliably. This
  unreliable kill time windows were very short and unlikely to hit, although if
  one was hit transactions were not aborted, and collection locks could be
  lingering until query timeout.

* Fixed issue #13169: arangoimport tsv conversion of bools and null, although
  switched off by `--convert false`.

  Importing unquoted `null`, `false` and `true` literals from delimited files
  get imported as strings now if `convert` is explicitly turned off. It
  previously affected unquoted numbers only.

* Added error handling for figures command in cluster. Previously errors
  returned by shards were ignored when aggregating the individual responses.

* Fixed issue BTS-353: memleak when running into an out-of-memory situation
  while repurposing an existing AqlItemBlock.

* Change metrics' internal `low()` and `high()` methods so that they  return by
  value, not by reference.

* Fix logging of urls when using `--log.level requests=debug`. There was an
  issue since v3.7.7 with the wrong URL being logged in request logging if
  multiple requests were sent over the same connection. In this case, the
  request logging only reported the first URL requested in the connection, even
  for all subsequent requests.

* Fix velocypack errors in cursor API in case an exception occurs while the
  cursor is being filled with data. In this case, the result could be in an
  arbitrary state (e.g. halfway produced), so adding more attributes to the
  result such as "error" and "code" was potentially unsafe. If the result was
  not properly built, trying to add "error" and "code" attributes could have led
  to arbitrary velocypack errors, which were then reported back to callers,
  masking the original error.
  Now the original error is returned, and no attempt is made to write into the
  result after an exception has occurred.

* Updated OpenSSL to 1.1.1k and OpenLDAP to 2.4.58.

* When using connections to multiple endpoints and switching between them,
  arangosh can now reuse existing connections by referring to an internal
  connection cache. This helps for arangosh scripts that repeatedly connect to
  multiple endpoints, and avoids wasting lots of ephemeral TCP ports remaining
  in CLOSE_WAIT state.
  This change is transparent to any arangosh scripts or commands that do not
  reconnect to other endpoints than the one specified at arangosh start.

* Fix a crash caused by returning a result produced by ANALYZER function.

* Fix implicit capture of views in a context of JS transaction.

* Use RebootTracker to abort cluster transactions on DB servers should the
  originating coordinator die or be rebooted. The previous implementation left
  the coordinator's transactions open on DB servers until they timed out there.
  Now, the coordinator's unavailability or reboot will be detected as early as
  it is reported by the agency, and all open transactions from that coordinator
  will be auto-aborted on DB servers.

* Fix an assertion failure that occurred when restoring view definitions from a
  cluster into a single server.

* Fixed Github issue #13665: Improve index selection when there are multiple
  candidate indexes.

* Allow process-specific logfile names.

  This change allows replacing '$PID' with the current process id in the
  `--log.output` and `--audit.output` startup parameters.
  This way it is easier to write process-specific logfiles.

* Backport a bugfix from upstream RocksDB for opening encrypted files with small
  sizes. Without the bugfix, the server may run into assertion failures during
  recovery.

* When dropping a collection or an index with a larger amount of documents, the
  key range for the collection/index in RocksDB gets compacted. Previously, the
  compaction was running in foreground and thus would block the deletion
  operations.
  Now, the compaction is running in background, so that the deletion operations
  can return earlier.
  The maximum number of compaction jobs that are executed in background can be
  configured using the new startup parameter
  `--rocksdb.max-parallel-compactions`, which defaults to 2.

* Fix slower-than-necessary arangoimport behavior:
  arangoimport has a built-in rate limiter, which can be useful for importing
  data with a somewhat constant rate. However, it is enabled by default and
  limits imports to 1MB per second. These settings are not useful.

  This change turns the rate limiting off by default, and sets the default chunk
  size to 8MB (up from 1MB) as well. This means that arangoimport will send
  larger batches to the server by default. The already existing `--batch-size`
  option can be used to control the maximum size of each batch.

  The new parameter `--auto-rate-limit` can now be used to toggle rate limiting.
  It defaults to off, whereas previously rate limiting was enabled by default
  unless `--batch-size` was specified when arangoimport was invoked.

* Fixed a bug in query cost estimation when a NoResults node occured in a
  spliced subquery. This could lead to a server crash.

* Fixed a bug in the index count optimization that doubled counted documents
  when using array expansions in the fields definition.


v3.7.10 (2021-03-14)
--------------------

* Put Sync/LatestID into hotbackup and restore it on hotbackup restore
  if it is in the backup. This helps with unique key generation after
  a hotbackup is restored to a young cluster.

* Reasonably harden MoveShard against unexpected VelocyPack input.

* Follower DB servers will now respond with error code
  `TRI_ERROR_CLUSTER_SHARD_FOLLOWER_REFUSES_OPERATION`
  to any read request. This fixes inadequate HTTP 404 responses from followers,
  e.g. during chaos tests.
  
* Fixed Github issue #13632: Query Fails on Upsert with Replace_nth.

* Updated arangosync to 1.2.3.

* Backported AQL sort performance improvements from devel.
  This change can improve the performance of local sorts operations, e.g.

  Baseline (3.7.9):

      Query String (94 chars, cacheable: false):
       FOR i IN 1..500000 LET value = CONCAT('testvalue-to-be-sorted', i) SORT value ASC RETURN value

      Execution plan:
       Id   NodeType            Calls    Items   Runtime [s]   Comment
        1   SingletonNode           1        1       0.00003   * ROOT
        2   CalculationNode         1        1       0.00003     - LET #2 = 1 .. 500000   /* range */   /* simple expression */
        3   EnumerateListNode     500   500000       0.08725     - FOR i IN #2   /* list iteration */
        4   CalculationNode       500   500000       0.22722       - LET value = CONCAT("testvalue-to-be-sorted", i)   /* simple expression */
        5   SortNode              500   500000       2.05180       - SORT value ASC   /* sorting strategy: standard */
        6   ReturnNode            500   500000       0.02911       - RETURN value

      Query Statistics:
       Writes Exec   Writes Ign   Scan Full   Scan Index   Filtered   Exec Time [s]
                 0            0           0            0          0         2.39644

  With sort optimization (3.7.10):

      Query String (94 chars, cacheable: false):
       FOR i IN 1..500000 LET value = CONCAT('testvalue-to-be-sorted', i) SORT value ASC RETURN value

      Execution plan:
       Id   NodeType            Calls    Items   Runtime [s]   Comment
        1   SingletonNode           1        1       0.00002   * ROOT
        2   CalculationNode         1        1       0.00003     - LET #2 = 1 .. 500000   /* range */   /* simple expression */
        3   EnumerateListNode     500   500000       0.08755     - FOR i IN #2   /* list iteration */
        4   CalculationNode       500   500000       0.26161       - LET value = CONCAT("testvalue-to-be-sorted", i)   /* simple expression */
        5   SortNode              500   500000       1.36070       - SORT value ASC   /* sorting strategy: standard */
        6   ReturnNode            500   500000       0.02864       - RETURN value

      Query Statistics:
       Writes Exec   Writes Ign   Scan Full   Scan Index   Filtered   Exec Time [s]
                 0            0           0            0          0         1.73940
* Fixed a problem that coordinators would vanish from the UI and the Health API
  if one switched the agency Supervision into maintenance mode and kept left
  that maintenance mode on for more than 24h.

* Fixed a bug in the web interface that displayed the error "Not authorized to
  execute this request" when trying to create an index in the web interface in a
  database other than `_system` with a user that does not have any access
  permissions for the `_system` database.
  The error message previously displayed error actually came from an internal
  request made by the web interface, but it did not affect the actual index
  creation.

* Fixed issue BTS-309: The Graph API (Gharial) did not respond with the correct
  HTTP status code when validating edges. It now responds with 400 (Bad Request)
  as documented and a new, more precise error code (1947) and message if a
  vertex collection referenced in the _from or _to attribute is not part of the
  graph.


v3.7.9 (2021-03-01)
-------------------

* Fix issue #13476: The Java driver v6.9.0 (and older) has bad performance when
  iterating over AQL cursor results in certain cases. This works around this.
  This workaround will no longer be available in 3.8.

* Enable statistics in web UI in non-`_system` databases in cluster mode.
  In cluster mode, the web UI dashboard did not display statistics properly when
  not being logged in to the `_system` database. For all other databases than
  `_system`, no statistics were displayed but just some "No data..."
  placeholders.
  Statistics for non-`_system` databases were not properly displayed since
  3.7.6 due to an internal change in the statistics processing.

  In addition, a new startup option `--server.statistics-all-databases` controls
  whether cluster statistics are displayed in the web interface for all
  databases (if the option is set to `true`) or just for the system database (if
  the option is set to `false`).
  The default value for the option is `true`, meaning statistics will be
  displayed in the web interface for all databases.

* Updated OpenSSL to 1.1.1j and OpenLDAP to 2.4.57.

* Cleanup old HotBackup transfer jobs in agency.

* Added logging of elapsed time of ArangoSearch commit/consolidation/cleanup
  jobs.

* Fix too early stop of replication, when waiting for keys in large
  collections/shards.

* Fixed issue BTS-268: fix a flaky Foxx self-heal procedure.

* Fixed issue DEVSUP-720: Within an AQL query, the "COLLECT WITH COUNT INTO"
  statement could lead to a wrong count output when used in combination with an
	index which has been created with an array index attribute.

* Fix profiling of AQL queries with the `silent` and `stream` options sets in
  combination. Using the `silent` option makes a query execute, but discard all
  its results instantly. This led to some confusion in streaming queries, which
  can return the first query results once they are available, but don't
  necessarily execute the full query.
  Now, `silent` correctly discards all results even in streaming queries, but
  this has the effect that a streaming query will likely be executed completely
  when the `silent` option is set. This is not the default however, and the
  `silent` option is normally not set. There is no change for streaming queries
  if the `silent` option is not set.

  As a side-effect of this change, this makes profiling (i.e. using
  `db._profileQuery(...)` work for streaming queries as well. Previously,
  profiling a streaming query could have led to some internal errors, and even
  query results being returned, even though profiling a query should not return
  any query results.

* Improved the wording for sharding options displayed in the web interface.

  Instead of offering `flexible` and `single`, now use the more intuitive
  `Sharded` and `OneShard` options, and update the help text for them.

* EE only bugfix: On DisjointSmartGraphs that are used in anonymous way, there
  was a chance that the query could fail, if non-disjoint collections were part
  of the query. Named DisjointSmartGraphs have been save to this bug.
  Example:
  DisjointSmartGraph (graph) on vertices -edges-> vertices
  Query:

  WITH vertices, unrelated
  FOR out IN 1 OUTBOUND "v/1:1" edges
    FOR u IN unrelated
    RETURN [out, u]
  
  The "unrelated" collection was pulled into the DisjointSmartGraph, causing the
  AQL setup to create erroneous state.
  This is now fixed and the above query works.
  This query:

  WITH vertices, unrelated
  FOR out IN 1 OUTBOUND "v/1:1" GRAPH "graph"
    FOR u IN unrelated
    RETURN [out, u]

  was not affected by this bug.

* Avoid a potential deadlock when dropping indexes.

  A deadlock could theoretically happen for a thread that is attempting to drop
  an index in case there was another thread that tried to create or drop an
  index in the very same collection at the very same time. We haven't managed to
  trigger the deadlock with concurrency tests, so it may have been a theoretical
  issue only. The underlying code was changed anyway to make sure this will not
  cause problems in reality.

* Make dropping of indexes in cluster retry in case of precondition failed.

  When dropping an indexes of a collection in the cluster, the operation could
  fail with a "precondition failed" error in case there were simultaneous index
  creation or drop actions running for the same collection. The error was
  returned properly internally, but got lost at the point when
  `<collection>.dropIndex()` simply converted any error to just `false`.
  We can't make `dropIndex()` throw an exception for any error, because that
  would affect downwards-compatibility. But in case there is a simultaneous
  change to the collection indexes, we can just retry our own operation and
  check if it succeeds then. This is what `dropIndex()` will do now.

* Improve incremental sync replication for single server and cluster to cope
  with multiple secondary index unique constraint violations (before this was
  limited to a failure in a single unique secondary index). This allows
  replicating the leader state to the follower in basically any order, as any
  *other* conflicting documents in unique secondary indexes will be  detected
  and removed on the follower.

* Fix potential undefined behavior when iterating over connected nodes in an
  execution plan and calling callbacks for each of the nodes: if the callbacks
  modified the list of connected nodes of the current that they were called
  from, this could lead to potentially undefined behavior due to iterator
  invalidation. The issue occurred when using a debug STL via `_GLIBCXX_DEBUG`.

* Fixed a RocksDB bug which could lead to an assertion failure when compiling
  with STL debug mode -D_GLIBCXX_DEBUG.

* Fixed a rare internal buffer overflow around ridBuffers.

* Issue #13141: The `move-filters-into-enumerate` optimization, when applied to
  an EnumerateCollectionNode (i.e. full collection scan), did not do regular
  checks for the query being killed during the filtering of documents, resulting
  in the maxRuntime option and manual kill of a query not working timely.

* Do not create index estimator objects for proxy collection objects on
  coordinators and DB servers. Proxy objects are created on coordinators and DB
  servers for all shards, and they also make index objects available. In order
  to reduce the memory usage by these objects, we don't create any index
  estimator objects for indexes in those proxy objects. Index estimators usually
  take several KB of memory each, so not creating them will pay out for higher
  numbers of collections/shards.

* Improvements for logging. This adds the following startup options to arangod:

  - `--log.max-entry-length`: controls the maximum line length for individual
    log messages that are written into normal logfiles by arangod (note: this
    does not include audit log messages).
    Any log messages longer than the specified value will be truncated and the
    suffix '...' will be added to them. The purpose of this parameter is to
    shorten long log messages in case there is not a lot of space for logfiles,
    and to keep rogue log messages from overusing resources.
    The default value is 128 MB, which is very high and should effectively mean
    downwards-compatiblity with previous arangod versions, which did not
    restrict the maximum size of log messages.
  
  - `--audit.max-entry-length`: controls the maximum line length for individual
    audit log messages that are written into audit logs by arangod. Any audit
    log messages longer than the specified value will be truncated and the
    suffix '...' will be added to them.
    The default value is 128 MB, which is very high and should effectively mean
    downwards-compatiblity with previous arangod versions, which did not
    restrict the maximum size of log messages.

  - `--log.in-memory-level`: controls which log messages are preserved in
    memory. The default value is `info`, meaning all log messages of types
    `info`, `warning`, `error` and `fatal` will be stored by an instance in
    memory (this was also the behavior in previous versions of ArangoDB).
    By setting this option to `warning`, only `warning` log messages will be
    preserved in memory, and by setting the option to `error` only error
    messages will be kept.
    This option is useful because the number of in-memory log messages is
    limited to the latest 2048 messages, and these slots are by default shared
    between informational, warning and error messages.

* Honor the value of startup option `--log.api-enabled` when set to `false`.
  The desired behavior in this case is to turn off the REST API for logging, but
  was not implemented. The default value for the option is `true`, so the REST
  API is enabled. This behavior did not change, and neither did the behavior
  when setting the option to a value of `jwt` (meaning the REST API for logging
  is only available for superusers with a valid JWT token).

* Fix error reporting in the reloadTLS route.

* Split the update operations for the _fishbowl system collection with Foxx apps
  into separate insert/replace and remove operations. This makes the overall
  update not atomic, but as removes are unlikely here, we can now get away with
  a simple multi-document insert-replace operation instead of a truncate and an
  exclusive transaction, which was used before.


v3.7.8 (2021-02-16)
-------------------

* Fixed ES-784 regression related to encryption cipher propagation to
  ArangoSearch data.


v3.7.7 (2021-02-05)
-------------------

* Added metrics for document read and write operations:

  - `arangodb_document_writes: Total number of document write operations
    (successful and failed) not performed by synchronous replication.
  - `arangodb_document_writes_replication`: Total number of document write
    operations (successful and failed) by cluster synchronous replication.
  - `arangodb_collection_truncates`: Total number of collection truncate
    operations (successful and failed) not performed by cluster synchronous
    replication.
  - `arangodb_collection_truncates_replication`: Total number of collection
    truncate operations (successful and failed) by synchronous replication.
  - `arangodb_document_read_time`: Execution time histogram of all document
    primary key read operations (successful and failed) [s]. Note: this does not
    include secondary index lookups, range scans and full collection scans.
  - `arangodb_document_insert_time`: Execution time histogram of all document
    insert operations (successful and failed) [s].
  - `arangodb_document_replace_time`: Execution time histogram of all document
    replace operations (successful and failed) [s].
  - `arangodb_document_remove_time`: Execution time histogram of all document
    remove operations (successful and failed) [s].
  - `arangodb_document_update_time`: Execution time histogram of all document
    update operations (successful and failed) [s].
  - `arangodb_collection_truncate_time`: Execution time histogram of all
    collection truncate operations (successful and failed) [s].

  The timer metrics are turned off by default, and can be enabled by setting the
  startup option `--server.export-read-write-metrics true`.

* Fixed issue #12543: Unused Foxx service config can not be discarded.

* Fixed issue #12363: Foxx HTTP API upgrade/replace always enables development
  mode.

* Fixed BTS-284: upgrading from 3.6 to 3.7 in cluster enviroment.
  Moved upgrade ArangoSearch links task to later step as it needs cluster
  connection. Removed misleading error log records for failed ArangoSearch index
  creation during upgrade phase.

* Normalize user-provided input/output directory names in arangoimport,
  arangoexport, arangodump and arangorestore before splitting them into path
  components, in the sense that now both forward and backward slashes can be
  used on Windows, even interchangingly.

* Fixed some wrong behaviour in single document updates. If the option
  ignoreRevs=false was given and the precondition _rev was given in the body but
  the _key was given in the URL path, then the rev was wrongly taken as 0,
  rather than using the one from the document body.

* Allow {USER} paceholder string also in `--ldap.search-filter`.

* Make `padded` and `autoincrement` key generators export their `lastValue`
  values, so that they are available in dumps and can be restored elsewhere from
  a dump.

* Fix decoding of values in `padded` key generator when restoring from a dump.

* Fixed error reporting for hotbackup restore from dbservers back to
  coordinators. This could for example swallow out of disk errors during
  hotbackup restore.

* Fix decoding of values in `padded` key generator when restoring from a dump.

* Fixed some situations of
  [...]
  SUBQUERY
  FILTER
  LIMIT
  [...]
  in AQL queries, yielding incorrect responses. A distributed state within the
  subquery was not resetted correctly. This could also lead into "shrink" errors
  of AQL item blocks, or much higher query runtimes.
  Fixes:
  - BTS-252
  - ES-687
  - github issue: #13099
  - github issue: #13124
  - github issue: #13147
  - github issue: #13305
  - DEVSUP-665

* Fix a bug in the agency Supervision which could lead to removeFollower
  jobs constantly being created and immediately stopped again.

* Limit additional replicas in failover cases to +2.

* Prepare register planning for rolling upgrades. Previously, changes in
  register planning from 3.7 to a minor future version (i.e. 3.8) could cause
  queries executed by a 3.7 coordinator in combination with a minor future
  version (i.e. 3.8) DBServer to fail during a rolling upgrade.

* Fixed rare objectId conflict for indexes.

* Fix for OASIS-409: fixed indexing _id attribute at recovery.

* Fix some issues with key generators not properly taking into account the
  `allowUserKeys` attribute when in a cluster.

* Added the following bit handling functions to AQL:

  - BIT_AND(array): and-combined result
  - BIT_OR(array): or-combined result
  - BIT_XOR(array): xor-combined result
  - BIT_NEGATE(value, bits): bitwise negation of `value`, with a mask of `bits`
    length
  - BIT_TEST(value, index): test if bit at position `index` is set in `value`
    (indexes are 0-based)
  - BIT_POPCOUNT(value): return number of bits set in `value`
  - BIT_SHIFT_LEFT(value, shift, bits): bitwise shift-left of `value` by `shift`
    bits, with a mask of `bits` length
  - BIT_SHIFT_RIGHT(value, shift, bits): bitwise shift-right of `value` by
    `shift` bits, with a mask of `bits` length
  - BIT_CONSTRUCT(array): construct a number with bits set at the positions
    given in the array
  - BIT_DECONSTRUCT(value): deconstruct a number into an array of its individual
    set bits
  - BIT_TO_STRING(value): create a bitstring representation from numeric `value`
  - BIT_FROM_STRING(value): parse a bitstring representation into a number

  `BIT_AND`, `BIT_OR` and `BIT_XOR` are also available as aggregate functions
  for usage inside COLLECT AGGREGATE.

  All above bit operations support unsigned integer values with up to 32 bits.
  Using values outside the supported range will make any of these bit functions
  return `null` and register a warning.

* Add binary (base 2) and hexadecimal (base 16) integer literals to AQL.
  These literals can be used where regular (base 10) integer literal can used.
  The prefix for binary integer literals is `0b`, e.g. `0b10101110`.
  The prefix for hexadecimal integer literals i `0x`, e.g. `0xabcdef02`.

  Binary and hexadecimal integer literals can only be used for unsigned
  integers.
  The maximum supported value is `(2 ^ 32) - 1`, i.e. `0xffffffff` (hexadecimal)
  or `0b11111111111111111111111111111111` (binary).

* Print a version mismatch (major/minor version difference) between the arangosh
  version and the remote arangod version at arangosh startup.

* Fix a potential shutdown deadlock in AgencyCache.

* Updated arangosync to 1.2.2.

* Minor and rare AQL performance improvement, in nested subqueries:
  LET sq1 ([..] FILTER false == true LET sq2 = (<X>) [..])
  where sq1 produces no data (e.g. by the above filter) for sq2, the part <X>
  have been asked two times (second returns empty result), instead of one, if
  and only if the mainquery executes sq1 exactly one time.
  Now we get away with one call only.
  In the case sq1 has data, or sq1 is executed more often, only one call was
  needed (assuming the data fits in one batch).

* Improve internal error reporting by cluster maintenance.

* Bug-Fix: In one-shard-database setups that were created in 3.6.* and then
  upgraded to 3.7.5 the DOCUMENT method in AQL will now return documents again.


v3.7.6 (2021-01-04)
-------------------

* Updated OpenSSL to 1.1.1i and OpenLDAP to 2.4.56.

* Added new metric: "arangodb_collection_lock_sequential_mode" this will count
  how many times we need to do a sequential locking of collections. If this
  metric increases this indicates lock contention in transaction setup.
  Most likely this is caused by exlcusive locks used on collections with more
  than one shard.

* Fix for BTS-213
  Changed the transaction locking mechanism in the cluster case.
  For all installations that do not use "exclusive" collection locks this change
  will not be noticable. In case of "exclusive" locks, and collections with more
  than one shard, it is now less likely to get a LOCK_TIMEOUT (ErrorNum 18).
  It is still possible to get into the LOCK_TIMEOUT case, especially if the
  "exclusive" operation(s) are long-running.

* Fixed an endless busy loop which could happen if a coordinator tries to roll
  back a database creation, but the database has already been dropped by other
  means.

* Make internal ClusterInfo::getPlan() wait for initial plan load from agency.

* Remove HTTP "Connection" header when forwarding requests in the cluster from
  one coordinator to another, and let the internal network layer handle closing
  of connections and keep-alive.

* Prevent a write to RocksDB during recovery in the case that the database
  already exists. The write at startup is potentially blocking, and will delay
  the startup for servers that were shut down while in a write-stopped state.

* Fix recovery of "clientId" values in Agency when restarting an agent from
  persistence.

* Added "startupTime", "computationTime" and "storageTime" to Pregel result
  statistics.

* Add query execution time and query id to audit log query messages.

* Fixed issue #13238 Thread naming API on Windows are now used only if
  available in KERNEL32.DLL

* Fix for issue #772: Optimized document counting for ArangoSearch views.
  Added new ArangoSearch view option 'countApproximate' for customizing view
  count strategy.

* Fix ordering of FollowerInfo lists in maintainer mode.

* Fix AR-113. Disallow non-values in the AQL geo-index-optimizer rule.

* Added SNI support for arangosh.

* Fix agency restart with mismatching compation and log indexes.

* Improve performance and memory efficiency of agency restart from persisted
  database directory.

* Added the following agency-related metrics:
  - `arangodb_agency_client_lookup_table_size`: current number of entries in
    agency client id lookup table. This gauge is available only on agent
    instances.
  - `arangodb_agency_cache_callback_count`: current number of entries in agency
    cache callbacks table. This gauge will be effective on coordinators and DB
    servers.
  - `arangodb_agency_callback_count`: current number of agency callbacks
    registered. This gauge will be effective on coordinators and DB servers.

* Fix cluster-internal replication of documents with special keys (percent
  character, which has a special meaning when used inside URLs).

* Improvements for the Pregel distributed graph processing feature:
  - during the loading/startup phase, the in-memory edge cache is now
    intentionally bypassed. The reason for this is that any edges are looked up
    exactly once, so caching them is not beneficial, but would only lead to
    cache pollution.
  - the loading/startup phase can now load multiple collections in parallel,
    whereas previously it was only loading multiple shards of the same
    collection in parallel. This change helps to reduce load times in case there
    are many collections with few shards, and on single server.
  - the loading and result storage phases code has been overhauled so that it
    runs slightly faster.
  - for Pregel runs that are based on named graphs (in contrast to explicit
    naming of the to-be-used vertex and edge collections), only those edge
    collections are considered that, according to the graph definition, can have
    connections with the vertex. This change can reduce the loading time
    substantially in case the graph contains many edge definitions.
  - the number of executed rounds for the underlying Pregel algorithm now does
    not vary for different `parallelism` values.

* Reimplement coordshort request handler. The new implementation only runs two
  DB queries without any additional requests to other coordinators, resulting in
  reduced load on the cluster. Previously this involved requests to all
  coordinators, where each of them ran two DB queries.

* When querying the list of currently running or slow AQL queries, ignore
  not-yet created databases on other coordinators.

* Fix AQL cost estimate of spliced subqueries which could lead to overly large
  numbers in the explain output of such queries.

* Add an AQL query kill check during early pruning. Fixes issue #13141.

* Fix Windows directory creation error handling.

* Added new metrics for tracking AQL queries and slow queries:
  * `arangodb_aql_query_time`: histogram with AQL query times distribution.
  * `arangodb_aql_slow_query_time`: histogram with AQL slow query times
    distribution.

* Reduce the number of dropped followers when running larger (>= 128 MB) write
  transactions.

* Remove a case in which followers were dropped unnecessarily in streaming
  transactions that replicated to the same follower.

* Added metrics for collection locks:
  - `arangodb_collection_lock_timeouts_exclusive`: Number of lock timeouts when
    trying to acquire collection exclusive locks
  - `arangodb_collection_lock_timeouts_write`: Number of lock timeouts when
    trying to acquire collection write locks
  - `arangodb_collection_lock_acquisition_micros`: Total amount of collection
    lock acquisition time [Î¼s]
  - `arangodb_collection_lock_acquisition_time`: Total collection lock
    acquisition time histogram [s]

* Reduce lock timeout on followers to 15 seconds.
  Rationale: we should not have any locking conflicts on followers, generally.
  Any shard locking should be performed on leaders first, which will then,
  eventually replicate changes to followers. replication to followers is only
  done once the locks have been acquired on the leader(s).

* Better tracking of memory used in AQL graph traversals, COLLECT and SORT
  operations. From this version onwards, certain AQL queries can report a higher
  memory usage than in previous versions of ArangoDB. This is not because the
  queries use more memory than before, but because the memory usage tracking has
  been improved.
  A side effect of this change is that queries with a memory limit set may now
  be aborted whereas in previous versions they ran through successfully (but
  actually violated the limit). In this case it may be necessary to adjust (i.e.
  raise) query memory limits accordingly.

* Added startup option `--foxx.force-update-on-startup` to toggle waiting for
  all Foxx services in all databases to be propagated to a coordinator before it
  completes the boot sequence.
  In case the option is set to `false` (i.e. no waiting), the coordinator will
  complete the boot sequence faster, and the Foxx services will be propagated
  lazily. Until the initialization procedure has completed for the local Foxx
  apps, any request to a Foxx app will be responded to with an HTTP 503 error
  and message

    waiting for initialization of Foxx services in this database

  This can cause an unavailability window for Foxx services on coordinator
  startup for the initial requests to Foxx apps until the app propagation has
  completed.

  When not using Foxx, this option should be set to `false` to benefit from a
  faster coordinator startup.
  Deployments relying on Foxx apps being available as soon as a coordinator is
  integrated or responding should set this option to `true` (which is the
  default value).
  The option only has an effect for cluster setups.
  On single servers and in active failover mode, all Foxx apps will be available
  from the very beginning.
  Note: ArangoDB 3.6 and 3.7 introduce this option with a default value of
  `true`. ArangoDB 3.8 changes the default value to `false`.

* Changed the server-side implementation of the following internal JavaScript
  APIs to no-ops:
  * `internal.reloadAqlFunctions()`: this is a no-op function now
  * `@arangodb/actions.buildRouting()`: this is a no-op function now
  * `@arangodb/actions.routingTree`: will return an empty object
  * `@arangodb/actions.routingList`: will return an empty object

  All the above APIs were intended to be used for internal means only. These
  APIs are deprecated now and will be removed in ArangoDB v3.9.

* Fix HTTP/1.1 status response header in fuerte responses

  This change makes fuerte return the full status header, including the numeric
  status code and the status string in the `http/1.1` header of fuerte
  responses.

  Previously, the return header lacked the numeric status code, so it looked
  like
  ```
  "http/1.1" : "Ok"
  ```
  Now, with the numeric status code, the response header will look like
  ```
  "http/1.1" : "200 Ok"
  ```
  This PR also adds a protocol() method for arango client connections in order
  to check the protocol in use. The possible return values are
  - "http" for HTTP/1.1 connections
  - "http2" for HTTP/2 connections
  - "vst" for VST connections
  - "unknown" for everyhting else
  This is needed during testing, but can also be used for other purposes.

* Fixed bug in the connection pool which could prevent connection reusage under
  high load and lead to lots of new connection creations, in particular with
  TLS.

* Added more metrics around connection pool.

* Fix a potential nullptr access in AsyncAgencyComm in case there was a specific
  error when sending an agency request.

* Clean up agency change log, cluster info caches.


v3.7.5 (2020-12-09)
-------------------

* Fixed ES-662 by introducing refactored thread pool to make more efficient
  consolidation and commit routines for links of ArangoSearch views.

  Added new command line options for fine-grained ArangoSearch maintenance
  control:
  - `--arangosearch.commit-threads` - max number of ArangoSearch commit
    threads
  - `--arangosearch.commit-threads-idle` - min number of ArangoSearch
    commit threads
  - `--arangosearch.consolidation-threads` - max number of ArangoSearch
    consolidation threads
  - `--arangosearch.consolidation-threads-idle` - min number of ArangoSearch
    consolidation threads

  Deprecated the following command line options:
  - `--arangosearch.threads` - will be used in --arangosearch.commit-threads`
    and `--arangosearch.consolidation-threads` as provided value divided by 2
    if set to non-autodetect value > 0
  - `--arangosearch.threads-limit`

* Updated ArangoDB Starter to 0.14.15-1.

* Fixed agency redirect in poll API.

* Updated arangosync to 1.2.1.

* Added support for fetching the list of currently running and slow AQL queries
  from all databases at once, by adding an `all` parameter to the following
  query APIs:

  * `require("@arangodb/aql/queries").current({ all: true })`: will return the
    currently running queries from all databases, not just the currently
    selected database.
  * HTTP GET `/_api/query/current?all=true`: same, but for the HTTP REST API.
  * `require("@arangodb/aql/queries").slow({ all: true })`: will return the slow
    query history from all databases, not just the currently selected database.
  * HTTP GET `/_api/query/slow?all=true`: same, but for the HTTP REST API.
  * `require("@arangodb/aql/queries").clearSlow({ all: true })`: will clear the
    slow query history for all databases, not just the currently selected
    database.
  * HTTP DELETE `/_api/query/slow?all=true`: same, but for the HTTP REST API.

  Using the `all` parameter is only allowed when making the call inside the
  `_system` database and with superuser privileges.

* Fixed issue #12734: Accept HTTP headers into Foxx framework.

* Fix Gauge class' assignment operators.

* Clean up callback bin and empty promises in single-host-agency.

* Fix an issue where a query would not return a result when the geo index was
  used.

* Fix the activation of the agency supervision maintenance via the REST API
  `/_admin/cluster/maintenance`. This API stored a boolean value instead of an
  (expected) maintenance period end date/time string.

* Make the cancel operation safe for asynchronoulsly started JavaScript
  transactions (via HTTP POST to `/_api/transaction` with the `x-arango-async`
  header set).

* Fixed initial population of local AgencyCache values after a server restart.
  Previously the local cache was populated from the agency using a commit index
  value of 1, whereas it should have been 0 to get the full agency snapshot.

* Updated OpenSSL to 1.1.1h.

* Make the number of network I/O threads properly configurable via the startup
  option `--network.io-threads`. This option existed before, but its configured
  value was effectively clamped to a value of `1`. ArangoDB 3.7.5 thus also uses
  a default value of `1` for this option to remain compatible in terms of
  default option values.

* Fix internal issue #777: Fixed memory access while substituting stored values
  for ArangoSearch view optimization.

* Added new metric `arangodb_network_forwarded_requests` to track the number
  of requests forwarded from one coordinator to another in a load-balancing
  context.

* Added new metric `arangodb_replication_cluster_inventory_requests` to track
  the number of requests received for cluster inventories. The cluster
  inventory API is called at the beginning of a dump process or by arangosync.

* Added new AQL metrics:
  - `arangodb_aql_total_query_time_msec": Total execution time of all AQL
    queries (ms)
  - `arangodb_aql_all_query`: total number of all AQL queries

* Added new metric `arangodb_aql_total_query_time_msec` to track the combined
  runtime of AQL queries (slow queries and non-slow queries).

* Added more scheduler metrics:

  - `arangodb_scheduler_threads_started`: Total number of scheduler threads
    started
  - `arangodb_scheduler_threads_stopped`: Total number of scheduler threads
    stopped
  - `arangodb_scheduler_jobs_done`: Total number of scheduler queue jobs done
  - `arangodb_scheduler_jobs_submitted`: Total number of jobs submitted to the
    scheduler queue
  - `arangodb_scheduler_jobs_dequeued`: Total number of jobs dequeued from the
    scheduler queue
  - `arangodb_scheduler_num_working_threads`: Number of currently working
    scheduler threads

* Added startup option `--server.unavailability-queue-fill-grade`. This option
  has a consequence for the `/_admin/server/availability` API only, which is
  often called by load-balancers and other availability probing systems.
  The `/_admin/server/availability` API will now return HTTP 200 if the fill
  grade of the scheduler's queue is below the configured value, or HTTP 503 if
  the fill grade is above it. This can be used to flag a server as unavailable
  in case it is already highly loaded.
  The default value for this option is `1`, which will mean that the
  availability API will start returning HTTP 503 responses in case the scheduler
  queue is completely full. This is mostly compatible with previous versions of
  ArangoDB.
  Previously the availability API still returned HTTP 200 in this situation, but
  this can be considered a bug, because the server was effectively totally
  overloaded.
  To restore 100% compatible behavior with previous version, it is possible to
  set the option to a value of `0`, which is a special value indicating that the
  queue fill grade will not be honored.

  To prevent sending more traffic to an already overloaded server, it can be
  sensible to reduce the default value to `0.75` or even `0.5`.
  This would mean that instances with a queue longer than 75% (or 50%, resp.) of
  their maximum queue capacity would return HTTP 503 instead of HTTP 200 when
  their availability API is probed.

  nb: the default value for the scheduler queue length is 4096.

* Fixed bug with ArangoSearch views on SmartGraph edge collections which could
  contain some documents twice.
  This change removes `_to_*` local auxiliary link creation and existence within
  a view linked with a SmartGraph edge collection.

* Fixed an AQL bug that ignored PRUNE statements in OneShard setups.

* Added arangobench options:
  `--create-database` to create the test database on start
	`--duration` to run test for a duration rather than a defined count

* Fixed a deadlock between AQL write transactions and hotbackup, since in AQL
  write transactions follower transactions did not know they are follower
  transactions.

* Make the DOCUMENT AQL function eligible for running on DB servers in OneShard
  deployment mode. This allows pushing more query parts to DB servers for
  execution.

* Fix REST API endpoint PUT `/_api/collection/<collection>/recalculateCount` on
  coordinators. Coordinators sent a wrong message body to DB servers here, so
  the request could not be handled properly.

* Fixed issue #12778: fails validation if additionalProperties: false.

* Added missing exceptions catch clause for some parts of supervision and
  heartbeat threads.

* Fixed potential deadlock in cluster transactions if a transaction is returned
  that was soft-aborted by transaction garbage collection before.
  This deadlock should rarely ever occur in practice, as it can only be
  triggered once during the server shutdown sequence.

* Fix a memory leak because server internal connections were not cleaned up for
  agency communication.

* Added compile option USE_JEMALLOC_PROF to enable memory profiling.

* Fixed BTS-233 issue: Fixed invalid IndexId comparator.

* Fixed very spurious errors if the `holdReadLockCollection` replication API for
  the getting-in-sync procedure of shards was called during server shutdown.
  In this case that method could ask the transaction manager for a specific
  transaction, but wasn't returning one due to the server shutdown.

* Agency cache clears change history. This keeps the change history, introduced
  in v3.7.4, from growing in size too much.

* Bug-fix: Allow to unlink a view created on a SmartGraphEdge collection.

* If a collection (or database) is dropped during the instantiation of an AQL
  query, the setup code now aborts with an ERROR_QUERY_COLLECTION_LOCK_FAILED
  and earlier.
  Before the setup code could abort with TRI_ERROR_INTERNAL in the same case.

* Bug-fix: Creating an additional index on the edge collection of a disjoint
  SmartGraph could falsely result in an error:
  `Could not find all smart collections ...`
  This is now ruled out and indexes can be created as expected.

* Fixed issue #12248: Web UI - Added missing HTML escaping in the setup script
  section of a foxx app.

* Add parameter so `db.collection.truncate({compact: false})` will stop
  compaction from happening. Compaction may have performance impacts even if the
  truncate was invoked on nearly empty collections.

* Instead of failing to connect to INADDR_ANY refuse it as a parameter, with a
  descriptive error message for novice users (issue #12871).

* Fixed collection count which could be off after a server crash.


v3.7.4 (2020-10-16)
-------------------

* Data definition reconciliation in cluster has been modified
  extensively to greatly accelerate the creation of 1000s of
  databases through following means:
  - AgencyCache offers change sets API based on Raft index.
  - ClusterInfo caches are only updated using change sets.
  - Maintenance uses local as well as agency change sets to limit
    the scope of every runtime to these change sets.


v3.7.3 (2020-10-14)
-------------------

* Added the following metrics for synchronous replication in the cluster:

  - `arangodb_refused_followers_count`: Number of times a shard leader received
    a refusal answer from a follower during synchronous replication.
  - `arangodb_sync_wrong_checksum`: Number of times a mismatching shard
    checksum was detected when syncing shards. In case this happens, a resync
    will be triggered for the shard.

* Fixed handling of failedLeaderJob. In case of a plan modification, that
  removes a server from the plan, e.g. reduce replication factor. Directly
  followed by a failure of the current shard leader, would reinsert the just
  removed server in the plan, which is undesired, we first need to have a full
  "desync" cycle on this server to be reusable in the plan again.

* Make sure the optimizer doesn't pick another index than the TTL index itself
  while fulfilling the expiry of TTL.

* Added optional verbose logging for agency write operations. This logging is
  configurable by using the new log topic "agencystore".

  The following log levels can be used for for the "agencystore" log topic to
  log writes to the agency:
  - DEBUG: will log all writes on the leader
  - TRACE: will log all writes on both leaders and followers
  The default log level for the "agencystore" log topic is WARN, meaning no
  agency writes will be logged.
  Turning on this logging can be used for auditing and debugging, but it is not
  recommended in the general case, as it can lead to large amounts of data being
  logged, which can have a performance impact and will lead to higher disk space
  usage.

* Print image base address and CPU context (if available) in crash handler
  messages.

* Added configuration option `--query.tracking-slow-queries` to decide whether
  slow queries are tracked extra.

* Added configuration option `--query.tracking-with-querystring` to decide
  whether the query string is shown in the slow query log and the list of
  currently running queries. The option is true by default.

  When turned off, querystrings in the slow query log and the list of currently
  running queries are just shown as "<hidden>".

* Added configuration option `--query.tracking-with-datasources` to toggle
  whether the names of data sources used by queries are shown in the slow query
  log and the list of currently running queries. The option is false by default.
  When turned on, the names of data sources used by the query will be shown in
  the slow query log and the list of currently running queries.

* Fixed handling of failoverCandidates. Sometimes, a server can still be a
  failoverCandidate even though it has been taken out of the Plan. With this
  fix, such a server is quickly taken out of failoverCandidates and it can never
  be re-added to the Plan before this has happened.

* Fix #12693: SORT inside a subquery could sometimes swallow part of its input
  when it crossed boundaries of internal row batches.

* Fixed issue BTS-212: Web UI doesn't let to make partial view update and
  partial view update should be audited (also reported as ES-700).
  Fixed link definition comparison logic: equality wasn`t properly detected and
  led to link recreation.

* Added configuration option `--rocksdb.sync-delay-threshold`.
  This option can be used to track if any RocksDB WAL sync operation is
  delayed by more than the configured value (in milliseconds). The intention
  is to get aware of severely delayed WAL sync operations.

* Add database, shard name and error information to several shard-related log
  messages.

* Display shard names of a collection in the web interface when in the details
  view of the collection.

* Added HTTP requests metrics for tracking the number of superuser and normal
  user requests separately:

  - `arangodb_http_request_statistics_superuser_requests`: Total number of HTTP
    requests executed by superuser/JWT
  - `arangodb_http_request_statistics_user_requests`: Total number of HTTP
    requests executed by clients

* Fixed a bug in handling of followers which refuse to replicate operations.
  In the case that the follower has simply been dropped in the meantime, we now
  avoid an error reported by the shard leader.

* Fix a performance regression when a LIMIT is combined with a COLLECT WITH
  COUNT INTO. Reported in ES-692.

* Fix REST handler GET /_admin/status when called with URL parameter value
  `overview=true`. For generating the `hash` attribute in the response, the
  current Plan was retrieved and analyzed. Due to a change in the internal Plan
  format the REST handler code failed to pick up the number of servers, which
  resulted in the REST handler returning HTTP 500 in cluster mode.

* Use rclone built from v1.51.0 source with go1.15.2 instead of prebuilt
  v1.53.0 release.

* Fixed a bug in AQL COLLECT with OPTIONS { "hash" } that led to a quadratic
  runtime in the number of output rows.

* Added startup option `--database.old-system-collections` to toggle automatic
  creation of system collections `_modules` and `_fishbowl`, along with their
  internal usage. These collections are useful only in very few cases, so it
  is normally not worth to create them in all databases.
  The `_modules` collection is only used to register custom JavaScript modules,
  for which there exists no API, and `_fishbowl` is used to store the temporary
  list of Foxx apps retrieved from the Github Foxx store.
  If the option value is `false` (which is the default from v3.8 onwards, but
  for v3.7 the default value is `true` for downwards-compatibility), the two
  collections will not be created for any new database. The `_fishbowl`
  collection will still be created dynamically when needed. If the option value
  is `true` (the default value in v3.7), the collections will be created
  regularly as before.
  The default value for the option is going to change to `false` in v3.8,
  meaning the collections will not be created anymore there by default.

  Any functionality related to the `_modules` system collection is deprecated
  and will be removed in ArangoDB v3.9.

  Two side effects of turning this option off are:
  * there will no be iteration over all databases at server startup just to
    check the contents of all `_modules` collections.
  * less collections/shards will be around for deployments that create a large
    number of databases.
  Already existing `_modules` and `_fishbowl` system collections will not be
  modified by this PR, even though they will likely be empty and unused.

* Don't iterate over all databases at server startup in order to initialize the
  routing information. This is not necessary, as the routing information is
  global and not tied to a specific database.

* Fixed a possible crash during instantiation of an AQL graph traversal.
  Reported in #12597.

* Added safeguards against using V8 internally in environments that have
  JavaScript turned off via the `--javascript.enabled false` option.

* Make scheduler properly count down the number of working threads in case an
  exception happens in a worker thread.

* Turn off upgrade checks in arangod in alpha/beta/preview Enterprise builds,
  too.
  Previously it was already turned off in arangod for Enterprise builds already,
  but only for stable releases and not preview releases.

* Fixed and extended LDAP log messages.

* Added LDAP_OFF if referrals and restart are false.

* If LDAP search fails, also retry (update to given number of retries).

* Fixed infinite reload of the login window after logout of an LDAP user.

* Make the reboot tracker catch failed coordinators, too. Previously the reboot
  tracker was invoked only when a DB server failed or was restarted, and when a
  coordinator was restarted. Now it will also act if a coordinator just fails
  (without restart).

* Added scheduler thread creation/destruction metrics:

  - `arangodb_scheduler_threads_started`: Number of scheduler threads started
  - `arangodb_scheduler_threads_stopped`: Number of scheduler threads stopped

* Added startup option `--query.max-runtime` to limit the maximum runtime of all
  AQL queries to a specified threshold value (in seconds). By default, the
  threshold is 0, meaning that the runtime of AQL queries is not limited.
  Setting it to any positive value will restrict the runtime of all AQL queries
  unless it is overwritten in the per-query "maxRuntime" query option.
  Please note that setting this option will affect *all* queries in all
  databases, and also queries issues for administration and database-internal
  purposes.
  If a query exceeds the configured runtime, it will be killed on the next
  occasion when the query checks its own status. Killing is best effort, so it
  is not guaranteed that a query will no longer than exactly the configured
  amount of time.

* Ensure that the argument to an AQL OPTIONS clause is always an object which
  does not contain any dynamic (run-time) values. Previously, this was only
  enforced for traversal options and options for data-modification queries. This
  change extends the check to all occurrences of OPTIONS.

* Added `details` option to figures command of a collection:
  `collection.figures(details)`

  Setting `details` to `true` will return extended storage engine-specific
  details to the figures. The details are intended for debugging ArangoDB itself
  and their format is subject to change. There is not much use in using the
  details from a client application.
  By default, `details` is set to `false`, so no details are returned and the
  behavior is identical to previous versions of ArangoDB.

* Implement RebootTracker usage for AQL queries in case of coordinator restarts
  or failures. This will clean up the rest of an AQL query on dbservers more
  quickly and in particular release locks faster.

* Serialize maintenance actions for each shard. This addresses lost document
  problems found in chaos testing.

* Enforce a maximum result register usage limit in AQL queries. In an AQL query,
  every user-defined or internal (unnamed) variable will need a register to
  store results in.

  AQL queries that use more result registers than allowed (currently 1000) will
  now abort deterministically during the planning stage with error 32
  (`resource limit exceeded`) and the error message
  "too many registers (1000) needed for AQL query".

  Before this fix, an AQL query that used more than 1000 result registers
  crashed the server when assertions were turned on, and the behavior was
  undefined when assertions were turned off.

* Fixed some cases where subqueries in PRUNE did not result in a parse error,
  but either in an incomprehensible error (in 3.7), or undefined behavior
  during execution (pre 3.7).

* Fixed an issue with audit logging misreporting some document requests as
  internal instead of logging the proper request information.

* Add attributes `database` and `user` when tracking current and slow AQL
  queries.
  `database` contains the name of the database the query is/was running in,
  `user` contains the name of the user that started the query.
  These attributes will be returned in addition when calling the APIs for
  current and slow query inspection:
  * GET `/_api/query/current` and `require("arangodb/aql/queries").current()`
  * GET `/_api/query/slow` and `require("arangodb/aql/queries").slow()`

  The "slow query" log message has also been augmented to contain the database
  name and the user name.

  The `user` attribute is now also displayed in the web interface in the
  "Running queries" and "Slow queries" views.

* Added metrics for V8 contexts usage:
  * `arangodb_v8_context_alive`: number of V8 contexts currently alive.
  * `arangodb_v8_context_busy`: number of V8 contexts currently busy.
  * `arangodb_v8_context_dirty`: number of V8 contexts currently dirty.
  * `arangodb_v8_context_free`: number of V8 contexts currently free.
  * `arangodb_v8_context_max`: maximum number of concurrent V8 contexts.
  * `arangodb_v8_context_min`: minimum number of concurrent V8 contexts.

* Updated arangosync to 0.7.11.

* Make followers in active failover run a compaction after they process a
  truncate operation and the truncate removed more than 4k documents. This can
  help to reclaim disk space on the follower earlier than without running the
  truncate.

* The REST API PUT `/_api/collection/<name>/truncate` will now also run a
  compaction if the truncation affected more than 4k documents. This may add
  extra latency to the truncate operation, but can help to reclaim disk space
  earlier.

* Added REST API PUT `/_admin/compact` for compacting the entire database data.
  This endpoint can be used to reclaim disk space after substantial data
  deletions have taken place. The command is also exposed via the JavaScript API
  as `db._compact();`.

  This command can cause a full rewrite of all data in all databases, which may
  take very long for large databases. It should thus only be used with care and
  only when additional I/O load can be tolerated for a prolonged time.

  This command requires superuser access and is only available for the RocksDB
  storage engine.

* Don't allow creation of smart satellite graphs or collections (i.e. using
  `"isSmart":true` together with `"replicationFactor":"satellite"` when creating
  graphs or collections. This combination of parameters makes no sense, so that
  the server will now respond with "bad parameter" and an HTTP status code of
  HTTP 400 ("Bad request").

* Add exit code for ICU database loading startup errors.

* Fixed issue #12507: SegFault when using an AQL for loop through edges.

* Make the `IS_IPV4` AQL function behave identical on macOS as on other
  platforms. It previously allowed leading zeros in octets on macOS, whereas on
  other platforms they were disallowed.
  Now this is disallowed on macOS as well.

* Added new metric "arangodb_aql_slow_query" for slow AQL queries, so this can
  be monitored more easily.

* Added new metric "arangodb_scheduler_queue_length" for the scheduler's
  internal queue length.

* Added new metric "arangodb_scheduler_queue_full_failures" for tracking cases
  of a full scheduler queue and dropping requests.

* Added new metrics for the number of V8 contexts dynamically created and
  destroyed ("arangodb_v8_context_created" and "arangodb_v8_context_destroyed")
  and for the number of times a V8 context was entered and left
  ("arangodb_v8_context_entered" and "arangodb_v8_context_exited"). There is
  also a new metric for tracking the cases when a V8 context cannot be
  successfully acquired and an operation is not performed
  ("arangodb_v8_context_enter_failures").

* Added extra info to "queue full" and "giving up waiting for unused v8 context"
  log messages.

* Request to the `/_admin/statistics` API now processed via the CLIENT_FAST
  lane.
  Previously they were handled in the CLIENT_SLOW lane, meaning that monitoring
  requests using that API didn't get through when the queue was rather full.

* Introduce an internal high-water mark for the maximum row number that was
  written to in an AqlItemBlock. Using this number several operations on the
  whole block, such as cleaning up or copying can be made more efficient when
  run on only partially filled blocks.

* Fixed issue BTS-169: cost estimation for LIMIT nodes showed wrong number of
  estimated items.


v3.7.2.2 (2020-10-07)
---------------------

* Fixed issue ES-664: SEARCH vs FILTER lookup performance.
  Consolidation functionality for ArangoSearch view links was able to hit non-
  mergable enormous amount of segments due to improper scheduling logic.

* Fix for issue BTS-183: added pending operations purging before ArangoSearch
  index truncation.

* Fixed: More cases in AQL can now react to a query being killed, so reaction
  time to query abortion is now shortened. This was a regression in comparison
  to 3.6 series.

* Fixed internal issue #741: STARTS_WITH fails to accept 'array' as variable.

* Fixed internal issue #738: PHRASE doesn't accept a reference to an array of
  arguments.

* Fixed internal issue #747: fixed possible dangling open files in ArangoSearch
  index after remove operations.


v3.7.2.1 (2020-09-02)
---------------------

* Add option `--rocksdb.max-write-buffer-size-to-maintain` with default of 0.
  This configures how much memory RocksDB is allowed to use for immutable
  flushed memtables/write-buffers. The default of 0 will usually be good for all
  purposes and restores the 3.6 memory usage for write-buffers.


v3.7.2 (2020-08-21)
-------------------

* Fixed internal issue #744: LIMIT with only offset and constrained heap
  optimization will use estimation value for ArangoSearch views.

* Make UPSERT statement with collection bind parameter behave identical to its
  non-bind parameter counterpart.

  For example, the query 

      FOR d IN ["key_1", "key_2", "key_3", "key_1"] 
        UPSERT d INSERT d UPDATE d IN @@collection

  would fail with "unique constraint violation" when used with a collection bind
  parameter, but the equivalent query

      FOR d IN ["key_1", "key_2", "key_3", "key_1"] 
        UPSERT d INSERT d UPDATE d IN collectionName

  with a hard-coded collection name would succeed. This is now fixed so both
  queries have the same behavior (no failure) in single server.

* Updated arangosync to 0.7.10.

* Fixed internal issue #742: Add tick storage in index meta payload for
  ArangoSearch view links after collection truncate operation.

* Fixed issue #12304: insert in transaction causing
  com.arangodb.ArangoDBException: Response: 500, Error: 4 - Builder value not
  yet sealed.

  This happened when too deeply-nested documents (more than 63 levels of
  nesting) were inserted. While indefinite nesting is still not supported, the
  error message has been corrected from the internal HTTP 500 error "Builder
  value not yet sealed" to the correct HTTP 400 "Bad parameter".

* Fixed: During a move-shard job which moves the leader there is a
  situation in which the old owner of a shard can reclaim ownership
  (after having resigned already), with a small race where it allows to
  write documents only locally, but then continue the move-shard to a
  server without those documents. An additional bug in the MoveShard
  Supervision job would then leave the shard in a bad configuration
  with a resigned leader permanently in charge.

* Fixed a problem with potentially lost updates because a failover could happen
  at a wrong time or a restarted leader could come back at an unlucky time.

* Fixed BTS-167: Lingering Queries - Canceled from the UI.
  This fixes queries not vanishing from the list of running queries in the web
  UI in case the query was canceled using the "Cancel" button in web UI's query
  editor.

* Fixed bad behavior in agency supervision in some corner cases involving
  already resigned leaders in Current.

* Fixed issue ES-664: the optimizer rule `inline-subqueries` must not pull out
  subqueries that contain a COLLECT statement if the subquery is itself called
  from within a loop. Otherwise the COLLECT will be applied to the values in the
  outer FOR loop, which can produce a different result.

* Fixed that dropping a vanished follower works again. An exception response
  to the replication request is now handled properly.

* Make AQL user-defined functions (UDFs) work in a cluster in case the UDF runs
  an AQL query inside its own function code (BTS-159).

* Fixed hotbackup S3 credentials validation and error reporting for upload and
  download.

* Fixed a blockage on hotbackup when writes are happening concurrently, since
  followers could no longer replicate leader transactions.

* Fix: The 'sorted' COLLECT variant would return undefined instead of null when
  grouping by a null value.

* Fix: writeConcern is now honored correctly (ES-655).

* Fixed internal issue #739: ArangoSearch filter volatility takes into account
  calculation nodes dependencies.

* Fixed OASIS-278 issue: Added proper sort/calc nodes cleanup for late
  materialzation after OneShard optimization.

* Slightly improve the performance of cluster DDL maintenance operations.

* Added AQL functions `IS_IPV4`, `IPV4_TO_NUMBER`, `IPV4_FROM_NUMBER` for IPv4
  address checks and conversions.

* Added AQL function `PRODUCT` to calculate the product of an array.

* Improve performance of internal cluster Plan and Current reload operations.

* Fixed issue #12349: arangosh compact Arangoerror 404.

* Improve performance of many non-subquery AQL queries, by optimizing away some
  storage overhead for subquery context data.

* Always fetch data for /_api/cluster/agency-dump from leader of the agency.
  Add option "redirectToLeader=true" to internal /_api/agency/state API.

* Fixed regression with view use-after-create in cluster (BTS-137).

* Slightly improved the performance of some k-shortest-path queries.


v3.7.1 (2020-08-07)
-------------------

* Fixed issue #12297: ArangoDB 3.6.5 Swagger Error?
  This issue caused the Swagger UI for displaying the APIs of user-defined Foxx
  services to remain invisible in the web UI, because of a JavaScript exception.
  This PR fixes the JavaScript exception, so the services API is displayed
  again properly.

* Fixed issue #12248: Web UI on 3.6.5: 404 error on adding new index.
  This issue caused "404: not found" errors when creating indexes via the web
  UI. The indexes were created successfully despite the error message popping
  up. This fix removes the misleading unconditional error message.


v3.7.1-rc.1 (2020-07-24)
------------------------

* Added startup option `--rocksdb.encryption-key-rotation` to
  activate/deactivate the encryption key rotation REST API. The API is disabled
  by default.

* Add internal caching for LogicalCollection objects inside
  ClusterInfo::loadPlan.

  This allows avoiding the recreation of LogicalCollection objects that did not
  change from one loadPlan run to the next. It reduces CPU usage considerably on
  both Coordinators and DB-servers.

* Fixed reading analyzers revisions for freshly updated cluster.

* Fixed undefined behavior in AQL COLLECT with multiple group variables (issue
  #12267).
  If you are grouping on "large" values that occur multiple times in different
  groups, and two different groups with the same large value are written to
  different batches in the output then the memory could be invalid.
  e.g. the following query is affected:
  ```
  FOR d IN documents
  FOR batchSizeFiller IN 1..1001
  COLLECT largeVal = d.largeValue, t = batchSizeFiller
  RETURN 1
  ```

* Revive faster out-of-range comparator for secondary index scans that do a full
  collection index scan for index types "hash", "skiplist", "persistent".

* Conflict error codes (1200) will now use the proper error message instead of a
  generic and misleading "precondition failed".

* Improve performance of agency cache by not copying the hierarchical Node
  tree result for serialization, but serializing it directly.

* Turn off maintenance threads on Coordinators, as they are not needed there.

* Fixed internal issue #733: Primary sort compression in ArangoSearch views now
  used properly.

* Change HTTP response code for error 1450 ("too many shards") from HTTP 500 to
  HTTP 400, as this is clearly a client error.

* Fix spurious lock timeout errors when restoring collections.

* Make sure cluster statistics in web UI work in case a coordinator is down.

* Updated arangosync to 0.7.8.

* Fixed a race between a new request and the keepAlive timeout.

* Fixed hotbackup upload and download with encryption at rest key indirection.

* Fixed crash in cleanup of parallel traversal queries.

* Added cluster metrics `arangodb_load_plan_accum_runtime_msec` and
  `arangodb_load_current_accum_runtime_msec` to track the total time spent in
  `loadPlan()` and `loadCurrent()` operations.

* Fixed wrong reporting of failures in all maintenace failure counter metrics
  (`arangodb_maintenance_action_failure_counter`). Previously, each successful
  maintenance operation was reported as a failure, so the failure counters
  actually were counters for the number of successful maintenance actions.

* Adjusted the scale of the `arangodb_maintenance_action_queue_time_msec` to
  cover a more useful range.

* The filter executor will now overfetch data again if followed by a limit, same
  as in 3.6 series.
  The following queries are effected:
  ```
  something
  FILTER a == b
  LIMIT 10
  ```
  something will now be asked for a full batch, instead of only 10 documents.

* In rare cases SmartBFS could use a wrong index for looking up edges. This is
  fixed now.

* The internally used JS-based ClusterComm send request function can now again
  use JSON, and does not require VelocyPack anymore. This fixes an issue with
  Foxx-App management (install, updated, remove) got delayed in a sharded
  environment, all servers do get all apps eventually, now the fast-path will
  work again.

* Fixed a rare race in Agents, if the leader is rebooted quickly there is a
  chance that it is still assumed to be the leader, but delivers a state shortly
  in the past.

* Keep the list of last-acknowledged entires in Agency more consistent.
  During leadership take-over it was possible to get into a situation that the
  new leader does not successfully report the agency config, which was
  eventually fixed by the Agent itself. Now this situation is impossible.

* Fixed a race in the ConnectionPool which could lease out a connection that got
  its idle timeout after the lease was completed. This could lead to sporadic
  network failures in TLS and to inefficiencies with TCP.

* Fixed restoring a SmartGraph into a database that already contains that same
  graph.
  The use case is restoring a SmartGraph from backup, apply some modifications,
  which are undesired, and then resetting it to the restored state, without
  dropping the database. One way to achieve this is to use arangorestore with
  the `overwrite` option on the same dataset, effectively resetting the
  SmartGraph to the original state. Without this fix, the workaround for is to
  either drop the graph (or the database) before the restore call, yielding an
  identical result.

* Fixed potential garbled output in syslog log output for the program name.

* Fixed infinite recursion when printing error messages about invalid logger
  configuration on startup.

* Fixed sporadic use-after-free ASan issue in logger shutdown.

* Added missing state rollback for failed attempt-based write locking of spin
  locks.

* Disable internal network protocol switch for cluster-internal communication,
  and hard-code the internal communication protocol to HTTP.

* Added vertex collection validation in case of a SmartGraph edge definition
  update.

* Fix cluster-internal request forwarding for VST requests that do not have any
  Content-Type header set. Such requests could have been caused by the Java
  driver (ES-635).

* Fixed issue OASIS-252: Hotbackup agency locks without clientId.

* Fixed internal-issue #726: added restore handling for custom analyzers.

* Fixed a potential agency crash if trace logging is on.

* Network layer now reports connection setup issues in more cases this replaces
  some INTERNAL_ERROR reports by more precise errors, those are only reached
  during failover scenarios.

* Fixed a document parsing bug in the Web UI. This issue occured in the
  document list view in case a document had an attribute called `length`.
  The result was an incorrect representation of the document preview.

* Re-enable access to GET `/_admin/cluster/numberOfServers` for all users by
  default. Requests to PUT `/_admin/cluster/numberOfServers` require admin
  user privileges. This restores the pre-3.7 behavior.
  In contrast to pre-3.7 behavior, the `--server.harden` option now can be
  used to restrict access to GET `/_admin/cluster/numberOfServers` to admin
  users, too. This can be used to lock down this API for non-admin users
  entirely.

* Fixed crash in HTTP2 implementation.

* Improve readability of running and slow queries in web UI by properly left-
  aligning the query strings.

* ClusterInfo will wait for syncer threads to shutdown.

* Correct some log entries.

* Allow changing collection properties for smart edge collections as well.
  Previously, collection property changes for smart edge collections were not
  propagated.

* Fixed BTS-110: Fulltext index with minLength <= 0 not allowed.

* Web UI: Removed unnecessary menubar entry in case of database node inspection.

* Adjust arangodump integration test to desired behavior, and make sure
  arangodump behaves as specified when invoking it with non-existing
  collections.

* Disallow using V8 dependent functions in SEARCH statement.

* Remove superflous `%>` output in the UI modal dialog in case the JSON editor
  was embedded.

* The Web UI now displays an error message inside the node information view in
  case the user has no access to retrieve the neccessary information.

* The `_from` and `_to` attributes of an edge document can now be edited from
  within the UI.

* Fixed a misleading error message in AQL.

* Fixed issue ES-609: "Transaction already in use" error when running
  transaction.
  Added option `--transaction.streaming-lock-timeout` to control the timeout in
  seconds in case of parallel access to a streaming transaction.

* Fixed internal issue BTS-107, offset over the main query passed through a
  subquery which has modification access to shards could yield incorrect
  results, if shards are large enough and skipping was large enough, both to
  overflow the batch-size in AQL for each shard individually.

  The following query would be affected:

  FOR x IN 1..100
  LET sub = (
    FOR y IN Collection
    UPDATE y WITH {updated: true} IN Collection
    RETURN new
  )
  LIMIT 5, 10
  REUTRN {x,sub}

  If a shard in Collection, has enough entries to fill a batch, the second shard
  could run into the issue that it actually does not skip the first 5 main
  queries, but reports their results in addition. This has the negative side
  effect that merging the subqueries back together was off.

* Fix undistribute-remove-after-enum-coll which would allow calculations to be
  pushed to a DBServer which are not allowed to run there.

* Removed the edge id's hover styling in case of embedded document editor in the
  Web UI as this functionality is disabled. This was misleading because the
  elements are not clickable.

* Internal issue BTS-71: Fixed error handling regarding communication with the
  agency. This could in a specific case cause collection creation in a cluster
  report success when it failed.

* Internal issue BTS-71: In a cluster, for collections in creation, suspend
  supervision jobs concerning replication factor until creation is completed.
  Previously, it could cause collection creation to fail (e.g. when a server
  failed during creation), even when it didn't have to.

* Internal issue BTS-71: Added a precondition to prevent creating a collection
  with an invalid `distributeShardsLike` property.

* In case a document got requested via the UI of a collection which does not
  exist, the UI now properly displays an error view instead of having a bad
  display state.

* Returned `AQL_WARNING()` to emit warnings from UDFs.

* Fixed internal issue #725: Added analyzers revision for _system database in
  queries.

* The Web UI is not disabling the query import button after file upload takes
  place.

* In case of a graph deletion failure, the Web UI displays now the correct
  error message.

* Privatized load plan / current in cluster info and cleanup following agency
  cache implementation.

* Allow removal of existing schemas by saving a schema of either `null` or `{}`
  (empty object). Using an empty string as schema will produce  an error in the
  web interface and will not remove the schema.

  The change also adjusts the behavior for the SCHEMA_VALIDATE AQL function in
  case the first parameter was no document/object. In this case, the function
  will now return nulland register a warning in the query, so the user can
  handle it.

* The Web UI is now reporting errors properly in case of editing ArangoSearch
  Views with invalid properties.

* Fixed bug in IResearchViewExecutor that lead to only up to 1000 rows being
  produced.

* Fixed an error scenario where a call could miss count skip.
  It was triggered in the case of Gather in Cluster, if we skipped over a full
  shard, and the shard did actually skip, but there are more documents to skip
  on another shard.

* Fixed that the hotbackup agency lock is released under all circumstances
  using scope guards. This addresses a rare case in which the lock was left
  behind.

* Allow restoring collections from v3.3.0 with their all-numeric collection GUID
  values, by creating a new, unambiguous collection GUID for them.
  v3.3.0 had a bug because it created all-numeric GUID values, which can be
  confused with numeric collection ids in lookups. v3.3.1 already changed the
  GUID routine to produce something non-numeric already, but collections created
  with v3.3.0 can still have an ambiguous GUID. This fix adjusts the restore
  routine to drop such GUID values, so it only changes something if v3.3.0
  collections are dumped, dropped on the server and then restored with the
  flawed GUIDs.

* Fixed hotbackup agency lock cleanup procedure.

* The Web UI's replication view is now checking the replication state
  automatically without the need of a manual reload.

* Changing the current users profile icon in the Web UI now renders the new icon
  and data directly without the need of a full UI browser reload.

* Fixed cluster behavior with HotBackup and non-existing backups on DB-Servers.

* Fixed that, when performing a graph AQL query while a (graceful) failover for
  the leader of the system collections is in progress, ArangoDB would report a
  "Graph not found error".
  The real error, namely that an agency transaction failed, was swallowed in
  the graph lookup code due to a wrong error code being used from Fuerte.
  We now generate a more appropriate 503 - Service Unavailable error.

* Fixed bad behavior that led to unnecessary additional revision tree rebuilding
  on server restart.

* Only advance shard version after follower is reported in sync in agency.

* Disabled new, potentially unsafe revision-based storage format.

* Allow for faster cluster shutdown. This should reduce the number of shutdown
  hangers in case the agents are stopped already and then coordinators or
  DB-Servers are shut down.

* Fixed issue ES-598. Web UI now shows correct permissions in case wildcard
  database level and wildcard collection level permissions are both set.

* Fixed non-deterministic test failure in Pregel WCC test.

* Add a recovery test to check that there are no warnings at server start after
  a graceful shutdown.

* Fixed unintentional connection re-use for cluster-internal communications.

* Fixed problem with newer replication protocol and ArangoSearch which could
  lead to server crashes during normal operation.

* Allow AQL queries on DB-Servers again. This is not an official supported
  feature, but is sometimes used for debugging. Previous changes made it
  impossible to run a query on a local shard.

* Fix restoring old arangodumps from ArangoDB 3.3 and before, which had index
  information stored in slightly different places in the dump.

* Fix internal test helper function `removeCost` to really remove costs.

* Fix invalid calls to `AgencyCommResult::slice()` method, which must only be
  made in case of an agency result was retrieved successfully. In case the call
  to the agency was not successful, `slice()` must not be called on it. This
  change makes sure this invariant holds true.

* Fix potential AQL query shutdown hanger.

* Modified the exception handling in the RestHandler. Asynchronous communication
  could lead to less detailed failure information.

* Use smarter default value preset in web UI for replication factor in case
  there are constraints established for the replication factor by the startup
  options `--cluster.min-replication-factor` and
  `--cluster.max-replication-factor`.

* Added permissions check before trying to read data from `_analyzers`
  collection.
  If these permissions are not there, no load is performed (user can not use
  analyzers from database anyway).

* Make the reboot tracker catch a failed server and permanently removed servers.
  This allows other servers in the cluster to move on more quickly, when a
  server fails and does not immediately come back.

* Added WCC pregel algorithm for weakly connected components.

* Display "geoJson" value for geo indexes in index overview in web UI.

* Allow specifiying arbitrary vertex attributes when creating a vertex via the
  graph viewer in the web UI. This is necessary for collections that have a
  schema and that require new vertices to have certain attributes/values.

* Pass through AQL query errors to UI graph viewer without turning them into
  "Internal Server Error"s.

* Augment nodes view in web UI with node id and full datetime of startup.

* Fixed "Create Debug Package" in web UI in cluster mode.

* Pick up selected "write concern" value entered in web UI when creating a
  collection in the cluster. Previously, the "write concern" value was not
  picked up correctly and collections were created with the default write
  concern.

* Fixed the creation of edges in the web UI, which did not work due to a
  JavaScript error.

* Fixed velocypack validator for proper check of keys.

* Handle missing "shardingStrategy" in Plan agency entries for collections.
  "shardingStrategy" entries can be missing for collections created with
  ArangoDB clusters in version 3.3 or earlier. A missing "shardingStrategy"
  entry can cause issues for certain queries that need to check the shard
  distribution on the DB-Servers.
  This change transparently handles missing "shardingStrategy" entries.


v3.7.1-beta.1 (2020-06-07)
--------------------------

* Fixed issue #8941: `frontendConfig.basePath` is duplicated for users API call

* Taken collection dropping from fast track in maintenance. This avoids blocking
  fast track maintenance threads when a shard cannot immediately be dropped
  because of some pending lock.

* Updated ArangoDB Starter to 0.14.15.

* Fixed a spuriously occuring shutdown deadlock between AgencyCallbackRegistry
  and AgencyCache.

* Fixed `"Plan" is not an object in agency` error messages when restarting
  DB-Servers that contained ArangoSearch Views with links.

* Fixed misordering of skipped number of rows report. Only triggered if you do a
  modification on a subquery nesting level 2 or more, e.g.:
  ```
    LET s1 = (
      LET s2 = (
        INSERT {} INTO collection
      )
      COLLECT WITH COUNT INTO c
      RETURN c
    )
  ```
  Here the c would be off, or a count on main query would be off.

* Fixed crash in execution of non-spliced subqueries if remainder of subquery is
  skipped.

* Added missing mutex to ConnectionPool::cancelConnections().

* Foxx API now respects "idiomatic" flag being explicitly set to false.

* Made modification subqueries non-passthrough. The passthrough logic only works
  if exactly as many output rows are produced as input rows are injected.
  If the subquery with modification is skipped however this API is violated, we
  perform the subquery with the modification, but then discard the input if
  skipped).

* Optimized Web UI styling for ArangoSearch Views when in read-only mode.

* Fixed a bug in the internal `recalculateCount()` method for adjusting
  collection counts. The bug could have led to count adjustments being
  applied with the wrong sign.

* Removed external file loading in a used 3rd party library (Web UI).

* More substantial metrics in agency.

* arangodump and arangorestore will now fail when using the `--collection`
  option and none of the specified collections actually exist in the database
  (on dump) or in the dump to restore (on restore). In case some of the
  specified collections exist, arangodump/restore will issue warnings about the
  invalid collections, but will continue to work for the valid collections.

* Improved network send request for more robustness.

* Added multiple RocksDB configuration options to arangod:
  * `--rocksdb.cache-index-and-filter-blocks` to make the RocksDB block cache
    quota also include RocksDB memtable sizes.
  * `--rocksdb.cache-index-and-filter-blocks-with-high-priority` to use cache
    index and filter blocks with high priority making index and filter blocks be
    less likely to be evicted than data blocks.
  * `--rocksdb.pin-l0-filter-and-index-blocks-in-cache` make filter and index
    blocks be pinned and only evicted from cache when the table reader is freed.
  * `--rocksdb.pin-top-level-index-and-filter` make the top-level index of
    partitioned filter and index blocks pinned and only be evicted from cache
    when the table reader is freed.

* Don't move potentially expensive AQL function calls into loops in the
  `remove-unnecessary-calculations-rule`.

  For example, in the query

      LET x = NOOPT(1..100)
      LET values = SORTED(x)
      FOR j IN 1..100
        FILTER j IN values
        RETURN j

  there is only one use of the `values` variable. So the optimizer can remove
  that variable and replace the filter condition with `FILTER j IN SORTED(x)`.
  However, that would move the potentially expensive function call into the
  inner loop, which could be a pessimization.

  Now the optimizer will not move the calculation of values into the loop when
  it merges calculations in the `remove-unnecessary-calculations` optimizer
  rule.

* Fixed modification executors inside of a subquery, where the subquery decided
  to fetch all rows from upstream first and the amount of rows is higher then
  the batch size.

* Fixed reporting of skipped number of documents if we have a LIMIT x, 0 right
  after the modification.

* Added exceptions catching in agency callbacks.

* Fixed issue #11104 case with complex condition on second branch of OR
  operator.

* Fixed bad behavior when dropping followers. A follower should be dropped
  immediately when it is officially FAILED, not only after a longish timeout.

* Fixed a bug in CollectionNameResolver which could lead to an extended busy
  spin on a core when a collection was dropped, but documents of it still
  remained in the WAL.

* Fixed return value of fuerte::H1Connection in case of timeout.

* Usability improvements for arangodump and arangorestore:
  - when a dump is restricted to one or multiple collections using the
    `--collection` option of arangodump, warnings are issued for all specified
    collections that are not present in the database. Previously there were
    no warnings, so missing a collection by misspelling its name could easily
    go unnoticed.
  - when a restore is restricted to one or multiple collections using the
    `--collection` option of arangorestore, warnings are issued for all
    specified collections that are not present in the dump. Previously there
    were no warnings, so missing a collection by misspelling its name could
    easily go unnoticed.
  - when a dump was taken using the `--overwrite` option, there was no check
    that validated whether the encryption mode used in the existing dump
    directory was the same as the requested encryption mode. This could have led
    to dump directories with both encrypted and unencrypted files. This was only
    the case when using `--overwrite true`, which is not the default.
  - when a restore was performed using the `--encryption.keyfile` option, there
    was no check whether the to-be-restored files were actually encrypted.
    Now this check is enforced and arangorestore will bail out with an error if
    the requested encryption mode for restore is not the same as for the stored
    dump files.

* Fixed traversal issue: If you have a traversal with different minDepth and
  maxDepth values and filter on path elements that are larger then minDepth, in
  a way that a shorter path would match the condition, the shorter paths would
  in some cases not be returned, even if they are valid. e.g.

  FOR v, e, p IN 1..3 OUTBOUND @start Graph "myGraph"
    FILTER p.vertices[2].label != "foo"
    RETURN p

  In the above query, a path of length 1 would be valid. There p.vertices[2]
  does not exist => Evaluated to `null`. `null`.label is again evaluated to
  `null` => `null != "foo"` is true, so the path is valid.

* Fixed traversal issue: If you have a filter on the path that is based on a
  variable value, which could not be deduced as constant during runtime, in a
  sharded GeneralGraph the filter was not applied correctly.
  SmartGraphs and SingleServer traversals are not effected by this issue.
  Also OneShard traversals are not effected.

* Added `securityScheme`, `securityScope` and `security` methods to Foxx
  endpoints to allow defining Swagger security schemes.

* Foxx routes now always have a Swagger `operationId`. If the route is unnamed,
  a distinct operationId will be generated based on the HTTP method and URL.

* Fixed, if you have a collection access within a Subquery, where the main query
  is fully skipped and the "splice-subqueries" rule is active. The information
  of the skip was not transported correctly. This could cause incorrect counting
  reports.
  If splice-subqueries are disabled, or the main-query is only partly skipped,
  everything worked as expected.

* Expanded -std=c++17 flag to all compilers.

* Fixed issue ES-600: log out of LDAP automatically.

  Option `--ldap.allow-offline` was added with `false` value by default to let
  server use cached offline credentials in case of LDAP server communication was
  lost and ArangoDB can't prolong cache online.

* Include all requests to the web interface at endpoint `/_admin/aardvark` in
  a node's requests statistics. Previously, requests going to `/_admin/aardvark`
  were excluded from the request statistics if and only if the requested
  database was the `_system` database.


v3.7.1-alpha.2 (2020-05-27)
---------------------------

* Fixed an issue with truncate of a collection after a dbserver was restarted
  very quickly. This could block arangosync from making progress because the
  _jobs collection could no longer be truncated.

* Web UI is now using pre-compiled templates.

* Fixed issue #11652: Error: AQL: missing variable #2 (1) for node #6
  (FilterNode) while planning registers.

* Fixed internal issue #650: Added analyzer cache update before ArangoSearch
  view creation/update.

* Update OpenLDAP to 2.4.50.

* Fixed issue ES-554: failed to build filter while querying ArangoSearch view.

  This change provides global fix for ArangoSearch analyzers distribution and
  management within a cluster.

* Fixed issue ES-545: failing modification operations led to random crashes with
  "Index out of bound" error message.

* Fix edge cases in RocksDB primary index range lookups for operators >= and <.

* Fixed issue #11525: Address security vulnerability by updating Swagger-UI
  dependency (upgraded Swagger UI to 3.25.1).

* Updated arangosync to 0.7.5.


v3.7.1-alpha.1 (2020-05-15)
---------------------------

* Fixed ability to edit graph edge in Graph Viewer of web UI.

* Fixed issue #10371: For k-shortest-paths queries on certain graphs a condition
  to abort search was too eager and hence missed the shortest path between two
  vertices and returned a longer one.

* Added feature: Disjoint SmartGraphs

  SmartGraphs have been extended to a new subtype, called `Disjoint SmartGraphs`.
  A Disjoint SmartGraph prohibits edges between different SmartGraph components.
  In case the graph schema can be represented without the need of connected
  SmartGraph components, a Disjoint SmartGraph should be used as this knowledge
  can be used by the internal optimizer to gain even better query traversal
  execution times.

* Fixed issue ES-605: arangodump broken when using encryption.

* Fixed a lockup in dropCollection due to a mutex being held for too long.

* Add an optimizer rule that enables execution of certain subqueries on a DB
  Server. For this optimization to work, the subquery must contain exactly one
  DISTRIBUTE/GATHER pair and only access at most one collection.

  This proves particularly useful for traversals, shortest path, and k-shortest
  paths queries on disjoint smart graphs where the entire traversal is executed
  on the DB Server without involvement of a coordinator.

* ClusterInfo does its own updating of plan / current caches.

* Properly forward requests based on the transaction header.

* Fix a bug in Agency poll, which would sometimes return a wrong firstIndex.

* Made several improvements to AQL's internal register planning. This will
  improve query performance in several cases; mainly of large queries with many
  variables, and of spliced subqueries. This also fixes the github issues #10853
  and #11358 which reported performance degradation caused by subquery splicing.

* Fixed issue #11590: Querying for document by _key returning only a single
  seemingly random property on entity ("old", in this case).

  This fixes single-key document lookups in the cluster for simple by-key AQL
  queries, such as `FOR doc IN collection FILTER doc._key == @key RETURN doc`
  in case the document has either an "old" or a "new" attribute.

* Restored behavior of Foxx API documentation being expanded to show all routes
  rather than collapsing all sections by default.

* Add optimization for subqueries for which only the number of results matters.
  The optimization will be triggered for read-only subqueries that use a full
  collection scan or an index scan, without any additional filtering (early
  pruning or document post-filtering) and without LIMIT.

  It will help in the following situation:

      FOR doc IN ...
        LET count = COUNT(
          FOR sub IN subCollection
            FILTER sub._from == doc._id
            RETURN sub
        )
        ...

  The restrictions are that the subquery result must only be used with the
  COUNT/LENGTH function and not for anything else. The subquery itself must be
  read-only (no data-modification subquery), not use nested FOR loops nor LIMIT,
  nor a FILTER condition or calculation that requires accessing the document
  data. Accessing index data is supported for filtering, but not for further
  calculations.

  If the optimization is triggered, it will show up in the query execution plan
  under the name `optimize-count`.

* Integrated libiresearch log topic properly into ArangoDB logging system.

* Fix a bad deadlock because transaction cleanup was pushed with too low
  priority.

* Allow specifying graph names as unquoted string in an AQL graph traversal
  query, e.g. `FOR ... IN ... GRAPH abc`. Previously, the graph name had to be a
  bind parameter or a string enclosed in quotes.

* loadPlan and loadCurrent have been fixed to not miss out on increments.

* ClusterInfo and all data definition changes use the local agency cache.

* Coordinators and DB servers cache the agency's read key value store.

* Agency offers the new poll API to subscribe to the Raft log stream.

* Added option `--rocksdb.edge-cache` to toggle in-memory caching for edges. The
  option is turned on by default. This normally helps with performance in
  read-only and read-mostly workloads.

* Fixed a bug in Maintenance which could prevent collection creation from
  working (made CreateCollection action idempotent).

* Fix potential undefined behavior in some operations issued to the REST handler
  at `/_api/collection` in cluster mode.

* `--cluster.agency-prefix` marked as obsolete. Did never work and is not
  supported.

* Removed old AgencyComm.

* Improve continuation behavior of AQL queries. We post the continuation handler
  on lane CLUSTER_AQL instead of CLIENT_AQL. This leads to MEDIUM prio instead
  of LOW.

* Fixed a sleeping barber in fuerte. Added TLA+ model to prove that there is not
  another one hiding somewhere.

* Fix spurious bugs in `resilience_move` tests due to replication context of
  to-be-dropped collections lingering around until timeout.

* Fixed issue 11243: `db._isSystem()` returns true for a non-system DB.

* Fix REST API `/_admin/cluster/removeServer`.
  This also fixes the removal of servers via the web interface.

* Updated OpenSSL to 1.1.1g.

* Lower the default values for `--rocksdb.block-cache-size` and
  `--rocksdb.total-write-buffer-size` for systems with less than 1 GB of RAM.
  For these systems the default value were presumably too high and needed manual
  adjustment.

* Allow to override the detected number of available CPU cores via an environment
  variable ARANGODB_OVERRIDE_DETECTED_NUMBER_OF_CORES.

* Fix a bug in the optimizer which handled combinations of SmartJoins with
  query components that had to run on a coordinator wrong.

* Remove the scatter-arangosearch-view-in-cluster optimizer rule; it was
  integrated into scatter-in-cluster.

* Issue #11447: expose extra RocksDB options `--rocksdb.target-file-size-base`
  and `--rocksdb.target-file-size-multiplier` to make them configurable.

* Allow STARTS_WITH function to process several prefixes with min match count
  parameter.

* Added collection property changes into auditing.

* Fix creation of example graphs when `--cluster.min-replication-factor` is set
  and no replication factor was specified for a graph. In this case, the new
  graph will use the configured minimum replication factor.

* Show some index details in index overview of the web interface, e.g. the
  `minLength` attribute for fulltext indexes or the `expireAfter` attribute for
  TTL indexes.

* Added startup option `--server.validate-utf8-strings`.

  This option controls whether strings in incoming JSON or VPack requests are
  validated for UTF-8 conformity. If set to `true` (which is the default value),
  all strings are validated, and requests containing invalid UTF-8 data will be
  rejected. Setting the option to `false` will turn off the UTF-8 string
  validation for incoming requests. This mode should only be used for
  deployments that are known to already contain invalid UTF-8 data and to keep
  them operational until the wrong string encoding is fixed in the data.

* Improve continuation behavior of AQL queries. We post the continuation handler
  on lane CLUSTER_AQL instead of CLIENT_AQL. This leads to MEDIUM prio instead
  of LOW.

* When relaying requests to other coordinators in a load-balanced setup, don't
  forward the "http/1.1" HTTP header from the remote response. Forwarding that
  header in the same way as any other header is not required, as each response
  will store and set its status separately anyway. Adding it to the headers
  again will only cause confusion and some client applications to choke.

* Fixed issue #11104: `ANALYZER` function doesn't correctly process `!= OR`
  queries.

* Obsoleted startup option `--database.maximal-journal-size`. This option was
  useful for the MMFiles storage engine only, but did not have an effect with
  the RocksDB engine. Using this startup option is not an error, but has no
  effect anymore.

* Added `JACCARD` AQL function.

* storedValues property is removed from ArangoSearch link properties output.

* Added primarySortCompression property to ArangoSearch views.

* Added compression property to ArangoSearch view storedValues.

* Removed deprecated MMFiles storage engine and also the `arango-dfdb`(datafile
  debugger) executable that could be used to validate MMFiles datafiles.

  This change also obsoletes all MMFiles-specific startup options in the
  `--wal.*` section. Using these startup options is not an error, but has no
  effect anymore.

* Fixed a bug in the agency supervision, which ignored the `failoverCandidates`
  field.

* Added `INTERLEAVE` AQL function.

* Upgraded bundled RocksDB library to version 6.8.0.


v3.7.0 (2020-04-11)
-------------------

* Updated OpenSSL to 1.1.1f.

* Fixed a bug which occurred if a DB-Server was shut down exactly when it was
  supposed to resign from its leadership for a shard.

* Fix a bug in the agency supervision, which could declare an already FAILED
  dbserver temporarily as GOOD again after an agency leader change.

* Added overwrite mode "ignore" for document inserts. This mode allows ignoring
  primary key conflicts on insert when the target document already exists.

  The following overwrite modes now exist:

  - "ignore": if a document with the specified `_key` value already exists,
    nothing will be done, and no write operation will be carried out. The
    insert operation will return success in this case. This mode does not
    support returning the old or new document versions using the `returnOld`
    and `returnNew` attributes.
  - "replace": if a document with the specified `_key` value already exists,
    it will be overwritten with the specified document value. This mode will
    also be used when no overwrite mode is specified but the `overwrite`
    flag is set to `true`.
  - "update": if a document with the specified `_key` value already exists,
    it will be patched (partially updated) with the specified document value.

  The overwrite mode "ignore" can also be used from AQL INSERT operations by
  specifying it in the INSERT's `OPTIONS`, e.g.

  INSERT { _key: ..., .... } INTO collection OPTIONS { overwriteMode: "ignore" }

  Again, when the overwrite mode "ignore" is used from AQL, it does not support
  returning the old or new document versions. Using "RETURN OLD" in an INSERT
  operation that uses the "ignore" overwrite mode will trigger a parse error, as
  there will be no old version returned, and "RETURN NEW" will only return the
  document in case it was inserted. In case the document already existed,
  "RETURN NEW" will return "null".

  The main use case of inserting documents with overwrite mode "ignore" is to
  make sure that certain documents exist in the cheapest possible way.
  In case the target document already exists, the "ignore" mode is most
  efficient, as it will not retrieve the existing document from storage and not
  write any updates to it.


v3.7.0-preview.1 (2020-03-27)
-----------------------------

* Added AQL function `IN_RANGE`.

* Added startup option `--ssl.prefer-http1-in-alpn` to optionally let the server
  prefer HTTP/1.1 over HTTP/2 in ALPN protocol negotiations.

* Compilation issues with wrong cv-qualifiers and unnecessary temporary copying.


v3.7.0-alpha.2 (2020-03-20)
---------------------------

* Add DTRACE points to track a request through the infrastructure.

* Fixed issue #11275: indexes backward compatibility broken in 3.5+.

* Updated snowball dependency to the latest version.
  More stemmers are available. Built-in analyzer list is unchanged.

* Reactive REST API endpoint at `/_admin/auth/reload`, as it is called by DC2DC.

* Fix an endless loop in FollowerInfo::persistInAgency which could trigger a
  hanger if a collection was dropped at the wrong time.

* Updated LZ4 dependency to version 1.9.2.

* Fix cluster representation of the collection figures for RocksDB.

* Changed behavior for creating new collections in OneShard databases (i.e.
  databases with "sharding" attribute set to "single"):

  Previously it was allowed to override "distributeShardsLike" and
  "numberOfShards" for each new collection. The default values were "_graphs"
  and 1 and were not modified if the user did not alter them, but it was still
  possible to alter them.
  This is now (silenly) ignored. Any attempt to set any value for
  "distributeShardsLike" or "numberOfShards" for new collections in a OneShard
  database will silently be ignored. The collection will automatically be
  sharded like the sharding prototype and will have a single shard.

  The behavior has been adjusted to match the behavior when creating collections
  in a cluster started with `--cluster.force-one-shard true` option. Here any
  user-supplied values for "distributeShardsLike" or "numberOfShards" were
  always ignored.

  Now the behavior is identical for OneShard databases and databases in cluster
  started with `--cluster.force-one-shard true`.

  The web interface now also hides the "Distribute shards like" settings in this
  case, and makes the "Number of shards" input box read-only.

* Fix premature access to temporary path before a user-specified path was read
  from the config options.

* Rebuild UI and update swagger.

* Added ability to store values in ArangoSearch views.

* Added LIKE operator/function support to SEARCH clause.

* Added NGRAM_SIMILARITY and NGRAM_POSITIONAL_SIMILARITY functions for
  calculating Ngram similarity.

* Added ngram fuzzy search. Supported by NGRAM_MATCH filter function for SEARCH
  clause.

* Allow to override the detected total amount of memory via an environment
  variable ARANGODB_OVERRIDE_DETECTED_TOTAL_MEMORY.
 
* `splice-subqueries` optimization is not limited by any type of operation
  within the subquery any more. It can now be applied on every subquery and will
  be by default.
  However they may be a performance impact on some queries where
  splice-subqueries are not as performant as non-spliced subqueries. This is due
  to internal memory management right now and will be addressed in future
  versions. Spliced subqueries can be less performant if the query around the
  subquery is complex and requires lots of variables, or variables with large
  content, but the subquery itself does not require a lot of variables and
  produces many intermediate results s.t. good batching within the query does
  not pay off against memory overhead.

* Supervision to clean up zombie servers after 24h, if no responsibility for
  shards.

* Fix SORT RAND() LIMIT 1 optimization for RocksDB when only a projection of the
  attributes was used. When a projection was used and that projection was
  covered by an index (e.g. `_key` via the primary index), then the access
  pattern was transformed from random order collection seek to an index access,
  which always resulted in the same index entry to be returned and not a random
  one.

* Mark server startup options `--foxx.*`, `--frontend.*` and `--javascript.*` as
  single server and Coordinator only for documentation (`--dump-options`).

* Supervision hot backup and supervision maintenance modes ttl fix.

* Fix a bug that leads to graph traversals yielding empty output when none of
  the output variables (vertex, edge, path) are used. This is relevant when a
  query is only interested in a COUNT of the outputs, for example.

* MoveShard to check, if target is in sync follower before promotion to leader.

* Agency ttl bug fix.

* Added the SNI feature for TLS. This means that one can configure multiple
  server keys and certificate chains and the system dynamically uses the right
  one depending on the value of the TLS servername extension.
  This allows to use different TLS setups for the same server which is reachable
  behind different DNS names, for example (Enterprise Edition only).

* Do not create a reboot tracker for empty serverId ubin sync repl.

* Fix the usage of the AQL functions `CALL` and `APPLY` for calling user-defined
  AQL functions when invoking an AQL query from the arangosh or a client
  application. Previously, invoking an AQL query and using the `CALL` or `APPLY`
  AQL functions to call user-defined AQL function caused undefined behavior.

* Improved graph traversal performance via some internal code refactoring:

  - Traversal cursors are reused instead of recreated from scratch, if possible.
    This can save lots of calls to the memory management subsystem.
  - Unnecessary checks have been removed from the cursors, by ensuring some
    invariants.
  - Each vertex lookup needs to perform slightly less work.

  The traversal speedups observed by these changes alone were around 8 to 10%
  for single-server traversals and traversals in OneShard setups. Cluster
  traversals will also benefit from these changes, but to a lesser extent. This
  is because the network roundtrips have a higher share of the total query
  execution times there.

* Traversal performance can also be improved by not fetching the visited
  vertices from the storage engine in case the traversal query does not refer to
  them.
  For example, in the query

      FOR v, e, p IN 1..3 OUTBOUND 'collection/startVertex' edges
        RETURN e

  the vertex variable (`v`) is never accessed, making it unnecessary to fetch
  the vertices from storage. If this optimization is applied, the traversal node
  will be marked with `/* vertex optimized away */` in the query's execution
  plan output.

* The existing optimizer rule "move-calculations-down" is now able to also move
  unrelated subqueries beyond SORT and LIMIT instructions, which can help avoid
  the execution of subqueries for which the results are later discarded.
  For example, in the query

    FOR doc IN collection1
      LET sub1 = FIRST(FOR sub IN coll2 FILTER sub.ref == doc._key RETURN sub)
      LET sub2 = FIRST(FOR sub IN coll3 FILTER sub.ref == doc._key RETURN sub)
      SORT sub1
      LIMIT 10
      RETURN { doc, sub1, sub2 }

  the execution of the `sub2` subquery can be delayed to after the SORT and
  LIMIT, turning it into

    FOR doc IN collection1
      LET sub1 = FIRST(FOR sub IN coll2 FILTER sub.ref == doc._key RETURN sub)
      SORT sub1
      LIMIT 10
      LET sub2 = FIRST(FOR sub IN coll3 FILTER sub.ref == doc._key RETURN sub)
      RETURN { doc, sub1, sub2 }

* Added JSON-Schema (draft-4) document validation. The validation can be
  specified by providing the new `schema` collection property when creating
  a new collection or when updating the properties of an existing collection.

  db.mycollection.properties({
    schema: {
      rule : { a : { type : "array", items : { type : "number", maximum : 6 }}},
      message : "Json-Schema validation failed"
    }
  });

* Fix supervision mode detection when unlocking agency in hot backup.

* Foxx Response#throw will no longer generate incomplete error objects when
  passed invalid arguments.

* Foxx will now always generate a numeric HTTP status code even if the response
  status code has been set to a non-numeric value.

* Make arangobench return a proper error message when its initial attempt to
  create the test collection fails.

* Added traversal options `vertexCollections` and `edgeCollections` to restrict
  traversal to certain vertex or edge collections.

  The use case for `vertexCollections` is to not follow any edges that will
  point to other than the specified vertex collections, e.g.

      FOR v, e, p IN 1..3 OUTBOUND 'products/123' components
        OPTIONS { vertexCollections: [ "bolts", "screws" ] }

  The traversal's start vertex is always considered valid, regardless of whether
  it is present in the `vertexCollections` option.

  The use case for `edgeCollections` is to not take into consideration any edges
  from edge collections other than the specified ones, e.g.

      FOR v, e, p IN 1..3 OUTBOUND 'products/123' GRAPH 'components'
        OPTIONS { edgeCollections: [ "productsToBotls", "productsToScrews" ] }

  This is mostly useful in the context of named graphs, when the named graph
  contains many edge collections. Not restricting the edge collections for the
  traversal will make the traversal search for edges in all edge collections of
  the graph, which can be expensive. In case it is known that only certain edges
  from the named graph are needed, the `edgeCollections` option can be a handy
  performance optimization.

* Make arangobench return a proper error message when its initial attempt to
  create the test collection fails.

* In some cases with a COLLECT LIMIT situation on a small limit the collect does
  more calls to upstream than without a limit to provide the same result. We
  improved this situation and made sure that LIMIT does not cause the COLLECT to
  fetch too few input rows. There is a chance that queries with a very small
  amount of data might suffer from this modification, most queries will benefit
  however.

* Use OpenSSL's EVP interface for SHA256 instead of the deprecated low-level
  message digest APIs.

* Prevent startup error "specifiy either '--server.jwt-secret-keyfile' or
  '--server.jwt-secret-folder' but not both." from occurring when specifying the
  JWT secret keyfile option.

* Honor `cacheEnabled` attribute when updating the properties of a collection
  in case of the cluster. Previously, this attribute was ignored in the cluster,
  but it shouldn't have been.

* Fixed issue #11137: http-proxy can't pass delete method req.body to foxx.

* Disable "collect-in-cluster" AQL optimizer rule in case a LIMIT node is
  between the COLLECT and the source data. In this case it is not safe to apply
  the distributed collect, as it may alter the results.


v3.7.0-alpha.1 (2020-02-19)
---------------------------

* Rebuild UI.

* Fix arangorestore:

  If a smartGraphAttribute value was changed after document creation,
  the _key of the document becomes inconsistent with the smartGraphAttribute
  entry. However, the sharding of documents stays intact.

  When restoring data from a dump that contains a document with inconsistent
  _key and smartGraphAttribute this would cause an error and the document
  would not be inserted. This is fixed by ignoring this inconsistency in
  the case of restore.

* Updated rclone to 1.51.0.

* Fixed a memory leak in ModificationExecutors.

* Updated arangosync to 0.7.3.

* Updated ArangoDB Starter to 0.14.14.

* Fixed handling of `geometry` attributes in query editor of the web UI.

  Previously, document attributes named `geometry` were treated in a special
  way, assuming they were JSON objects. In case a document contained a
  `geometry` attribute that had a non-object value (e.g. `null`) the web UI
  threw a JavaScript exception and would not display AQL query results properly.

* Disable "collect-in-cluster" AQL optimizer rule in case a LIMIT node is
  between the COLLECT and the source data. In this case it is not safe to apply
  the distributed collect, as it may alter the results.

* Fixed internal issue #4932: COLLECT WITH COUNT together with FILTER yields
  zero.

  This bugfix fixes an issue when skipping over documents in an index scan 
  using a covering index and also at the same time using an early-pruning
  filter.

  In this case wrong document data may have been injected into the filter 
  condition for filtering, with the filter wrongfully deciding which documents
  to filter out.

* Added crash handler for Linux builds that taps into the following signals:
  - SIGSEGV (segmentation violation)
  - SIGBUS (bus error)
  - SIGILL (illegal instruction)
  - SIGFPE (floating point exception)

  In case the arangod process catches one these signals, the crash handler
  tries to log a message and a backtrace into the installation's logfile before
  terminating the process.

* Removed dependency on libcurl library.

* Remove storage engine selection from package installers. The default storage
  engine for new installations is now RocksDB. The MMFiles storage engine cannot
  be selected for new installations.

* Remove misleading error messages about upgrade commands to carry out in
  case of a database upgrade. These commands are highly platform-dependent and
  also depend on whether ArangoDB is started manually, via the ArangoDB starter
  or as a service.
  In order to not confuse end users, remove the potentially misleading
  instructions.

* Clear backups from DB servers and agency, when plan unchanged not met and not
  allowing for inconsistency.

* V8 version upgrade to 7.9.317; ICU version upgrade to 64.2.
  - JSON parsing is roughly 60% faster than in V8 7.1; you should prefer 
    JSON.parse(string) over deeply nested javascript variable declarations.
  - several new javascript language features

* Clean out server job checks preconditions plan version unchanged for start.

* Cluster collection creation preconditioned on involved db servers not in
  process of being cleaned and Fixed.

* Fixed a bug, where a single host agency logged too early.

  This issue caused spurious "Could not integrate into cluster" error messages
  when starting coordinators.

* Fixed issue #11051: AQL query wrongfully returning no results on 3.6.1.

  This change prevents the "late-document-materialization" optimizer rule from
  kicking in in cases when an index scan also uses early pruning of index
  entries. In this case, the late document materialization will lead to the
  filter condition used in early pruning not being applied correctly,
  potentially producing wrong results.

* Fixed internal issue #596: Added ability to disable DNF conversion in SEARCH
  condition for ArangoSearch views. As this conversion might result in
  overcomplicated disjunction and heavily slowdown execution.

* Implement a new API to reload TLS server key and certificates as well as
  client certificate CA. This allows to rotate TLS certificates without a
  restart. One can also query the currently loaded state of the TLS data to
  decide if a reload has actually happened. The new certificates will only be
  used for new TLS-based connections.

* Add acquisition of system report to arangod instances.

* JS `query` helper function can now be used with options.

* Fix a bug in collection creation with `distributeShardsLike`.

* Added load-balancing support for listing currently running and slow AQL
  queries, killing running AQL queries and clearing the list of slow AQL
  queries.

  This change also will also modify the values returned in the "id" attribute of
  AQL query objects. While the values of the "id" attribute remain strings with
  numeric content, the ids will now start at arbitrary offsets after server
  start and are supposed to have much higher numeric values than in previous
  ArangoDB versions.
  In previous ArangoDB versions, query ids always started at value 1 after a
  server start/restart and were increased in increments of 1.
  This change may lead to query ids being greater than what a 4 byte integer can
  hold, which may affect client applications that treat the ids as numeric
  values and do not have proper support for integer numbers requiring more than
  4 byte storage.

* remove unused REST API /_admin/aql/reload, and make the JavaScript bindings
  for `internal.reloadAqlFunctions()` do nothing. The reason for this is that
  AQL functions are automatically reloaded when functions are changed, and
  there is no need to call these APIs manually.

* Fixed issue #10949: k shortest paths behavior wrong with zero weights.

* Added `REPLACE_NTH` AQL function to replace a member inside an array.

* Added JWT secret rotation (Enterprise Edition only).

* The `--dump-options` command for arangod now also provides the "components"
  attribute that shows for which components (agent, coordinator, db server,
  single server) an option is relevant. Additionally, each option provides an
  "os" attribute that indicates on which operating systems the option is
  supported.
  This can be used when reading the options descriptions programmatically, e.g.
  for auto-generating the documentation.

* Add update-insert operation similar to existing replace-insert functionality
  of insert.

  Like for the existing variant the `overwrite` flag has to be set to true.
  Then the update version can be selected by setting the `overwriteMode` to
  `"update"`.


v3.6.1 (2020-01-29)
-------------------

* Updated ArangoDB Starter to 0.14.13.

* Added HotBackup events into auditing.

* Internal statistics API now uses integer timestamps instead of doubles. The
  old behavior sometimes leads to failed requests because of parse errors
  which occurred in the internally used JavaScript joi library.

* Fixed issue #10896: Variables defined inside spliced subqueries would leak
  into following COLLECT ... INTO var operations.

* Fixed COLLECT not invalidating variables for following COLLECT ... INTO var
  operations.

* Removed an unnecessary and wrong request from within the Web UI to the
  `/_admin/cluster/health` API. This lead to unauthorized network calls.

* Allowed PHRASE function to process empty arrays without generating error.

* Add acquisition of system report to arangod instances.

* React dev mode now supports hot reload combined with proxy for development.

* Fix issue #10897, using COLLECT in a subquery could lead to unexpected and
  confusing error messages

* Fix potential nullptr dereference in view optimizer rule, when there were was
  a LIMIT outside of a FOR loop.

* Added missing "global" parameter in Swagger REST API documentation for some
  replication endpoints.

* Fixed an edge case in VelocyPack when the padding of a 1-byte offsetSize array
  is removed but the first few entries of the array contain a Slice of type
  None.

* Fix Foxxmaster failover retry loop to not spin forever and give up after a
  while.

* Fix "random" Valgrind "invalid free/delete" errors caused by usage of
  `alignas(64)` for shared_ptrs in the SupervisedScheduler.

* MoveShard to check, if target is in sync follower before promotion to leader.

* Fixed issue #10867: arangod based binaries lack product version info and icon
  on Windows

* Fixed internal traversal edge collection cache being filled up correctly.
  Edges are able to point to other edges, but those we're not applied to the
	cache.

* Fixed issue #10852: Nested spliced subqueries could return wrong results in
  some cases when some of the concerned subqueries did not return any values.

* Fix string comparison bug that lead to traversal queries accepting prefixes of
  "edges" and "vertices" to be used as object accessors for the path object.

* Fix bug affecting spliced subqueries when memory blocks are reused.

* Now clearing an internal map inside the traverser engine correctly.

* Add ability to choose logging to file in Windows installer of ArangoDB server
  (enabled by default).

  ArangoDB-logs folder with arangod.log should be stored in %PROGRAMDATA% and
  %LOCALAPPDATA% for all users and single user installation respectively.

* Fix installation of arangoinspect and libraries in Windows client installer.

* Disable cluster AQL parallelization for queries that contain traversal,
  shortest path or k_shortest_path nodes.
  This avoids potential undefined behavior in case a parallel GatherNode is used
  in graph queries.

* Fixed internal issue #656: While executing large amount of concurrent
  insert/removes iresearch seems to leak open file handles. This results in
  error 32 in cleanup (observed only on Windows as Linux doesn`t have file
  sharing locks).

* Updated arangosync to 0.7.2.

* Fix Windows client package JS installation paths.

* Added option that makes ArangoDB write logfiles in the Windows installer.


v3.6.0 (2020-01-08)
-------------------

* Do not create a reboot tracker for empty serverId ubin sync repl.

* Update swagger.


v3.6.0-rc.2 (2019-12-23)
------------------------

* Fixed issue #10725: Wrong document count shown inside an empty collection in
  the web UI.

* Fixed bug that made AQL queries eligible for parallelization even in case
  they couldn't be parallelized, leading to undefined behavior due to the
  thread races.

* Fixed bug in `--query.parallelize-gather-writes-behavior` which led to
  the option not working correctly.

* Agency relational operators TTL fix.

* Improve latency in scheduler.

* Fixed internal issue #4748: Editing a single edgeDefinition using the graph
  API failed if it was not shared between all available graphs.

* Fixed undefined behavior on node delete.

* Fixed agency invalid operation.

* Added google tests for Node.

* Bugfix: An AQL ternary expression with the condition being true at query
  compile time would not execute its false branch.

* When starting the RocksDB engine, first create small-sized WAL files.

  This is a precaution so that when repeatedly trying to start an arangod
  instance, an instant instance startup failure will not lead to the disk
  filling up so quickly with WAL file data.

  The WAL file size is increased later on in the startup sequence, so
  everything should be fine if the startup works.

  This fixes a problem with the disk filling up quickly when the arangodb 
  starter tries to start an instance 100 times in a row but instantaneously 
  gets a failure back from it.

* Fixed a permissions bug for /_admin/cluster/rebalanceShards.


v3.6.0-rc.1 (2019-12-10)
------------------------

* Renamed document / index / vertex "handle" to "identifier" / "id" for
  consistency in documentation and error messages.

* Fixed a bug in smart graph bfs traversals that might violate path uniqueness
  requirements in rare cases.

* Fixed permissions for dump/restore.

* Rename `minReplicationFactor` into `writeConcern` to make it consistent with
  `--cluster.write-concern` and avoid confusion with
  `--cluster.min-replication-factor`

* ArangoDB web UI switched build system from grunt to webpack.
  It supports React now, but the previous framework is still in use.

* Enable the `parallelize-gather` AQL optimizer rule for certain write queries.
  The optimization is turned on by default and can be disabled by setting the
  startup option `--query.parallelize-gather-writes` to `false`.

* Shard synchronization readlock aware of rebootId.

* Bugfix: In an AQL cluster query, when gathering unsorted data in combination
  with a LIMIT with non-zero offset, if this offset exactly matches the number
  of documents in the first shards consumed, the rest of the documents was not
  returned.

* REMOTE and GATHER no longer make subqueries unsuitable for the
  `splice-subqueries` optimization.

* New internal counter and histogram support.

* Add a Prometheus endpoint for metrics, expose new metrics, old statistics
  and RocksDB metrics.

* Fixed known issue #509: ArangoSearch index consolidation does not work during
  creation of a link on existing collection which may lead to massive file
  descriptors consumption.

* Added support of array comparison operators to ArangoSearch.

* Added support of arrays to TOKENS function.

* Added support of arrays to PHRASE function.

* Added a new optimization called `late-document-materialization`,
  `late-document-materialization` for indexes and views correspondingly.

  This optimization reduces amount of documents to read from storage engine to
  the limit explicitly stated in LIMIT node.

* Added support of "Edge N-grams" to `text` analyzer.

* Added support of UTF8 to `ngram` analyzer.

* Added ability to mark beginning/end of the sequence to `ngram` analyzer.

* Added a new subquery optimization called `splice-subqueries`.

  This optimization splices the execution of subqueries inside the
  execution of their surrounding query. For subqueries with a fairly
  small number of returned documents this saves a significant amount of
  overhead initializing cursors.

  This optimization is performed by the optimizer rule
  `splice-subqueries`, and currently only works on suitable
  subqueries; A subquery becomes unsuitable if it contains a LIMIT,
  REMOTE, GATHER or a COLLECT node where the operation is WITH COUNT INTO.
  A subquery also becomes unsuitable if it is contained in an
  (sub)query which contains unsuitable operations after the subquery.

  The implementation of this feature required reworking the dataflow
  query execution.

* Added startup option `--query.optimizer-rules` to selectively enable or
  disable optimizer rules by default. The option can be specified multiple
  times, and takes the same input as the query option of the same name. 

  For example, to turn off the rule _use-indexes-for-sort_, use

      --query.optimizer-rules "-use-indexes-for-sort"

  The purpose of this option is to be able to enable potential future
  experimental optimizer rules, which may be shipped in a disabled-by-default
  state.

* Allow the AQL query optimizer to remove the DistributeNodes for several
  data-modification queries. So far, only REMOVE queries benefitted. Now the
  optimization can also be applied for REPLACE and UPDATE queries in case the
  query does not use LIMIT and there is no further cluster-internal
  communication after the REMOVE, REPLACE or UPDATE node. 

* Include ArangoSearch data in HotBackups.

* Allow to restore 3.5 HotBackups in 3.6.

* Fixed an issue where removeServer left behind current coordinators

* Allow usage of AQL function `RANDOM_TOKEN` with an argument value of `0`. This
  now produces an empty string, whereas in older versions this threw an invalid
  value exception.

* Add startup option `--rocksdb.exclusive-writes` to avoid write-write
  conflicts.

  This options allows for an easier transition from MMFiles to the RocksDB
  storage engine, but comes with a big performance penalty as all collections
  will be locked exclusively for writes.

* Added new maxRuntime option for queries. If a query does not finish execution
  within the given time (in seconds) it will be killed.

* Added limit for AQL range materialization to prevent out-of-memory errors.

  When materializing ranges created by either the AQL `RANGE` function or by
  using the built-in `..` operator (e.g. `1 .. 1000000`), a check is now
  performed if the range is too big to be materialized. The threshold value is
  set to 10 million members. Ranges with at most that many members can be
  materialized, ranges with more members will fail to materialize and abort the
  query with the exception `number out of range` (error code 1504).

  It is still possible to create ranges with more than 10 million members as
  long as they are not materialized. For example, the following is still valid:

      FOR i IN 1 .. 1000000000 INSERT {_key: CONCAT('test', i)} INTO collection

* No longer put system services into `_apps` on single server. On cluster, this
  has never worked. This was unnecessary.

* Added AQL optimizer rule "move-filters-into-enumerate", to allow for early
  pruning of non-matching documents while full-scanning or index-scanning
  documents. This optimization can help to avoid a lot of temporary document
  copies.

* Added "SmartJoins for Views" to the ArangoDB Enterprise Edition that allows
  running cluster joins between two certain sharded collections or views with
  performance close to that of a local join operation.

* Allow collection names to be at most 256 characters long, instead of 64
  characters in previous versions.

* Upgraded bundled Boost library version to 1.71.

* Use `-std=c++17` for ArangoDB compilation.

* Made the mechanism in the Web UI of replacing and upgrading a foxx app more
  clear.

* Show shards of all collections (including system collections) in the web UI's
  shard distribution view.

  This is necessary to access the prototype collections of a collection sharded
  via `distributeShardsLike` in case the prototype is a system collection, and
  the prototype should be moved to another server.

* Rclone URL normalization.

* Disallow using `_id` or `_rev` as shard keys in clustered collections.

  Using these attributes for sharding was not supported before, but didn't
  trigger any errors. Instead, collections were created and silently using
  `_key` as the shard key, without making the caller aware of that an
  unsupported shard key was used.

* Use execvp instead of execv in HotBackup restore.

* Re-enabled the AQL sort-limit optimization rule in conjunction with fullCount
  in the cluster. It now also may speed up fullCount with sorted indexes and a
  limit.

* Make the scheduler enforce the configured queue lengths. The values of the
  options `--server.scheduler-queue-size`, `--server.prio1-size` and
  `--server.maximal-queue-size` will now be honored and not exceeded.

  The default queue sizes in the scheduler for requests buffering have
  also been changed as follows:

      request type        before      now
      -----------------------------------
      high priority          128     4096
      medium priority    1048576     4096
      low priority          4096     4096

  The queue sizes can still be adjusted at server start using the above-
  mentioned startup options.

* Add replicationFactor, minReplicationFactor and sharding strategy to database
  creation dialog in web UI. Preselect database default values for collection
  creation in web UI.

* Add new JavaScript function `db._properties()` that provides information about
  the current database's properties.

* Add new options `sharding` and `replicationFactor` for database creation
  methods. The specified values will provide the defaults for all collections
  created in a database.

  Valid values for `sharding` are `""`, "flexible", "single". The first 2 values
  are treated equally. Values for `replicationFactor` are natural numbers or the
  string `satellite`.

* Add new server option `--cluster.default-replication-factor` that allows to
  set the default replication factor for non-system collections (default: 1).

* Enabled IPO with cmake as an option, default is on for release builds without
  google tests.

* Bugfix: The AQL sort-limit optimization was applied in some cases it
  shouldn't, resulting in undefined behavior.

* Remove operations for documents in the cluster will now use an optimization,
  if all sharding keys are specified. Should the sharding keys not match the
  values in the actual document, a not found error will be returned.

* Retry hot backup list in cluster for 2 minutes before reporting error.

* Allowing inconsistent rather than forcing hot backups.

* Geo functions will now have better error reporting on invalid input.

* Upgraded bundled jemalloc library to version 5.2.1.

* Added TransactionStatistics to ServerStatistics (transactions started /
  aborted / committed and number of intermediate commits).

* Added AQL function DATE_ROUND to bin a date/time into a set of equal-distance
  buckets.

* Enforced the valid date range for working with date/time in AQL. The valid
  date ranges for any AQL date/time function are:

  - for string date/time values: `"0000-01-01T00:00:00.000Z"` (including) up to
    `"9999-12-31T23:59:59.999Z"` (including)
  - for numeric date/time values: -62167219200000 (including) up to
    253402300799999 (including). These values are the numeric equivalents of
    `"0000-01-01T00:00:00.000Z"` and `"9999-12-31T23:59:59.999Z"`.

  Any date/time values outside the given range that are passed into an AQL date
  function will make the function return `null` and trigger a warning in the
  query, which can optionally be escalated to an error and stop the query.

  Any date/time operations that produce date/time outside the valid ranges
  stated above will make the function return `null` and trigger a warning too.
  An example for this is.

      DATE_SUBTRACT("2018-08-22T10:49:00+02:00", 100000, "years")

* Fixed bug in MoveShard::abort which causes a duplicate entry in the follower
  list. (Internal Bug #4378)

* Updated TOKENS function to deal with primitive types and arrays.


v3.5.3 (2019-11-28)
-------------------

* Fixed GET _api/gharial to also include the name property in every returned
  graph. This is a consistency fix within the API as all other APIs include the
  name. As a work around the returned _key can be used, which is identical to
  the name.

* The _users collection is now properly restored when using arangorestore.

* Allow the optimizer to use indexes when a collection attribute is compared to
  an expansion followed by an attribute name, e.g.
  `doc.value IN something[*].name`.

* Updated arangosync to 0.7.0.

* Fixed issue #10470: The WebUI now shows potential errors and details which
  occured using _api/import (e.g. unique constraint violated).

* Fixed issue #10440: Incorrect sorting with sort criteria partially covered 
  by index.

* Make the timeouts for replication requests (for active failover and master-slave 
  replication configurable via startup options:

      --replication.connect-timeout
      --replication.request-timeout

* Fixed internal issue #4647: dead Coordinators are not removed for agency.

* Fixed UPSERT matching.

  Empty objects in the `UPSERT { ... }` expression will not be omitted anymore:

      db._collection("col").insert({ "find" : "me" });
      db._query(` UPSERT { "find" : "me", "foo" : {} }
                  UPDATE { "foo"  : "not gonna happen" }
                  INSERT { "find" : "me", "foo"  : {} }
                  INTO col
      `)

  This will now correctly insert a document instead of updating the existing,
  that only partially matches the upsert-expression.

* Fixed undefined behaviour with creation of ArangoSearch links with custom
  analyzers in cluster environment.

* Fixed internal issue #651: analyzer duplication in _analyzers collection.

* Fixed internal issue #4597: rebalanceShards API cannot work on any database
  other than the _system database.

* Stop putting system services in _apps on single server, this has never
  worked on cluster and was not needed.

* Fixed issue #10371: K_SHORTEST_PATHS LIMIT 1 can not return the shortest path.
  Now the shortest path is returned as the first one in such queries.

* Improve killability of some types of cluster AQL queries. Previously, several
  cluster queries, especially those containing a `DistributeNode` in their
  execution plans, did not respond to a kill instruction.

  This change also introduces a new query status "killed", which may now be
  returned by the REST APIs at `/_api/query/current` and `/_api/query/slow` in
  the `state` attribute of each query.

* Improve shutdown of some cluster AQL queries on the coordinator in case the
  query has multiple coordinator snippets (true for queries involving more than
  one collection) and the database server(s) cannot be reached on query
  shutdown. In this case the proper shutdown of the coordinator parts of the
  query previously was deferred until the coordinator snippets were removed by
  the automatic garbage collection. Now, the cleanup of the coordinator snippets
  will happen much more quickly, which reduces the chances of the queries
  blocking resources.

* Fixed ArangoSearch index removes being discarded on commiting consolidation
  results with pending removes after some segments under consolidation were
  already committed.

* Assertion fail when no timestamp in agency's persistence.

* Fixed internal issue #647: custom analyzer provokes errors on Active Failover
  deployment.

* Upgraded bundled version of libcurl to 7.66.0.
* When starting a coordinator, wait up to 15 seconds for it to appear
  in the agency under key `Supervision/Health` before reporting as "ready".
  This is necessary because if the coordinator reports ready beforehand
  and is used to create databases etc., the supervision may remove all
  of the jobs started by non-ready coordinators, considering them to be
  from a failed coordinator.
  To avoid huge startup delays, the startup will proceed after waiting
  futilely for 15 seconds and log a message.

* Fixed issue #10270: Query: Expecting type Array or Object (while executing).

* Fix a problem with AQL constrained sort in the cluster, which might abort
  queries. The AQL sort-limit optimization rule may now also speed up fullCount
  with sorted indexes and a limit in the cluster.

* Prevent spurious log message "Scheduler queue is filled more than 50% in last
  x s" from occurring when this is not the case. Due to a data race, the
  message could previously also occur if the queue was empty.

* The General Graph document API is now consistent with the document API in its
  error messages. When attempting to create / modify edges pointing to non
  existing vertex collections HTTP 400 is returned instead of 404.

* Disallow the usage of subqueries inside AQL traversal PRUNE conditions.
  Using subqueries inside PRUNE conditions causes undefined behavior,
  so such queries will now be aborted early on with a parse error
  instead of running into undefined behavior.

* Fixed available flag for hotbackup.

* Fixed list with id for partially available hotbackups.

* Fixed agency TTL bug happening under certain rare conditions.

* Improved performance of some agency helper functions.

* Fixed search not working in document view while in code mode.

* Fixed issue #10090: fix repeatable seek to the same document in
  SEARCH operations for ArangoSearch views.

* Fixed issue #10193: Arangoexport does not handle line feeds when exporting as
  csv.

* Removed debug log messages "found comm task ..." that could be logged
  on server shutdown.

* Fixed issue #10183: Arangoimport imports on _system when you try to
  create a new database.

  This bugfix fixes the output of arangoimport, which could display a
  wrong target database for the import if the option `--create-database`
  was used.

* Fixed issue #10158: Invalid Query Crashes ArangoDB.

  This fixes traversal queries that are run on a static empty start vertex
  string.


v.3.5.2 (2019-11-06)
--------------------

* Fixed ArangoSearch upgrade procedure from previous minor version and
  patches.

* Separately account for superuser and user request traffic.


v3.5.1 (2019-10-07)
-------------------

* Properly report parse errors for extraneous unterminated string literals
  at the end of AQL query strings. For example, in the query `RETURN 1 "abc`,
  the `RETURN 1` part was parsed fully, and the `"abc` part at the end was
  parsed until the EOF and then forgotten. But as the fully parsed tokens
  `RETURN 1` already form a proper query, the unterminated string literal
  at the end was not reported as a parse error.
  This is now fixed for unterminated string literals in double and single
  quotes as well as unterminated multi-line comments at the end of the query
  string.

* Fix config directory handling, so we don't trap into UNC path lookups on Windows.

* Ignore symlinks when copying JavaScript files at startup via the option
  `--javascript.copy-installation`. This potentially fixes the following
  error message at startup:

      Error copying JS installation files to '...':
      failed to open source file ...: No such file or directory

* Added startup option `--cluster.max-number-of-shards` for restricting the
  maximum number of shards when creating new collections. The default
  value for this setting is `1000`, which is also the previously hard-coded
  built-in limit. A value of `0` for this option means "unrestricted".
  When the setting is adjusted, it will not affect already existing
  collections, but only collections that are created or restored
  afterwards.

* Added startup options for managing the replication factor for newly
  created collections:

  - `--cluster.min-replication-factor`: this settings controls the minimum
    replication factor value that is permitted when creating new collections.
    No collections can be created which have a replication factor value
    below this setting's value. The default value is 1.
  - `--cluster.max-replication-factor`: this settings controls the maximum
    replication factor value that is permitted when creating new collections.
    No collections can be created which have a replication factor value
    above this setting's value. The default value is 10.
  - `--cluster.default-replication-factor`: this settings controls the default
    replication factor value that is used when creating new collections and
    no value of replication factor has been specified.
    If no value is set for this option, the value of the option
    `--cluster.min-replication-factor` will be used.

* Fixed unintended multiple unlock commands from coordinator to
  transaction locked db servers.

* DB server locking / unlocking for hot backup revisited and enhanced.

* Rely on reboot ids for declaring end of cluster hot restore on coordinators.

* Obtain new unique IDs via a background thread.

* Fixed issue #10078: FULLTEXT with sort on same field not working.

* Fixed issue #10062: AQL: could not extract custom attribute.

* Fix compilation issue with clang 10.

* Fixed error message for error 1928 ("not in orphan") to "collection is
  not in list of orphan collections".

* Fix strange shutdown hanger which came from the fact that currently
  libgcc/libmusl wrongly detect multi-threadedness in statically linked
  executables.

* Fixed a shutdown bug coming from a read/write lock race.

* Fixed a bug in the edge cache's internal memory accounting, which led
  to the edge cache underreporting its current memory usage.

* Fixed "ArangoDB is not running in cluster mode" errors in active failover setups.
  This affected at least /_admin/cluster/health.

* Made the mechanism in the Web UI of replacing and upgrading a foxx app more clear.

* Fixed the AQL sort-limit optimization which was applied in some cases it should
  not, resulting in undefined behaviour.

* Add --server.statistics-history flag to allow disabling of only the historical
  statistics.  Also added rocksdbengine.write.amplification.x100 statistics
  for measurement of compaction option impact.  Enabled non-historical
  statistics for agents.

* Fixed AQL constrained-heap sort in conjunction with fullCount.

* Added support for AQL expressions such as `a NOT LIKE b`, `a NOT =~ b` and
  `a NOT !~ b`. Previous versions of ArangoDB did not support these expressions,
  and using them in an AQL query resulted in a parse error.

* Disallow creation of TTL indexes on sub-attributes.

  Creation of such indexes was not caught before, but the resulting
  indexes were defunct. From now on the creation of TTL indexes on sub-
  attributes is disallowed.

* Added HotBackup feature.

* Improved database creation within the cluster. In the case of
  coordinator outages during the creation of the database there was a
  chance that not all relevant system collections had been created.
  Although this database was accessible now some features did not work
  as expected (e.g. creation of graphs). We modified creation of a new
  database as an all or nothing operation and only allow access to the
  database after all system collections are properly perpared to avoid
  the above inconsistencies. Also creation of databases are now secured
  against coordinator outages, they will either be fully created or not
  visible and eventually dropped. This does not require any change on the
  client code.

* Added UI support to create documents in a collection using smartGraphAttribute
  and/or smartJoinAttribute.

* Add count of objects to latency reporting in arangoimport.

* Harden database creation against spurious "duplicate name" errors that
  were caused by other parallel operations lazily creating required
  system collections in the same database.

* Fixed internal issue #633: made ArangoSearch functions BOOST, ANALYZER, MIN_MATCH
  callable with constant arguments. This will allow running queries where all arguments
  for these functions are deterministic and do not depend on loop variables.

* Automatically turn values for deprecated startup option parameters
  `--log.use-microtime` and `--log.use-local-time` into equivalent option values
  of the new, preferred option `--log.time-format`.

* Drop collection action to timeout more quickly to stay on fast lane.

* Make arangorestore restore data into the `_users` collection last. This is to
  ensure that arangorestore does not overwrite the credentials of its invoker while
  the restore is running, but only at the very end of the process.
  This change also makes arangorestore restore the `_system` database last if it
  is started with the `--all-databases` option.

* Fixed the removal (including a collection drop) of an orphanCollection from a
  graph definition when using the arango shell. The boolean flag whether to drop
  the collection or not was not transferred properly.

* Check for duplicate server endpoints registered in the agency in sub-keys of
  `/Current/ServersRegistered`.

  Duplicate endpoints will be registered if more than one arangod instance is
  started with the same value for startup option `--cluster.my-address`. This can
  happen unintentionally due to typos in the configuration, copy&paste remainders etc.

  In case a duplicate endpoint is detected on startup, a warning will be written
  to the log, indicating which other server has already "acquired" the same endpoint.

* Make graph operations in general-graph transaction-aware.

* Fixed adding an orphan collections as the first collection in a SmartGraph.

* Fixed non-deterministic occurrences of "document not found" errors in sharded
  collections with custom shard keys (i.e. non-`_key`) and multi-document lookups.

* Fixed issue #9862: ServerException: RestHandler/RestCursorHandler.cpp:279

  This fixes an issue with the RocksDB primary index IN iterator not resetting its
  internal iterator after being rearmed with new lookup values (which only happens
  if the IN iterator is called from an inner FOR loop).

* Geo functions will now have better error reporting on invalid input.

* The graph viewer of the web interface now tries to find a vertex document of
  all available vertex collections before it aborts.

* Fixed issue #9795: fix AQL `NOT IN` clause in SEARCH operations for ArangoSearch
  views.

* Make minimum timeout for synchronous replication configurable via parameter
  (--cluster.synchronous-replication-timeout-minimum) and increase default value
  to prevent dropping followers unnecessarily.

* Added support for TLS 1.3 for the arangod server and the client tools.

  The default TLS protocol for the arangod server is still TLS 1.2 however, in order
  to keep compatibility with previous versions of ArangoDB.

  The arangod server and any of the client tools can be started with option
  `--ssl.protocol 6` to make use of TLS 1.3.

  To configure the TLS version for arangod instances started by the ArangoDB starter,
  one can use the `--all.ssl.protocol=VALUE` startup option for the ArangoDB starter,
  where VALUE is one of the following:

  - 4 = TLSv1
  - 5 = TLSv1.2
  - 6 = TLSv1.3

* Fixed parsing of "NOT IN" in AQL, which previously didn't correctly parse
  "NOT IN_RANGE(...)" because it checked if the "NOT" token was followed by
  whitespace and then the two letters "IN".

* Changed log level for message "keep alive timeout - closing stream!" from INFO to
  DEBUG.

* Don't create temporary directories named "arangosh_XXXXXX" anymore.

* Add TransactionStatistics to ServerStatistics (transactions started /
  aborted / committed and number of intermediate commits).

* Upgraded version of bundled curl library to 7.65.3.

* Don't retry persisting follower information for collections/shards already
  dropped. The previous implementation retried (unsuccessfully in this case)
  for up to 2 hours, occupying one scheduler thread.

* Fixed internal issue #4407: remove storage engine warning.

* Agents to remove callback entries when responded to with code 404.

* Fixed internal issue #622: Analyzer cache is now invalidated for dropped database.

* Show query string length and cacheability information in query explain output.

* The AQL functions `FULLTEXT`, `NEAR`, `WITHIN` and `WITHIN_RECTANGLE` are now
  marked as cacheable, so they can be used in conjunction with the AQL query
  results cache on a single server.

* Fixed issue #9612: fix ArangoSearch views getting out of sync with collection.

* Fix an issue with potential spurious wakeups in the internal scheduler code.

* Changes the _idle_ timeout of stream transactions to 10 seconds and the total
  per DB server size of stream transaction data to 128 MB. The idle timer is
  restarted after every operation in a stream transaction, so it is not the
  total timeout for the transaction.

  These limits were documented in the manual for stream transactions since 3.5.0,
  but are enforced only as of 3.5.1. Enforcing the limits is useful to free up
  resources from abandoned transactions.

* Consistently honor the return value of all attempts to queue tasks in the
  internal scheduler.

  Previously some call sites did not check the return value of internal queueing
  operations, and if the scheduler queue was full, operations that were thought
  to be requeued were silently dropped. Now, there will be reactions on such
  failures. Requeuing an important task with a time offset (Scheduler::queueDelay)
  is now also retried on failure (queue full) up to at most five minutes. If after
  five minutes such a task still cannot be queued, a fatal error will be logged
  and the server process will be aborted.

* Made index selection much more deterministic in case there are
  multiple competing indexes.

* Fixed issue #9654: honor value of `--rocksdb.max-write-buffer-number` if it
  is set to at least 9 (which is the recommended value). Ignore it if it is
  set to a lower value than 9, and warn the end user about it.

  Previous versions of ArangoDB always silently ignored the value of this setting
  and effectively hard-coded the value to 9.

* Fixed internal issue #4378: fix bug in MoveShard::abort which causes a
  duplicate entry in the follower list.

* Fixed cut'n'pasting code from the documentation into arangosh.

* Fixed issue #9652: fix ArangoSearch wrong name collision and raising
  "Name collision detected" error during creation of a custom analyzer with
  stopwords.

* Fixed an agency bug found in Windows tests.

* Added initial support for wgs84 reference ellipsoid in GEO_DISTANCE through
  third optional parameter to AQL function.

* Added support for area calculations with GEO_AREA AQL function.

* Correct RocksDB statistics to report sums from column families instead of
  single value from default column family.

* Fixed agency nodes to not create bogus keys on delete / observe / unobserve.

* Fixed issue #9660: fix multiple plans processing during optimization of AQL
  query which uses ArangoSearch scorers.

* Fixed issue #9679: improved error message for FULLTEXT function invocation
  failures.

* Fixed error message "Invalid argument: assume_tracked is set but it is not
  tracked yet" when trying to write to the same keys in concurrent RocksDB
  transactions. This error will now be reported as a "Lock timeout" error,
  with error code 1200.

* Added resign leadership job to supervision.

* Hide MMFiles-specific information in web UI (in detail view of a collection)
  when the storage engine is not MMFiles or when the information is not
  available.

* Keep followers in sync if the old leader resigned and stopped writes.

* Abort a FailedLeader job when its _to_ server fails.

* Decreased unnecessary wait times for agency callbacks in case they were
  called earlier than expected by main thread.

* Make arangosh not close the connection after the user aborts an operation.
  This restores the same behavior as in previous versions of ArangoDB, which
  also left the connection open.

* Refactor maintenance to use a TakeoverShardLeadership job. This fixes a bug
  that a shard follower could have set the wrong leader for the shard locally.


v3.5.0 (2019-08-20)
-------------------

* Fix web UI link to ArangoSearch views documentation.

* Rebuild swagger for web UI and documentation.


v3.5.0-rc.7 (2019-08-01)
------------------------

* Upgraded arangodb starter version to 0.14.12.


v3.5.0-rc.6 (2019-07-29)
------------------------

* Fixed issue #9459: Optimization rule remove-collect-variables does not KEEP all necessary data.

* Added gzip and encryption options to arangoimport and arangoexport.

* Added missing REST API route GET /_api/transaction for retrieving the list of
  currently ongoing transactions.

* Fixed issue #9558: RTRIM not working as expected.

* Added startup error for bad temporary directory setting.

  If the temporary directory (--temp.path) setting is identical to the database
  directory (`--database.directory`) this can eventually lead to data loss, as
  temporary files may be created inside the temporary directory, causing overwrites of
  existing database files/directories with the same names.
  Additionally the temporary directory may be cleaned at some point, and this would lead
  to an unintended cleanup of the database files/directories as well.
  Now, if the database directory and temporary directory are set to the same path, there
  will be a startup warning about potential data loss (though in ArangoDB 3.4 allowing to
  continue the startup - in 3.5 and higher we will abort the startup).

* Make TTL indexes behave like other indexes on creation.

  If a TTL index is already present on a collection, the previous behavior
  was to make subsequent calls to `ensureIndex` fail unconditionally with
  the error "there can only be one ttl index per collection".

  Now we are comparing the attributes of the to-be-created index with the
  attributes of the existing TTL index and make it only fail when the
  attributes differ. If the attributes are identical, the `ensureIndex`
  call succeeds and returns the existing index.


v3.5.0-rc.5 (2019-07-22)
------------------------

* MinReplicationFactor:
  Collections can now be created with a minimal replication factor (minReplicationFactor), which defaults to 1.
  If minReplicationFactor > 1 a collection will go into "read-only" mode as soon as it has less then minReplicationFactor
  many insync followers. With this mechanism users can avoid to have collections diverge too much in case of failure scenarios.
  minReplicationFactor can have the values: 1 <= minReplicationFactor <= replicationFactor.
  Having minReplicationFactor == 1 ArangoDB behaves the same way as in any previous version.

* Fixed a query abort error with SmartJoins if both collections were restricted to a
  single shard using the "restrict-to-single-shard" optimizer rule.

* Fixed a performance regression of COLLECT WITH COUNT INTO.

* Fixed some races in cluster collection creation, which allowed collections with the
  same name to be created in parallel under some rare conditions.

* arangoimport would not stop, much less report, communications errors.  Add CSV reporting
  of line numbers that are impacted during such errors

* Prevent rare cases of duplicate DDL actions being executed by Maintenance.

* Coordinator code was reporting rocksdb error codes, but not the associated detail message.
  Corrected.

* The keep alive timeout specified via --http.keep-alive-timeout is now being honored

* Replication requests on Document API are now on higher priority then client-triggered requests.
  This should help to keep in sync replication up and running even if the server is overloaded.

* Bugfix: Save distinct WAL ticks for multiple replication clients from the same
  server. Also, when a follower is added for synchronous replication, the WAL
  tick held by the client is freed immediately, rather than waiting for a
  timeout.
  The corresponding APIs get a new parameter `syncerId`, which, if given,
  supersedes `serverId`. This affects the routes /_api/wal/tail,
  /_api/replication/batch, /_api/replication/logger-follow and the internal
  route /_api/replication/addFollower. The new field `syncerId` is also added to
  the response of /_api/replication/logger-state.

* Disallow indexing the `_id` attribute even as a sub-attribute.
  Previous versions of ArangoDB did not allow creating indexes on the `_id` attribute, but didn't
  check if an index was created on the `_id` attribute of a sub-attribute, e.g. `referredTo._id`
  or `data[*]._id`.
  Such indexes could be created with previous versions of ArangoDB, but they were non-functional.
  From now on, such indexes cannot be created anymore, and any attempts to create them will fail.

* Added option `--replication.max-parallel-tailing-invocations` to limit the maximum number
  of concurrent WAL tailing invocations.

  The option can be used to limit the usage of the WAL tailing APIs in order to control
  server load.

* Fixed agency bug with election lock step.

* Fixed some error reporting and logging in Maintenance.

* Fixed an error condition in which an ex-leader for a short still believed
  to be the leader and wrongly reported to Current.


v3.5.0-rc.4 (2019-06-15)
------------------------

* Speed up collection creation process in cluster, if not all agency callbacks are
  delivered successfully.

* Increased performance of document inserts, by reducing the number of checks in unique / primary indexes.

* Fixed a callback function in the web UI where the variable `this` was out of scope.

* Fixed editing a user within the web UI if the user added a gravatar profile picture.

* Allow pregel to select the shard key via `shardKeyAttribute` in pregel start parameters.

* Added --server.jwt-secret-keyfile to arangosh.

* Fixed internal issue #4040: gharial api is now checking existence of `_from` and `_to` vertices
  during edge replacements and edge updates.

* Fixed `Buffer.alloc` method.

* `Buffer` is now iterable and accepts `ArrayBuffer` values as input.

* Fix timeout-response in case of simultaneous index create/drop in cluster.

* Enabled dropping an index by its name.

* Fixed lookup of index from collection by fully qualified name, e.g.
  `db.testCollection.index('testCollection/primary')`.

* Fixed agency bug with TTL object writes discovered in 3.4.6.


v3.5.0-rc.3 (2019-05-31)
------------------------

* Fixed issue #9106: Sparse Skiplist Index on multiple fields not used for FILTER + SORT query.

  Allow AQL query optimizer to use sparse indexes in more cases, specifically when
  indexes could not be used for filtering and there finally was an `EnumerateCollectionNode`
  in the query execution plan followed by a `SortNode`. In this case, sparse indexes were
  not considered for enumeration in sorted order, because it was unclear to the optimizer
  if the result set would contain null values or not.

* Upgraded RocksDB to version 6.2.

* Updated ArangoDB Starter to 0.14.4.

* The system collection '_jobs' will from now on be created with non-unique, non-sparse indexes.

* Bugfix for smart graph traversals with uniqueVertices: path, which could
  sometimes lead to erroneous traversal results.

* Pregel algorithms can be run with the option "useMemoryMaps: true" to be
  able to run algorithms on data that is bigger than the available RAM.

* Fixed a race in TTL thread deactivation/shutdown.

* Fixed internal issue #3919: The web UI is now using precompiled ejs templates.

* Fixed agency issue in abort of cleanOutServer job.

v3.5.0-rc.2 (2019-05-23)
------------------------

* Fixed "collection not found" exception during setup of 3-way SmartJoin queries in the
  cluster.

* Fixed an edge case of handling `null` values in the AQL function `MIN` for input
  sequences that started with a `null` value. In this case, `null` was always returned as the
  minimum value even though other non-null values may have followed, and `MIN` was supposed
  to return `null` only if there are no input values or all input values are `null`.

* Fixed a crash when posting an async request to the server using the "x-arango-async"
  request header and the server's request queue was full.

* Added error code 1240 "incomplete read" for RocksDB-based reads which cannot retrieve
  documents due to the RocksDB block cache being size-restricted (with size limit enforced)
  and uncompressed data blocks not fitting into the block cache.

  The error can only occur for collection or index scans with the RocksDB storage engine
  when the RocksDB block cache is used and set to a very small size, plus its maximum size is
  enforced by setting the `--rocksdb.enforce-block-cache-size-limit` option to `true`.

  Previously these incomplete reads could have been ignored silently, making collection or
  index scans return less documents than there were actually present.

* Fixed internal issue #3918: added optional second parameter "withId" to AQL
  function PREGEL_RESULT.

  This parameter defaults to `false`. When set to `true` the results of the Pregel
  computation run will also contain the `_id` attribute for each vertex and not
  just `_key`. This allows distinguishing vertices from different vertex collections.

* Made Swagger UI work with HTTPS-enabled ArangoDBs too.

* Improved scheduler performance for single-connection cases.

* Internally switched unit tests framework from catch to gtest.

* Disabled selection of index types "hash" and "skiplist" in the web interface when
  using the RocksDB engine. The index types "hash", "skiplist" and "persistent" are
  just aliases of each other with the RocksDB engine, so there is no need to offer all
  of them. After initially only offering "hash" indexes, we decided to only offer
  indexes of type "persistent", as it is technically the most
  appropriate description.

* Fixed client id lookup table in state.


v3.5.0-rc.1 (2019-05-14)
------------------------

* Removed bug during start up with a single agent, that leads to dbserver crash.

* Fixed issue #7011: description when replacing a foxx application was misleading.

* Fixed issue #8841: Graph Viewer dropped ability to edit an edge after
  rerendering.

* Upgraded arangodb starter version to 0.14.3.

* ArangoQueryStreamCursor.prototype.id needs to be a string, v8 32 bit integers
  can't hold the full data.

* Upgraded Swagger UI to 3.22.1.

* Added --compress-output flag to arangodump. Activates gzip compression for
  collection data. Metadata files, such as .structure.json and .view.json,
  do not get compressed. No option is needed for arangorestore to restore
  .data.json.gz files.

* Added options to make server more secure:

  - `--server.harden`: denies access to certain REST APIs that return server internals
  - `--foxx.api`: set to false disables Foxx management API
  - `--foxx.store`: set to false disables Foxx UI
  - `--javascript.allow-port-testing`: enables internal.testPort()
  - `--javascript.allow-external-process-control`: enables external process control
  - `--javascript.harden`: disables getPid() and logLevel()
  - `--javascript.startup-options-whitelist`: control startup options visible in JavaScript
  - `--javascript.environment-variables-whitelist`: control environment variables visible in JavaScript
  - `--javascript.endpoints-whitelist`: control accessible endpoints in JavaScript
  - `--javascript.files-whitelist`: control file access in JavaScript

  Note: There is a [detailed description of all options](https://www.arangodb.com/docs/devel/security-security-options.html).

* Prevented arangod from making a call to www.arangodb.com at startup.

  This call was done to check for available updates, but it could have contributed
  to small startup delays in case outgoing connections were blocked.

* Removed support for undocumented HTTP header `x-arango-v8-context`, which
  allowed controlling in which particular V8 context number a JavaScript-based
  action was executed. This header was only used internally for testing.

* `db._query` now handles additional arguments correctly when passing an AQL
  query object instead of a query string and separate bindVars.

* Added req.auth property to Foxx.

* Added collection.documentId method to derive document id from key.

* Indexes created with the 'inBackground', will not hold an
  exclusive collection lock for the entire index creation period (rocksdb only).

* Fixed internal issue #536: ArangoSearch may crash server during term lookup.

* Fixed internal issue #2946: Create graph autocomplete was not working under
  certain circumstances.

* Added `filter` option to Foxx HTTP API for running tests.

* Added function `db.<collection>.getResponsibleShard()` to find out which is the
  responsible shard for a given document. Note that this function is only available
  in a cluster coordinator.

* Updated bundled version of jemalloc memory allocator to 5.2.0.

* Don't create per-database system collection `_frontend` automatically.
  This collection is only needed by the web UI, and it can be created lazily
  when needed.

* Added logging option `--log.time-format` to configure the time format used
  in log output. The possible values for this option are:

  - timestamp: unix timestamps, in seconds
  - timestamp-millis: unix timestamps, in seconds, with millisecond precision
  - timestamp-micros: unix timestamps, in seconds, with microsecond precision
  - uptime: seconds since server start
  - uptime-millis: seconds since server start, with millisecond precision
  - uptime-micros: seconds since server start, with microsecond precision
  - utc-datestring: UTC-based date and time in format YYYY-MM-DDTHH:MM:SSZ
  - utc-datestring-millis: UTC-based date and time in format YYYY-MM-DDTHH:MM:SS.FFFZ
  - local-datestring: local date and time in format YYYY-MM-DDTHH:MM:SS

  This change deprecates the existing options `--log.use-microtime` and
  `--log.use-localtime`, because the functionality provided by these options
  is covered by `--log.time-format` too.

* Added "SmartJoins" to the ArangoDB Enterprise Edition that allows running cluster
  joins between two certain sharded collections with performance close to that of a
  local join operation.

* Fixed internal issue #3815: fixed the removal of connected edges when
  removing a vertex graph node in a smart graph environment.

* Show startup warning in case kernel setting `vm.overcommit_memory` is set
  to a value of 2 and the jemalloc memory allocator is in use. This combination
  does not play well together.

* Added AQL function DECODE_REV for decomposing `_rev` values into their
  individual parts.

* Added AQL functions CRC32 and FNV64 for hashing data.

* Renamed attribute key `openssl-version` in server/client tool version
  details output to `openssl-version-compile-time`.

  This change affects the output produced when starting one of the ArangoDB
  executables with the `--version` command. It also changes the attribute
  name in the detailed response of the `/_api/version` REST API.

* Fixed the sorting of the databases in the database
  selection dropdown in the web ui. The sort order differed based on whether
  authentication was enabled or disabled. (Internal issue #2276)

* Improved the shards view in the web ui if there
  is only one shard to display. (Internal issue #3546)

* Restricted the allowed query names for user
  defined custom queries within the web ui. (Internal issue #3789)

* Upgraded bundled RocksDB version to 6.0.

* Added "--log.ids" option to arangod.

  The default value for this option is `true`. Setting the option to `false`
  will prevent embedding unique log ids into all log messages generated by
  ArangoDB C++ code. The unique ids allow for easy access to the location in
  the source code from which a message originates. This should help customers
  to configure custom monitoring/alerting based on specific log id occurrences
  and will also be helpful for support and development in identifying problems.

* Fixed wrong equals behavior on arrays with ArangoSearch. (Internal issue #8294)

* Fixed ArangoSearch range query sometimes not working
  correctly with numeric values. (Internal issue #528)

* Changed type of the startup option `--rocksdb.recycle-log-file-num` from
  numeric to boolean, as this is also the type the options has in the RocksDB
  library.

* Renamed hidden startup option `--rocksdb.delayed_write_rate` to the more
  consistent `--rocksdb.delayed-write-rate`. When the old option name is
  used, the arangod startup will be aborted with a descriptive error message.

* If not explicitly configured, make agency nodes start removing their unused
  WAL files a few seconds after the completed server startup already. This is
  because on agency nodes, unused WAL files do not need to be retained for
  potential replication clients to read them.

* Added option `--all-databases` to arangodump and arangorestore.

  When set to true, this makes arangodump dump all available databases
  the current user has access to. The option `--all-databases` cannot be
  used in combination with the option `--server.database`.

  When `--all-databases` is used, arangodump will create a subdirectory
  with the data of each dumped database. Databases will be dumped one
  after the other. However, inside each database, the collections of the
  database can be dumped in parallel using multiple threads.

  For arangorestore, this makes it restore all databases from inside the
  subdirectories of the specified dump directory. Using the option for
  arangorestore only makes sense for dumps created with arangodump and
  the `--all-databases` option. As for arangodump, arangorestore cannot
  be invoked with the options `--all-databases` and `--server.database`
  at the same time. Additionally, the option `--force-same-database` cannot
  be used together with `--all-databases`.

  If the to-be-restored databases do not exist on the target server, then
  restoring data into them will fail unless the option `--create-database`
  is also specified. Please note that in this case a database user must
  be used that has access to the `_system` database, in order to create
  the databases on restore.

* Added index hints feature to AQL.

* Added "name" property for indices.

  If a name is not specified on index creation, one will be auto-generated.

* Under normal circumstances there should be no need to connect to a
  database server in a cluster with one of the client tools, and it is
  likely that any user operations carried out there with one of the client
  tools may cause trouble.

  The client tools arangosh, arangodump and arangorestore will now emit
  a warning when connecting with them to a database server node in a cluster.

* Fixed compaction behavior of followers.

* Added "random" masking to mask any data type, added wildcard masking.

* Added option `--console.history` to arangosh for controlling whether
  the command-line history should be loaded from and persisted in a file.

  The default value for this option is `true`. Setting it to `false`
  will make arangosh not load any command-line history from the history
  file, and not store the current session's history when the shell is
  exited. The command-line history will then only be available in the
  current shell session.

* Display the server role when connecting arangosh against a server (e.g.
  SINGLE, COORDINATOR).

* Fixed overflow in Windows NowNanos in RocksDB.

* Allowed MoveShard from leader to a follower, thus swapping the two.

* Supervision fix: Satellite collections, various fixes.

* Added coordinator route for agency dump.

* Supervision fix: abort MoveShard job does not leave a lock behind.

* Supervision fix: abort MoveShard (leader) job moves forwards when point
  of no return has been reached.

* Supervision fix: abort CleanOutServer job does not leave server in
  ToBeCleanedServers.

* Supervision fix: move shard with data stopped to early due to wrong usage
  of compare function.

* Supervision fix: AddFollower only counts good followers, fixing a
  situation after a FailedLeader job could not find a new working
  follower.

* Supervision fix: FailedLeader now also considers temporarily BAD
  servers as replacement followers and does not block servers which
  currently receive a new shard.

* Supervision fix: Servers in ToBeCleanedServers are no longer considered
  as replacement servers.

* Maintenance fix: added precondition of unchanged Plan in phase2.

* Added "PRUNE <condition>" to AQL Traversals. This allows to early abort searching of
  unnecessary branches within a traversal.
  PRUNE is only allowed in the Traversal statement and only between the graph
  definition and the options of the traversal.
  e.g.:
  `FOR v, e, p IN 1..3 OUTBOUND @source GRAPH "myGraph"
    PRUNE v.value == "bar"
    OPTIONS {} /* These options remain optional */
    RETURN v`
  for more details refer to the documentation chapter.

* Fixed a display issue when editing a graph within the web UI.

* Fixed some escaping issues within the web UI.

* Follow up to fix JWT authentication in arangosh (Internal issue #7530):
  also fixed reconnect.

* Now also syncing _jobs and _queues collections in active failover mode.

* Upgraded lodash to 4.17.11 because of CVE-2018-16487.

* `--query.registry-ttl` is now honored in single-server mode, and cursor TTLs
  are now honored on DBServers in cluster mode.

* Added "TTL" index type, for optional auto-expiration of documents.

* Disabled selection of index types "persistent" and "skiplist" in the web
  interface when using the RocksDB engine. The index types "hash", "skiplist"
  and "persistent" are just aliases of each other with the RocksDB engine,
  so there is no need to offer all of them.

* Fixed JS AQL query objects with empty query strings not being recognized
  as AQL queries.

* Updated JavaScript dependencies, including semver major updates to joi, mocha
  and eslint. The eslint config modules were incompatible with the existing
  coding style, so the old rules were inlined and the config dependencies removed.

  Note that joi removed Joi.date().format() in v10.0.0. You can bundle your own
  version of joi if you need to rely on version-dependent features.

  - accepts: 1.3.4 -> 1.3.5
  - ansi_up: 2.0.2 -> 4.0.3
  - content-disposition: 0.5.2 -> 0.5.3
  - dedent: 0.6.0 -> 0.7.0
  - error-stack-parser: 1.3.6 -> 2.0.2
  - eslint: 2.13.1 -> 5.16.0
  - eslint-config-semistandard: 6.0.2 -> removed
  - eslint-config-standard: 5.3.1 -> removed
  - eslint-plugin-promise: 1.3.2 -> removed
  - eslint-plugin-standard: 1.3.2 -> removed
  - highlight.js: 9.12.0 -> 9.15.6
  - http-errors: 1.6.2 -> 1.7.2
  - iconv-lite: 0.4.19 -> 0.4.24
  - joi: 9.2.0 -> 14.3.1
  - joi-to-json-schema: 2.3.0 -> 4.0.1
  - js-yaml: 3.10.0 -> 3.13.1
  - marked: 0.3.9 -> 0.6.2
  - mime-types: 2.1.12 -> 2.1.22
  - mocha: 2.5.3 -> 6.1.3
  - qs: 6.5.1 -> 6.7.0
  - semver: 5.4.1 -> 6.0.0
  - statuses: 1.4.0 -> 1.5.0
  - timezone: 1.0.13 -> 1.0.22
  - type-is: 1.6.15 -> 1.6.16
  - underscore: 1.8.3 -> 1.9.1

* Updated V8 to 7.1.302.28.

  New V8 behavior introduced herein:

  - ES2016 changed the default timezone of date strings to be conditional on
    whether a time part is included. The semantics were a compromise approach
    based on web compatibility feedback from V8, but until now, we have been
    shipping ES5.1 default timezone semantics. This patch implements the
    new semantics, following ChakraCore and SpiderMonkey (though JSC
    implements V8's previous semantics).

* Fixed JS AQL query objects with empty query strings not being recognized as AQL queries.

* Report run-time openssl version (for dynamically linked executables).

* Added greeting warning about maintainer mode.

* Improved insertion time into non-unique secondary indexes with the RocksDB
  engine.

* Fixed possible segfault when using COLLECT with a LIMIT and an offset.

* Fixed COLLECT forgetting top-level variables after 1000 rows.

* Added sort-limit optimization in AQL; Improved memory usage and execution
  time for some queries.

* Upgraded to OpenSSL 1.1.0j.

* Added configurable masking of dumped data via `arangodump` tool to obfuscate
  exported sensible data.

* Fixed arangoimp script for MacOSX CLI Bundle.

* Added "peakMemoryUsage" in query results figures, showing the peak memory
  usage of the executed query. In a cluster, the value the peak memory usage
  of all shards, but it is not summed up across shards.

* Fixed an issue where a crashed coordinator can lead to some Foxx queue jobs
  erroneously either left hanging or being restarted.

* Fixed bind values of `null` are not replaced by
  empty string anymore, when toggling between json and table
  view in the web-ui. (Interal issue #7900)

* Fixed regression on ISO8601 string compatibility in AQL. (Internal issue #2786)

  millisecond parts of AQL date values were limited to up to 3 digits.
  Now the length of the millisecond part is unrestricted, but the
  millisecond precision is still limited to up to 3 digits.

* The RocksDB primary index can now be used by the optimizer to optimize queries
  that use `_key` or `_id` for sorting or for range queries.

* The web UI will now by default show the documents of a collection lexicographically
  sorted when sorting documents by their `_key` values.

  Previous versions of ArangoDB tried to interpret `_key` values as numeric values if
  possible and sorted by these. That previous sort strategy never used an index and
  could have caused unnecessary overhead. The new version will now use an index for
  sorting for the RocksDB engine, but may change the order in which documents are
  shown in the web UI (e.g. now a `_key` value of "10" will be shown before a `_key`
  value of "9").

* Fixed known issue #445: ArangoSearch ignores `_id` attribute even if `includeAllFields`
  is set to `true`.

* Upgraded bundled boost library to version 1.69.0.

* Upgraded bundled curl library to version 7.63.

* Used base64url to encode and decode JWT parts.

* Added --server.jwt-secret-keyfile option.

* Sped up data-modification operations in exclusive transactions in the RocksDB
  storage engine.

* An AQL query that uses the edge index only and returns the opposite side of
  the edge can now be executed in a more optimized way, e.g.

    FOR edge IN edgeCollection FILTER edge._from == "v/1" RETURN edge._to

  is fully covered by RocksDB edge index. For MMFiles this rule does not apply.

* Reverted accidental change to error handling in geo index.

  In previous versions, if non-valid geo coordinates were contained in the
  indexed field of a document, the document was simply ignored an not indexed.
  In 3.4.0, this was accidentally changed to generate an error, which caused
  the upgrade procedure to break in some cases.

* Fixed TypeError being thrown instead of validation errors when Foxx manifest
  validation fails.

* Improved confirmation dialog when clicking the
  Truncate button in the Web UI. (Internal issue #2786)

* Made `--help-all` now also show all hidden program options.

  Previously hidden program options were only returned when invoking arangod or
  a client tool with the cryptic `--help-.` option. Now `--help-all` simply
  retuns them as well.

  The program options JSON description returned by `--dump-options` was also
  improved as follows:

  - the new boolean attribute "dynamic" indicates whether the option has a dynamic
    default value, i.e. a value that depends on the target host capabilities or
    configuration

  - the new boolean attribute "requiresValue" indicates whether a boolean option
    requires a value of "true" or "false" when specified. If "requiresValue" is
    false, then the option can be specified without a boolean value following it,
    and the option will still be set to true, e.g. `--server.authentication` is
    identical to `--server.authentication true`.

  - the new "category" attribute will contain a value of "command" for command-like
    options, such as `--version`, `--dump-options`, `--dump-dependencies` etc.,
    and "option" for all others.

* Fixed issue #7586: a running query within the user interface was not shown
  if the active view was `Running Queries` or `Slow Query History`.

* Fixed issue #7743: Query processing discrepancy between Rocks and MMFiles databases.

  This change enforces the invalidation of variables in AQL queries after usage of
  a COLLECT statement as documented. The documentation for variable invalidation claims
  that

      The COLLECT statement will eliminate all local variables in the current scope.
      After COLLECT only the variables introduced by COLLECT itself are available.

  However, the described behavior was not enforced when a COLLECT was preceded by a
  FOR loop that was itself preceded by a COLLECT. In the following query the final
  RETURN statement accesses variable `key1` though the variable should have been
  invalidated by the COLLECT directly before it:

      `FOR x1 IN 1..2
        COLLECT key1 = x1
        FOR x2 IN 1..2
          COLLECT key2 = x2
          RETURN [key2, key1]`

  In previous releases, this query was
  parsed ok, but the contents of variable `key1` in the final RETURN statement were
  undefined.

  This change is about making queries as the above fail with a parse error, as an
  unknown variable `key1` is accessed here, avoiding the undefined behavior. This is
  also in line with what the documentation states about variable invalidation.

* Fixed issue #7763: Collect after update does not execute updates.

* Fixed issue #7749: AQL query result changed for COLLECT used on empty data/array.

* Fixed issue #7757: Using multiple filters on nested objects produces wrong results.

* Fixed a rare thread local dead lock situation in replication:
  If a follower tries to get in sync in the last steps it requires
  a lock on the leader. If the follower cancels the lock before the leader
  has succeeded with locking we can end up with one thread being deadlocked.

* Allowed usage of floating point values in AQL without leading zeros, e.g.
  `.1234`. Previous versions of ArangoDB required a leading zero in front of
  the decimal separator, i.e `0.1234`.

* Foxx `req.makeAbsolute` now will return meaningful values when ArangoDB is using
  a unix socket endpoint. URLs generated when using a unix socket follow the format
  http://unix:<socket-path>:<url-path> used by other JS tooling.

* Updated joi library (Web UI), improved foxx mount path validation.

* Do not create `_routing` collection for new installations/new databases,
  as it is not needed anymore. Redirects to the web interface's login screen, which
  were previously handled by entries in the `_routing` collection are now handled
  from the responsible REST action handler directly.

  Existing `_routing` collections will not be touched as they may contain other
  entries as well, and will continue to work.

* Do not create `_modules` collection for new databases/installations.

  `_modules` is only needed for custom modules, and in case a custom
  module is defined via `defineModule`, the _modules collection will
  be created lazily automatically.

  Existing modules in existing `_modules` collections will remain
  functional even after this change

* Disabled in-memory cache for edge and traversal data on agency nodes, as it
  is not needed there.

* Removed bundled Valgrind headers, removed JavaScript variable `valgrind`
  from the `internal` module.

* Upgraded JEMalloc version to 5.1.0.

* Use `-std=c++14` for ArangoDB compilation.

* In case of resigned leader, set `isReady=false` in clusterInventory.

* Abort RemoveFollower job if not enough in-sync followers or leader failure.

* Fixed shrinkCluster for satelliteCollections.

* Fixed crash in agency supervision when leadership is lost.

* Leader's timestamps in append entries protocol.

* Sped up agency supervision in case of many jobs.

* Fixed log spam in agency supervision when leader resigned.

* Made AddFollower less aggressive.

* Several agency performance improvements, mostly avoiding copying.

* Priority queue for maintenance jobs.

* Do not wait for replication after each job execution in Supervision.

* Added support for configuring custom Analyzers via JavaScript and REST.


v3.4.7 (2019-07-02)
-------------------

* updated arangosync to 0.6.4

* bug-fix for a very rare race condition on cluster collection creation process. It can only occur in the following
  situation:
  1) DBServer sucessfully creates all shards for collections, and reports it back
  2) DBServer dies
  3) Agency recognized death and reorganizes shards
  4) Coordinator receives (1) but not (3) and decided everything was good
  5) => Coordinator reverts (3) only on this collection
  6) Coordinator receives (3) now
  The bugfix disallows (5) if (3) is run in Agency.

* fixed internal issue #4040: gharial api is now checking existence of `_from` and `_to` vertices
  during edge replacements and edge updates

* fix timeout-response in case of simultaneous index create/drop in cluster

* Fixed editing a user within the web UI if the user added a gravatar profile picture

* fixed internal issue #4172: on agent servers the RocksDB WAL files in the archive directory
  were retained due to an error. Now they are removed, eventually

* fix error reporting in arangorestore. previously, when the server returned an HTTP 404 error,
  arangorestore unconditionally translated this into a "collection not found" error, which may
  have masked an actually different server-side error. Instead, make arangorestore return the
  server's actual error message as is.

* allow pregel to select the shard key via `shardKeyAttribute` in pregel start parameters

* Speed up collection creation process in cluster, if not all agency callbacks are
  delivered successfully.

* Fixed parsing of ArangoDB config files with inlined comments. Previous versions didn't handle
  line comments properly if they were appended to an otherwise valid option value.

  For example, the comment in the line

      max-total-wal-size = 1024000 # 1M

  was not ignored and made part of the value. In the end, the value was interpreted as if

      max-total-wal-size = 10240001000000

  was specified.
  This version fixes the handling of comments in the config files so that they behave as intended.

* Fixed AQL query tracing for traversals, this lead to wrong "calls" count in
  query profiles

* Pregel algorithms can be run with the option "useMemoryMaps: true" to be able to run algorithms
  on data that is bigger than the available RAM.

* Bugfix for smart graph traversals with uniqueVertices: path, which could
  sometimes lead to erroneous traversal results

* The system-collection '_jobs' will from now on use non-unique, non-sparse indexes.

* fixed internal issue #3919: The web UI is now using precompiled ejs templates.

* properly create VERSION files for any newly created databases again, as it has been the
  case in 3.3 and before. If no VERSION files are created, this may cause problems on upgrade
  to 3.5. The 3.5 branch was also modified to handle this problem gracefully.

* fixed "collection not found" exception during setup of 3-way SmartJoin queries in the
  cluster

* fixed an edge case of handling `null` values in the AQL function `MIN` for input
  sequences that started with a `null` value. In this case, `null` was always returned as the
  minimum value even though other non-null values may have followed, and `MIN` was supposed
  to return `null` only if there are no input values or all input
  values are `null`.

* fix client id lookup table in agency

* Fix agency's handling of object assignments with TTL

* reduce resident memory usage when creating non-unique indexes with the RocksDB storage engine

* Fix agency election bug.


v3.4.6.1 (2019-06-05)
---------------------

* upgraded ArangoDB Starter version to 0.14.5

* Speed up collection creation process in cluster, if not all agency callbacks are
  delivered successfully.

* Fix agency's handling of object assignments with TTL


v3.4.6 (2019-05-21)
-------------------

* updated ArangoDB Starter to 0.14.4

* fixed internal issue #3912: improved the performance of graph creation with multiple
  relations. They do now create multiple collections within a single round-trip in the agency.

* fixed a crash when posting an async request to the server using the "x-arango-async"
  request header and the server's request queue was full

* added error code 1240 "incomplete read" for RocksDB-based reads which cannot retrieve
  documents due to the RocksDB block cache being size-restricted (with size limit enforced)
  and uncompressed data blocks not fitting into the block cache

  The error can only occur for collection or index scans with the RocksDB storage engine
  when the RocksDB block cache is used and set to a very small size, plus its maximum size is
  enforced by setting the `--rocksdb.enforce-block-cache-size-limit` option to `true`.

  Previously these incomplete reads could have been ignored silently, making collection or
  index scans return less documents than there were actually present.

* fixed internal issue #3918: added optional second parameter "withId" to AQL
  function PREGEL_RESULT

  this parameter defaults to `false`. When set to `true` the results of the Pregel
  computation run will also contain the `_id` attribute for each vertex and not
  just `_key`. This allows distinguishing vertices from different vertex collections.

* fixed issue #7011: description when replacing a foxx application was misleading

* fixed issue #8841: Graph Viewer dropped ability to edit an edge after
  rerendering.

* upgraded arangodb starter version to 0.14.3

* fix removal of sort order for sorted results that were passed into a RETURN DISTINCT
  in an AQL query running on a collection with more than a single shard. In this case,
  the results from the individual shards were sorted, but the overall result was not
  sorted according to the sort criterion.

* removed bug during start up with a single agent, that leads to dbserver crash.

* fix the creation of debug packages (in the web interface) for queries that
  involve smart graphs and/or multiple edge collections from a traversal

* add --compress-output flag to arangodump. Activates gzip compression for
  collection data. Metadata files, such as .structure.json and .view.json,
  do not get compressed. No option is needed for arangorestore to restore
  .data.json.gz files.

* fixed internal issue #3893: correct wrong option name `--database.ignore-logfile-errors`
  in error message and correct it to `--database.ignore-datafile-errors`

* fixed internal issue #2946: create graph autocomplete was not working under
  certain circumstances.

* fixed a memory leak in PRUNE operation for traversals.
  The leak occurred only when PRUNE was actively used in queries.

* fixed a crash (SIGSEGV) when opening a RocksDB database directory that
  contained an empty (0 bytes filesize) journal file and encryption was in use.

* improve test performance by doing the 0.1 delay between phase 1 and 2 only
  if phase 1 is slow

* port back timestamp replication in agency from devel

* better logging when scheduler queue is half-full and full

* fixed internal issue #536: ArangoSearch query may crash during internal lookup in some
  cases due to invalid index structure for exact input data


v3.4.5 (2019-03-27)
-------------------

* fixed a shutdown issue when the server was shut down while there were
  active Pregel jobs executing

* fixed internal issue #3815: fixed the removal of connected edges when
  removing a vertex graph node in a smart graph environment.

* added AQL functions CRC32 and FNV64 for hashing data

* internal issue #2276: fixed the sorting of the databases in the database
  selection dropdown in the web ui. The sort order differed based on whether
  authentication was enabled or disabled.

* fixed internal issue #3789: restricted the allowed query names for user-
  defined custom queries within the web ui.

* fixed internal issue #3546: improved the shards view in the web ui if there
  is only one shard to display.

* fixed a display issues when editing a graph within the web ui

* fixed internal-issue #3787: Let user know about conflicting attribute in
  AQL queries if information is available.

* fixed issue #8294: wrong equals behavior on arrays with ArangoSearch

* fixed internal issue #528: ArangoSearch range query sometimes doesn't work
  correctly with numeric values

* fixed internal issue #3757: when restarting a follower in active failover mode,
  try an incremental sync instead of a full resync. Fixed also the case where a
  double resync was made

* don't check for the presence of ArangoDB upgrades available when firing up an
  arangosh enterprise edition build

* added startup option `--rocksdb.allow-fallocate`

  When set to true, allows RocksDB to use the fallocate call. If false, fallocate
  calls are bypassed and no preallocation is done. Preallocation is turned on by
  default, but can be turned off for operating system versions that are known to
  have issues with it.
  This option only has an effect on operating systems that support fallocate.

* added startup option `--rocksdb.limit-open-files-at-startup`

  If set to true, this will limit the amount of .sst files RocksDB will inspect at
  startup, which can reduce the number of IO operations performed at start.

* don't run compact() on a collection after a truncate() was done in the same
  transaction

  running compact() in the same transaction will only increase the data size on
  disk due to RocksDB not being able to remove any documents physically due to the
  snapshot that is taken at transaction start.

  This change also exposes db.<collection>.compact() in the arangosh, in order to
  manually run a compaction on the data range of a collection should it be needed
  for maintenance.

* don't attempt to remove non-existing WAL files, because such attempt will
  trigger unnecessary error log messages in the RocksDB library

* updated arangosync to 0.6.3

* added --log.file-mode to specify a file mode of newly created log files

* added --log.file-group to specify the group of newly created log files

* fixed some escaping issues within the web ui.

* fixed issue #8359: How to update a document with unique index (constraints)?

* when restarting a follower in active failover mode, try an incremental sync instead
  of a full resync

* add "PRUNE <condition>" to AQL Traversals (internal issue #3068). This allows to early
  abort searching of unnecessary branches within a traversal.
  PRUNE is only allowed in the Traversal statement and only between the graphdefinition
  and the options of the traversal.
  e.g.:

      FOR v, e, p IN 1..3 OUTBOUND @source GRAPH "myGraph"
        PRUNE v.value == "bar"
        OPTIONS {} /* These options remain optional */
        RETURN v

  for more details refer to the documentation chapter.

* added option `--console.history` to arangosh for controlling whether
  the command-line history should be loaded from and persisted in a file.

  The default value for this option is `true`. Setting it to `false`
  will make arangosh not load any command-line history from the history
  file, and not store the current session's history when the shell is
  exited. The command-line history will then only be available in the
  current shell session.

* display the server role when connecting arangosh against a server (e.g.
  SINGLE, COORDINATOR)

* added replication applier state figures `totalDocuments` and `totalRemovals` to
  access the number of document insert/replace operations and the number of document
  removals operations separately. Also added figures `totalApplyTime` and
  `totalFetchTime` for determining the total time the replication spent for
  applying changes or fetching new data from the master. Also added are the figures
  `averageApplyTime` and `averageFetchTime`, which show the average time spent
  for applying a batch or for fetching data from the master, resp.

* fixed race condition in which the value of the informational replication applier
  figure `ticksBehind` could underflow and thus show a very huge number of ticks.

* always clear all ongoing replication transactions on the slave if the slave
  discovers the data it has asked for is not present anymore on the master and the
  `requireFromPresent` value for the applier is set to `false`.

  In this case aborting the ongoing transactions on the slave is necessary because
  they may have held exclusive locks on collections, which may otherwise not be
  released.

* added option `--rocksdb.wal-archive-size-limit` for controlling the
  maximum total size (in bytes) of archived WAL files. The default is 0
  (meaning: unlimited).

  When setting the value to a size bigger than 0, the RocksDB storage engine
  will force a removal of archived WAL files if the total size of the archive
  exceeds the configured size. The option can be used to get rid of archived
  WAL files in a disk size-constrained environment.
  Note that archived WAL files are normally deleted automatically after a
  short while when there is no follower attached that may read from the archive.
  However, in case when there are followers attached that may read from the
  archive, WAL files normally remain in the archive until their contents have
  been streamed to the followers. In case there are slow followers that cannot
  catch up this will cause a growth of the WAL files archive over time.
  The option `--rocksdb.wal-archive-size-limit` can now be used to force a
  deletion of WAL files from the archive even if there are followers attached
  that may want to read the archive. In case the option is set and a leader
  deletes files from the archive that followers want to read, this will abort
  the replication on the followers. Followers can however restart the replication
  doing a resync.

* agents need to be able to overwrite a compacted state with same _key

* in case of resigned leader, set isReady=false in clusterInventory

* abort RemoveFollower job if not enough in-sync followers or leader failure

* fix shrinkCluster for satelliteCollections

* fix crash in agency supervision when leadership is lost

* speed up supervision in agency for large numbers of jobs

* fix log spamming after leader resignation in agency

* make AddFollower less aggressive

* fix cases, where invalid json could be generated in agents' store dumps

* coordinator route for full agency dumps contains compactions and
  time stamps

* lots of agency performance improvements, mostly avoiding copying

* priority queue for maintenance jobs

* do not wait for replication after each job execution in Supervision

* fix a blockage in MoveShard if a failover happens during the operation

* check health of servers in Current before scheduling removeFollower jobs

* wait for statistics collections to be created before running resilience
  tests

* fix ttl values in agency when key overwritten with no ttl


v3.4.4 (2019-03-12)
-------------------

* added missing test for success in failed leader: this could lead
  to a crash

* follow up to fix JWT authentication in arangosh (#7530):
  also fix reconnect

* now also syncing _jobs and _queues collections in active failover mode

* fixed overflow in Windows NowNanos in RocksDB

* fixed issue #8165: AQL optimizer does not pick up multiple geo index


* when creating a new database with an initial user, set the database permission
  for this user as specified in the documentation

* Supervision fix: abort MoveShard job does not leave a lock behind,

* Supervision fix: abort MoveShard (leader) job moves forwards when point
  of no return has been reached,

* Supervision fix: abort CleanOutServer job does not leave server in
  ToBeCleanedServers,

* Supervision fix: move shard with data stopped to early due to wrong usage
  of compare function

* Supervision fix: AddFollower only counts good followers, fixing a
  situation after a FailedLeader job could not find a new working
  follower

* Supervision fix: FailedLeader now also considers temporarily BAD
  servers as replacement followers and does not block servers which
  currently receive a new shard

* Supervision fix: Servers in ToBeCleanedServers are no longer considered
  as replacement servers

* Maintenance fix: added precondition of unchanged Plan in phase2

* Allow MoveShard from leader to a follower, thus swapping the two

* Supervision fix: Satellite collections, various fixes

* Add coordinator route for agency dump

* speed up replication of transactions containing updates of existing documents.

  The replication protocol does not provide any information on whether a document
  was inserted on the master or updated/replaced. Therefore the slave will always
  try an insert first, and move to a replace if the insert fails with "unique
  constraint violation". This case is however very costly in a bigger transaction,
  as the rollback of the insert will force the underlying RocksDB write batch to be
  entirely rewritten. To circumvent rewriting entire write batches, we now do a
  quick check if the target document already exists, and then branch to either
  insert or replace internally.


v3.4.3 (2019-02-19)
-------------------

* fixed JS AQL query objects with empty query strings not being recognized as AQL queries

* fixed issue #8137: NULL input field generates U_ILLEGAL_ARGUMENT_ERROR

* fixed issue #8108: AQL variable - not working query since upgrade to 3.4 release

* fixed possible segfault when using COLLECT with a LIMIT and an offset

* fixed COLLECT forgetting top-level variables after 1000 rows

* fix undefined behavior when calling user-defined AQL functions from an AQL
  query via a streaming cursor

* fix broken validation of tick range in arangodump

* updated bundled curl library to version 7.63.0

* added "peakMemoryUsage" in query results figures, showing the peak memory
  usage of the executed query. In a cluster, the value contains the peak memory
  usage across all shards, but it is not summed up across shards.

* data masking: better documentation, fixed default phone number,
  changed default range to -100 and 100 for integer masking function

* fix supervision's failed server handling to transactionally create
  all failed leader/followers along


v3.4.2.1 (2019-02-01)
---------------------

* upgrade to new velocypack version


v3.4.2 (2019-01-24)
-------------------

* added configurable masking of dumped data via `arangodump` tool to obfuscate exported sensible data

* upgraded to OpenSSL 1.1.0j

* fixed an issue with AQL query IN index lookup conditions being converted into
  empty arrays when they were shared between multiple nodes of a lookup condition
  that used an IN array lookup in an OR that was multiplied due to DNF transformations

  This issue affected queries such as the following

      FILTER (... && ...) || doc.indexAttribute IN non-empty-array

* upgraded arangodb starter version to 0.14.0

* upgraded arangosync version to 0.6.2

* fixed an issue where a crashed coordinator can lead to some Foxx queue jobs
  erroneously either left hanging or being restarted

* fix issue #7903: Regression on ISO8601 string compatibility in AQL

  millisecond parts of AQL date values were limited to up to 3 digits.
  Now the length of the millisecond part is unrestricted, but the
  millisecond precision is still limited to up to 3 digits.

* fix issue #7900: Bind values of `null` are not replaced by
  empty string anymore, when toggling between json and table
  view in the web-ui.

* Use base64url to encode and decode JWT parts.

* added AQL function `CHECK_DOCUMENT` for document validity checks

* when detecting parse errors in the JSON input sent to the restore API, now
  abort with a proper error containing the problem description instead of aborting
  but hiding there was a problem.

* do not respond with an internal error in case of JSON parse errors detected
  in incoming HTTP requests

* added arangorestore option `--cleanup-duplicate-attributes` to clean up input documents
  with redundant attribute names

  Importing such documents without the option set will make arangorestore fail with an
  error, and setting the option will make the restore process clean up the input by using
  just the first specified value for each redundant attribute.

* the arangorestore options `--default-number-of-shards` and `--default-replication-factor`
  are now deprecated in favor of the much more powerful options `--number-of-shards`
  and `--replication-factor`

  The new options `--number-of-shards` and `--replication-factor` allow specifying
  default values for the number of shards and the replication factor, resp. for all
  restored collections. If specified, these default values will be used regardless
  of whether the number of shards or the replication factor values are already present
  in the metadata of the dumped collections.

  It is also possible to override the values on a per-collection level by specifying
  the options multiple times, e.g.

      --number-of-shards 2 --number-of-shards mycollection=3 --number-of-shards test=4

  The above will create all collections with 2 shards, except the collection "mycollection"
  (3 shards) and "test" (4 shards).

  By omitting the default value, it is also possible to use the number of shards/replication
  factor values from the dump for all collections but the explicitly specified ones, e.g.

      --number-of-shards mycollection=3 --number-of-shards test=4

  This will use the number of shards as specified in the dump, except for the collections
  "mycollection" and "test".

  The `--replication-factor` option works similarly.

* validate uniqueness of attribute names in AQL in cases in which it was not
  done before. When constructing AQL objects via object literals, there was
  no validation about object attribute names being unique. For example, it was
  possible to create objects with duplicate attribute names as follows:

      INSERT { a: 1, a: 2 } INTO collection

  This resulted in a document having two "a" attributes, which is obviously
  undesired. Now, when an attribute value is used multiple times, only the first
  assigned value will be used for that attribute in AQL. It is not possible to
  specify the same attribute multiple times and overwrite the attribute's value
  with by that. That means in the above example, the value of "a" will be 1,
  and not 2.
  This changes the behavior for overriding attribute values in AQL compared to
  previous versions of ArangoDB, as previous versions in some cases allowed
  duplicate attribute names in objects/documents (which is undesired) and in
  other cases used the _last_ value assigned to an attribute instead of the _first_
  value. In order to explicitly override a value in an existing object, use the
  AQL MERGE function.

  To avoid all these issues, users are encouraged to use unambiguous attribute
  names in objects/documents in AQL. Outside of AQL, specifying the same attribute
  multiple times may even result in a parse error, e.g. when sending such data
  to ArangoDB's HTTP REST API.

* fixed issue #7834: AQL Query crashes instance

* Added --server.jwt-secret-keyfile option.

* Improve single threaded performance by scheduler optimization.

* Releveling logging in maintenance


v3.4.1 (2018-12-19)
-------------------

* fixed issue #7757: Using multiple filters on nested objects produces wrong results

* fixed issue #7763: Collect after update does not execute updates

* fixed issue #7586: a running query within the user interface was not shown
  if the active view was `Running Queries` or `Slow Query History`.

* fixed issue #7749: AQL Query result changed for COLLECT used on empty data/array

* fixed a rare thread local dead lock situation in replication:
  If a follower tries to get in sync in the last steps it requires
  a lock on the leader. If the follower cancels the lock before the leader
  has succeeded with locking we can end up with one thread being deadlocked.

* fix thread shutdown in _WIN32 builds

  Previous versions used a wrong comparison logic to determine the current
  thread id when shutting down a thread, leading to threads hanging in their
  destructors on thread shutdown

* reverted accidental change to error handling in geo index

  In previous versions, if non-valid geo coordinates were contained in the
  indexed field of a document, the document was simply ignored an not indexed.
  In 3.4.0, this was accidentally changed to generate an error, which caused
  the upgrade procedure to break in some cases.

* fixed TypeError being thrown instead of validation errors when Foxx manifest
  validation fails

* make AQL REMOVE operations use less memory with the RocksDB storage engine

  the previous implementation of batch removals read everything to remove into
  memory first before carrying out the first remove operation. The new version
  will only read in about 1000 documents each time and then remove these. Queries
  such as

      FOR doc IN collection FILTER ... REMOVE doc IN collection

  will benefit from this change in terms of memory usage.

* make `--help-all` now also show all hidden program options

  Previously hidden program options were only returned when invoking arangod or
  a client tool with the cryptic `--help-.` option. Now `--help-all` simply
  retuns them as well.

  The program options JSON description returned by `--dump-options` was also
  improved as follows:

  - the new boolean attribute "dynamic" indicates whether the option has a dynamic
    default value, i.e. a value that depends on the target host capabilities or
    configuration

  - the new boolean attribute "requiresValue" indicates whether a boolean option
    requires a value of "true" or "false" when specified. If "requiresValue" is
    false, then the option can be specified without a boolean value following it,
    and the option will still be set to true, e.g. `--server.authentication` is
    identical to `--server.authentication true`.

  - the new "category" attribute will contain a value of "command" for command-like
    options, such as `--version`, `--dump-options`, `--dump-dependencies` etc.,
    and "option" for all others.

* Fixed a bug in synchroneous replication intialization for, where a
  shard's db server is rebooted during that period


v3.4.0 (2018-12-06)
-------------------

* Add license key checking to enterprise version in Docker containers.


v3.4.0-rc.5 (2018-11-29)
------------------------

* Persist and check default language (locale) selection.
  Previously we would not check if the language (`--default-language`) had changed
  when the server was restarted. This could cause issues with indexes over text fields,
  as it will resulted in undefined behavior within RocksDB (potentially missing entries,
  corruption, etc.). Now if the language is changed, ArangoDB will print out an error
  message on startup and abort.

* fixed issue #7522: FILTER logic totally broke for my query in 3.4-rc4

* export version and storage engine in `_admin/cluster/health` for Coordinators
  and DBServers.

* restrict the total amount of data to build up in all in-memory RocksDB write buffers
  by default to a certain fraction of the available physical RAM. This helps restricting
  memory usage for the arangod process, but may have an effect on the RocksDB storage
  engine's write performance.

  In ArangoDB 3.3 the governing configuration option `--rocksdb.total-write-buffer-size`
  had a default value of `0`, which meant that the memory usage was not limited. ArangoDB
  3.4 now changes the default value to about 50% of available physical RAM, and 512MiB
  for setups with less than 4GiB of RAM.

* lower default value for `--cache.size` startup option from about 30% of physical RAM to
  about 25% percent of physical RAM.

* fix internal issue #2786: improved confirmation dialog when clicking the truncate
  button in the web UI

* Updated joi library (web UI), improved Foxx mount path validation

* disable startup warning for Linux kernel variable `vm.overcommit_memory` settings
  values of 0 or 1.
  Effectively `overcommit_memory` settings value of 0 or 1 fix two memory-allocation
  related issues with the default memory allocator used in ArangoDB release builds on
  64bit Linux.
  The issues will remain when running with an `overcommit_memory` settings value of 2,
  so this is now discouraged.
  Setting `overcommit_memory` to 0 or 1 (0 is the Linux kernel's default) fixes issues
  with increasing numbers of memory mappings for the arangod process (which may lead
  to an out-of-memory situation if the kernel's maximum number of mappings threshold
  is hit) and an increasing amount of memory that the kernel counts as "committed".
  With an `overcommit_memory` setting of 0 or 1, an arangod process may either be
  killed by the kernel's OOM killer or will die with a segfault when accessing memory
  it has allocated before but the kernel could not provide later on. This is still
  more acceptable than the kernel not providing any more memory to the process when
  there is still physical memory left, which may have occurred with an `overcommit_memory`
  setting of 2 after the arangod process had done lots of allocations.

  In summary, the recommendation for the `overcommit_memory` setting is now to set it
  to 0 or 1 (0 is kernel default) and not use 2.

* fixed Foxx complaining about valid `$schema` value in manifest.json

* fix for supervision, which started failing servers using old transient store

* fixed a bug where indexes are used in the cluster while still being
  built on the db servers

* fix move leader shard: wait until all but the old leader are in sync.
  This fixes some unstable tests.

* cluster health features more elaborate agent records

* agency's supervision edited for advertised endpoints


v3.4.0-rc.4 (2018-11-04)
------------------------

* fixed Foxx queues not retrying jobs with infinite `maxFailures`

* increase AQL query string parsing performance for queries with many (100K+) string
  values contained in the query string

* increase timeouts for inter-node communication in the cluster

* fixed undefined behavior in `/_api/import` when importing a single document went
  wrong

* replication bugfixes

* stop printing `connection class corrupted` in arangosh

 when just starting the arangosh without a connection to a server and running
 code such as `require("internal")`, the shell always printed "connection class
 corrupted", which was somewhat misleading.

* add separate option `--query.slow-streaming-threshold` for tracking slow
  streaming queries with a different timeout value

* increase maximum number of collections/shards in an AQL query from 256 to 2048

* don't rely on `_modules` collection being present and usable for arangod startup

* force connection timeout to be 7 seconds to allow libcurl time to retry lost DNS
  queries.

* fixes a routing issue within the web ui after the use of views

* fixes some graph data parsing issues in the ui, e.g. cleaning up duplicate
  edges inside the graph viewer.

* in a cluster environment, the arangod process now exits if wrong credentials
  are used during the startup process.

* added option `--rocksdb.total-write-buffer-size` to limit total memory usage
  across all RocksDB in-memory write buffers

* suppress warnings from statistics background threads such as
  `WARNING caught exception during statistics processing: Expecting Object`
  during version upgrade


v3.4.0-rc.3 (2018-10-23)
------------------------

* fixed handling of broken Foxx services

  Installation now also fails when the service encounters an error when
  executed. Upgrading or replacing with a broken service will still result
  in the broken services being installed.

* restored error pages for broken Foxx services

  Services that could not be executed will now show an error page (with helpful
  information if development mode is enabled) instead of a generic 404 response.
  Requests to the service that do not prefer HTML (i.e. not a browser window)
  will receive a JSON formatted 503 error response instead.

* added support for `force` flag when upgrading Foxx services

  Using the `force` flag when upgrading or replacing a service falls back to
  installing the service if it does not already exist.

* The order of JSON object attribute keys in JSON return values will now be
  "random" in more cases. In JSON, there is no defined order for object attribute
  keys anyway, so ArangoDB is taking the freedom to return the attribute keys in
  a non-deterministic, seemingly unordered way.

* Fixed an AQL bug where the `optimize-traversals` rule was falsely applied to
  extensions with inline expressions and thereby ignoring them

* fix side-effects of sorting larger arrays (>= 16 members) of constant literal
  values in AQL, when the array was used not only for IN-value filtering but also
  later in the query.
  The array values were sorted so the IN-value lookup could use a binary search
  instead of a linear search, but this did not take into account that the array
  could have been used elsewhere in the query, e.g. as a return value. The fix
  will create a copy of the array and sort the copy, leaving the original array
  untouched.

* disallow empty LDAP password

* fixes validation of allowed or not allowed foxx service mount paths within
  the Web UI

* The single database or single coordinator statistics in a cluster
  environment within the Web UI sometimes got called way too often.
  This caused artifacts in the graphs, which is now fixed.

* An aardvark statistics route could not collect and sum up the statistics of
  all coordinators if one of them was ahead and had more results than the others

* Web UI now checks if server statistics are enabled before it sends its first
  request to the statistics API

* fix internal issue #486: immediate deletion (right after creation) of
  a view with a link to one collection and indexed data reports failure
  but removes the link

* fix internal issue #480: link to a collection is not added to a view
  if it was already added to other view

* fix internal issues #407, #445: limit ArangoSearch memory consumption
  so that it won't cause OOM while indexing large collections

* upgraded arangodb starter version to 0.13.5

* removed undocumented `db.<view>.toArray()` function from ArangoShell

* prevent creation of collections and views with the same in cluster setups

* fixed issue #6770: document update: ignoreRevs parameter ignored

* added AQL query optimizer rules `simplify-conditions` and `fuse-filters`

* improve inter-server communication performance:
  - move all response processing off Communicator's socket management thread
  - create multiple Communicator objects with ClusterComm, route via round robin
  - adjust Scheduler threads to always be active, and have designated priorities.

* fix internal issue #2770: the Query Profiling modal dialog in the Web UI
  was slightly malformed.

* fix internal issue #2035: the Web UI now updates its indices view to check
  whether new indices exist or not.

* fix internal issue #6808: newly created databases within the Web UI did not
  appear when used Internet Explorer 11 as a browser.

* fix internal issue #2957: the Web UI was not able to display more than 1000
  documents, even when it was set to a higher amount.

* fix internal issue #2688: the Web UI's graph viewer created malformed node
  labels if a node was expanded multiple times.

* fix internal issue #2785: web ui's sort dialog sometimes got rendered, even
  if it should not.

* fix internal issue #2764: the waitForSync property of a satellite collection
  could not be changed via the Web UI

* dynamically manage libcurl's number of open connections to increase performance
  by reducing the number of socket close and then reopen cycles

* recover short server id from agency after a restart of a cluster node

  this fixes problems with short server ids being set to 0 after a node restart,
  which then prevented cursor result load-forwarding between multiple coordinators
  to work properly

  this should fix arangojs#573

* increased default timeouts in replication

  this decreases the chances of followers not getting in sync with leaders because
  of replication operations timing out

* include forward-ported diagnostic options for debugging LDAP connections

* fixed internal issue #3065: fix variable replacements by the AQL query
  optimizer in arangosearch view search conditions

  The consequence of the missing replacements was that some queries using view
  search conditions could have failed with error messages such as

  "missing variable #3 (a) for node #7 (EnumerateViewNode) while planning registers"

* fixed internal issue #1983: the Web UI was showing a deletion confirmation
  multiple times.

* Restricted usage of views in AQL, they will throw an error now
  (e.g. "FOR v, e, p IN 1 OUTBOUND @start edgeCollection, view")
  instead of failing the server.

* Allow VIEWs within the AQL "WITH" statement in cluster environment.
  This will now prepare the query for all collections linked within a view.
  (e.g. "WITH view FOR v, e, p IN OUTBOUND 'collectionInView/123' edgeCollection"
  will now be executed properly and not fail with unregistered collection any more)

* Properly check permissions for all collections linked to a view when
  instantiating an AQL query in cluster environment

* support installation of ArangoDB on Windows into directories with multibyte
  character filenames on Windows platforms that used a non-UTF8-codepage

  This was supported on other platforms before, but never worked for ArangoDB's
  Windows version

* display shard synchronization progress for collections outside of the
  `_system` database

* change memory protection settings for memory given back to by the bundled
  JEMalloc memory allocator. This avoids splitting of existing memory mappings
  due to changes of the protection settings

* added missing implementation for `DeleteRangeCF` in RocksDB WAL tailing handler

* fixed agents busy looping gossip

* handle missing `_frontend` collections gracefully

  the `_frontend` system collection is not required for normal ArangoDB operations,
  so if it is missing for whatever reason, ensure that normal operations can go
  on.


v3.4.0-rc.2 (2018-09-30)
------------------------

* upgraded arangosync version to 0.6.0

* upgraded arangodb starter version to 0.13.3

* fixed issue #6611: Properly display JSON properties of user defined foxx services
  configuration within the web UI

* improved shards display in web UI: included arrows to better visualize that
  collection name sections can be expanded and collapsed

* added nesting support for `aql` template strings

* added support for `undefined` and AQL literals to `aql.literal`

* added `aql.join` function

* fixed issue #6583: Agency node segfaults if sent an authenticated HTTP
  request is sent to its port

* fixed issue #6601: Context cancelled (never ending query)

* added more AQL query results cache inspection and control functionality

* fixed undefined behavior in AQL query result cache

* the query editor within the web UI is now catching HTTP 501 responses
  properly

* added AQL VERSION function to return the server version as a string

* added startup parameter `--cluster.advertised-endpoints`

* AQL query optimizer now makes better choices regarding indexes to use in a
  query when there are multiple competing indexes and some of them are prefixes
  of others

  In this case, the optimizer could have preferred indexes that covered less
  attributes, but it should rather pick the indexes that covered more attributes.

  For example, if there was an index on ["a"] and another index on ["a", "b"], then
  previously the optimizer may have picked the index on just ["a"] instead the
  index on ["a", "b"] for queries that used all index attributes but did range
  queries on them (e.g. `FILTER doc.a == @val1 && doc.b >= @val2`).

* Added compression for the AQL intermediate results transfer in the cluster,
  leading to less data being transferred between coordinator and database servers
  in many cases

* forward-ported a bugfix from RocksDB (https://github.com/facebook/rocksdb/pull/4386)
  that fixes range deletions (used internally in ArangoDB when dropping or truncating
  collections)

  The non-working range deletes could have triggered errors such as
  `deletion check in index drop failed - not all documents in the index have been deleted.`
  when dropping or truncating collections

* improve error messages in Windows installer

* allow retrying installation in Windows installer in case an existing database is still
  running and needs to be manually shut down before continuing with the installation

* fix database backup functionality in Windows installer

* fixed memory leak in `/_api/batch` REST handler

* `db._profileQuery()` now also tracks operations triggered when using `LIMIT`
  clauses in a query

* added proper error messages when using views as an argument to AQL functions
  (doing so triggered an `internal error` before)

* fixed return value encoding for collection ids ("cid" attribute") in REST API
  `/_api/replication/logger-follow`

* fixed dumping and restoring of views with arangodump and arangorestore

* fix replication from 3.3 to 3.4

* fixed some TLS errors that occurred when combining HTTPS/TLS transport with the
  VelocyStream protocol (VST)

  That combination could have led to spurious errors such as "TLS padding error"
  or "Tag mismatch" and connections being closed

* make synchronous replication detect more error cases when followers cannot
  apply the changes from the leader

* fixed issue #6379: RocksDB arangorestore time degeneration on dead documents

* fixed issue #6495: Document not found when removing records

* fixed undefined behavior in cluster plan-loading procedure that may have
  unintentionally modified a shared structure

* reduce overhead of function initialization in AQL COLLECT aggregate functions,
  for functions COUNT/LENGTH, SUM and AVG

  this optimization will only be noticable when the COLLECT produces many groups
  and the "hash" COLLECT variant is used

* fixed potential out-of-bounds access in admin log REST handler `/_admin/log`,
  which could have led to the server returning an HTTP 500 error

* catch more exceptions in replication and handle them appropriately

* agency endpoint updates now go through RAFT

* fixed a cleanup issue in Current when a follower was removed from Plan

* catch exceptions in MaintenanceWorker thread

* fixed a bug in cleanOutServer which could lead to a cleaned out server
  still being a follower for some shard

v3.4.0-rc.1 (2018-09-06)
------------------------

* Release Candidate for 3.4.0, please check the `ReleaseNotes/KnownIssues34.md`
  file for a list of known issues.

* upgraded bundled RocksDB version to 5.16.0

* upgraded bundled Snappy compression library to 1.1.7

* fixed issue #5941: if using breadth first search in traversals uniqueness checks
  on path (vertices and edges) have not been applied. In SmartGraphs the checks
  have been executed properly.

* added more detailed progress output to arangorestore, showing the percentage of
  how much data is restored for bigger collections plus a set of overview statistics
  after each processed collection

* added option `--rocksdb.use-file-logging` to enable writing of RocksDB's own
  informational LOG files into RocksDB's database directory.

  This option is turned off by default, but can be enabled for debugging RocksDB
  internals and performance.

* improved error messages when managing Foxx services

  Install/replace/upgrade will now provide additional information when an error
  is encountered during setup. Errors encountered during a `require` call will
  also include information about the underlying cause in the error message.

* fixed some Foxx script names being displayed incorrectly in web UI and Foxx CLI

* major revision of the maintenance feature

* added `uuidv4` and `genRandomBytes` methods to crypto module

* added `hexSlice` methods `hexWrite` to JS Buffer type

* added `Buffer.from`, `Buffer.of`, `Buffer.alloc` and `Buffer.allocUnsafe`
  for improved compatibility with Node.js

* Foxx HTTP API errors now log stacktraces

* fixed issue #5831: custom queries in the ui could not be loaded if the user
  only has read access to the _system database.

* fixed issue #6128: ArangoDb Cluster: Task moved from DBS to Coordinator

* fixed some web ui action events related to Running Queries view and Slow
  Queries History view

* fixed internal issue #2566: corrected web UI alignment of the nodes table

* fixed issue #5736: Foxx HTTP API responds with 500 error when request body
  is too short

* fixed issue #6106: Arithmetic operator type casting documentation incorrect

* The arangosh now supports the velocystream transport protocol via the schemas
  "vst+tcp://", "vst+ssl://", "vst+unix://" schemes.

* The server will no longer lowercase the input in --server.endpoint. This means
  Unix domain socket paths will now  be treated as specified, previously they were lowercased

* fixed logging of requests. A wrong log level was used

* fixed issue #5943: misplaced database ui icon and wrong cursor type were used

* fixed issue #5354: updated the web UI JSON editor, improved usability

* fixed issue #5648: fixed error message when saving unsupported document types

* fixed internal issue #2812: Cluster fails to create many indexes in parallel

* Added C++ implementation, load balancer support, and user restriction to Pregel API.

  If an execution is accessed on a different coordinator than where it was
  created, the request(s) will be forwarded to the correct coordinator. If an
  execution is accessed by a different user than the one who created it, the
  request will be denied.

* the AQL editor in the web UI now supports detailed AQL query profiling

* fixed issue #5884: Subquery nodes are no longer created on DBServers

* intermediate commits in the RocksDB engine are now only enabled in standalone AQL queries

  (not within a JS transaction), standalone truncate as well as for the "import" API

* the AQL editor in the web UI now supports GeoJSON types and is able to render them.

* fixed issue #5035: fixed a vulnerability issue within the web ui's index view

* PR #5552: add "--latency true" option to arangoimport.  Lists microsecond latency

* added `"pbkdf2"` method to `@arangodb/foxx/auth` module

* the `@arangodb/foxx/auth` module now uses a different method to generate salts,
  so salts are no longer guaranteed to be alphanumeric

* fixed internal issue #2567: the Web UI was showing the possibility to move a shard
  from a follower to the current leader

* Renamed RocksDB engine-specific statistics figure `rocksdb.block-cache-used`
  to `rocksdb.block-cache-usage` in output of `db._engineStats()`

  The new figure name is in line with the statistics that the RocksDB library
  provides in its new versions.

* Added RocksDB engine-specific statistics figures `rocksdb.block-cache-capacity`,
  `rocksdb.block-cache-pinned-usage` as well as level-specific figures
  `rocksdb.num-files-at-level` and `rocksdb.compression-ratio-at-level` in
  output of `db._engineStats()`

* Added RocksDB-engine configuration option `--rocksdb.block-align-data-blocks`

  If set to true, data blocks are aligned on lesser of page size and block size,
  which may waste some memory but may reduce the number of cross-page I/Os operations.

* Usage RocksDB format version 3 for new block-based tables

* Bugfix: The AQL syntax variants `UPDATE/REPLACE k WITH d` now correctly take
  _rev from k instead of d (when ignoreRevs is false) and ignore d._rev.

* Added C++ implementation, load balancer support, and user restriction to tasks API

  If a task is accessed on a different coordinator than where it was created,
  the request(s) will be forwarded to the correct coordinator. If a
  task is accessed by a different user than the one who created it, the request
  will be denied.

* Added load balancer support and user-restriction to async jobs API.

  If an async job is accessed on a different coordinator than where it was
  created, the request(s) will be forwarded to the correct coordinator. If a
  job is accessed by a different user than the one who created it, the request
  will be denied.

* switch default storage engine from MMFiles to RocksDB

  In ArangoDB 3.4, the default storage engine for new installations is the RocksDB
  engine. This differs to previous versions (3.2 and 3.3), in which the default
  storage engine was the MMFiles engine.

  The MMFiles engine can still be explicitly selected as the storage engine for
  all new installations. It's only that the "auto" setting for selecting the storage
  engine will now use the RocksDB engine instead of MMFiles engine.

  In the following scenarios, the effectively selected storage engine for new
  installations will be RocksDB:

  * `--server.storage-engine rocksdb`
  * `--server.storage-engine auto`
  * `--server.storage-engine` option not specified

  The MMFiles storage engine will be selected for new installations only when
  explicitly selected:

  * `--server.storage-engine mmfiles`

  On upgrade, any existing ArangoDB installation will keep its previously selected
  storage engine. The change of the default storage engine is thus only relevant
  for new ArangoDB installations and/or existing cluster setups for which new server
  nodes get added later. All server nodes in a cluster setup should use the same
  storage engine to work reliably. Using different storage engines in a cluster is
  unsupported.

* added collection.indexes() as an alias for collection.getIndexes()

* disable V8 engine and JavaScript APIs for agency nodes

* renamed MMFiles engine compactor thread from "Compactor" to "MMFilesCompactor".

  This change will be visible only on systems which allow assigning names to
  threads.

* added configuration option `--rocksdb.sync-interval`

  This option specifies interval (in milliseconds) that ArangoDB will use to
  automatically synchronize data in RocksDB's write-ahead log (WAL) files to
  disk. Automatic syncs will only be performed for not-yet synchronized data,
  and only for operations that have been executed without the *waitForSync*
  attribute.

  Automatic synchronization is performed by a background thread. The default
  sync interval is 100 milliseconds.

  Note: this option is not supported on Windows platforms. Setting the sync
  interval to a value greater 0 will produce a startup warning.

* added AQL functions `TO_BASE64`, `TO_HEX`, `ENCODE_URI_COMPONENT` and `SOUNDEX`

* PR #5857: RocksDB engine would frequently request a new DelayToken.  This caused
  excessive write delay on the next Put() call.  Alternate approach taken.

* changed the thread handling in the scheduler. `--server.maximal-threads` will be
  the maximum number of threads for the scheduler.

* The option `--server.threads` is now obsolete.

* use sparse indexes in more cases now, when it is clear that the index attribute
  value cannot be null

* introduce SingleRemoteOperationNode via "optimize-cluster-single-document-operations"
  optimizer rule, which triggers single document operations directly from the coordinator
  instead of using a full-featured AQL setup. This saves cluster roundtrips.

  Queries directly referencing the document key benefit from this:

      UPDATE {_key: '1'} WITH {foo: 'bar'} IN collection RETURN OLD

* Added load balancer support and user-restriction to cursor API.

  If a cursor is accessed on a different coordinator than where it was created,
  the requests will be forwarded to the correct coordinator. If a cursor is
  accessed by a different user than the one who created it, the request will
  be denied.

* if authentication is turned on requests to databases by users with insufficient rights
 will be answered with the HTTP forbidden (401) response.

* upgraded bundled RocksDB library version to 5.15

* added key generators `uuid` and `padded`

  The `uuid` key generator generates universally unique 128 bit keys, which are
  stored in hexadecimal human-readable format.
  The `padded` key generator generates keys of a fixed length (16 bytes) in
  ascending lexicographical sort order.

* The REST API of `/_admin/status` added: "operationMode" filed with same meaning as
  the "mode" field and field "readOnly" that has the inverted meaning of the field
  "writeOpsEnabled". The old field names will be deprecated in upcoming versions.

* added `COUNT_DISTINCT` AQL function

* make AQL optimizer rule `collect-in-cluster` optimize aggregation functions
  `AVERAGE`, `VARIANCE`, `STDDEV`, `UNIQUE`, `SORTED_UNIQUE` and `COUNT_DISTINCT`
  in a cluster by pushing parts of the aggregation onto the DB servers and only
  doing the total aggregation on the coordinator

* replace JavaScript functions FULLTEXT, NEAR, WITHIN and WITHIN_RECTANGLE with
  regular AQL subqueries via a new optimizer rule "replace-function-with-index".

* the existing "fulltext-index-optimizer" optimizer rule has been removed because its
  duty is now handled by the "replace-function-with-index" rule.

* added option "--latency true" option to arangoimport. Lists microsecond latency
  statistics on 10 second intervals.

* fixed internal issue #2256: ui, document id not showing up when deleting a document

* fixed internal issue #2163: wrong labels within foxx validation of service
  input parameters

* fixed internal issue #2160: fixed misplaced tooltips in indices view

* Added exclusive option for rocksdb collections. Modifying AQL queries can
  now set the exclusive option as well as it can be set on JavaScript transactions.

* added optimizer rule "optimize-subqueries", which makes qualifying subqueries
  return less data

  The rule fires in the following situations:
  * in case only a few results are used from a non-modifying subquery, the rule
    will add a LIMIT statement into the subquery. For example

        LET docs = (
          FOR doc IN collection
            FILTER ...
            RETURN doc
        )
        RETURN docs[0]

    will be turned into

        LET docs = (
          FOR doc IN collection
            FILTER ...
            LIMIT 1
            RETURN doc
        )
        RETURN docs[0]

    Another optimization performed by this rule is to modify the result value
    of subqueries in case only the number of results is checked later. For example

        RETURN LENGTH(
          FOR doc IN collection
            FILTER ...
            RETURN doc
        )

    will be turned into

        RETURN LENGTH(
          FOR doc IN collection
            FILTER ...
            RETURN true
        )

  This saves copying the document data from the subquery to the outer scope and may
  enable follow-up optimizations.

* fixed Foxx queues bug when queues are created in a request handler with an
  ArangoDB authentication header

* abort startup when using SSLv2 for a server endpoint, or when connecting with
  a client tool via an SSLv2 connection.

  SSLv2 has been disabled in the OpenSSL library by default in recent versions
  because of security vulnerabilities inherent in this protocol.

  As it is not safe at all to use this protocol, the support for it has also
  been stopped in ArangoDB. End users that use SSLv2 for connecting to ArangoDB
  should change the protocol from SSLv2 to TLSv12 if possible, by adjusting
  the value of the `--ssl.protocol` startup option.

* added `overwrite` option to document insert operations to allow for easier syncing.

  This implements almost the much inquired UPSERT. In reality it is a REPSERT
  (replace/insert) because only replacement and not modification of documents
  is possible. The option does not work in cluster collections with custom
  sharding.

* added startup option `--log.escape`

  This option toggles the escaping of log output.

  If set to `true` (which is the default value), then the logging will work
  as before, and the following characters in the log output are escaped:

  * the carriage return character (hex 0d)
  * the newline character (hex 0a)
  * the tabstop character (hex 09)
  * any other characters with an ordinal value less than hex 20

  If the option is set to `false`, no characters are escaped. Characters with
  an ordinal value less than hex 20 will not be printed in this mode but will
  be replaced with a space character (hex 20).

  A side effect of turning off the escaping is that it will reduce the CPU
  overhead for the logging. However, this will only be noticable when logging
  is set to a very verbose level (e.g. debug or trace).

* increased the default values for the startup options `--javascript.gc-interval`
  from every 1000 to every 2000 requests, and for `--javascript.gc-frequency` from
  30 to 60 seconds

  This will make the V8 garbage collection run less often by default than in previous
  versions, reducing CPU load a bit and leaving more contexts available on average.

* added `/_admin/repair/distributeShardsLike` that repairs collections with
  distributeShardsLike where the shards aren't actually distributed like in the
  prototype collection, as could happen due to internal issue #1770

* Fixed issue #4271: Change the behavior of the `fullCount` option for AQL query
  cursors so that it will only take into account `LIMIT` statements on the top level
  of the query.

  `LIMIT` statements in subqueries will not have any effect on the `fullCount` results
  any more.

* We added a new geo-spatial index implementation. On the RocksDB storage engine all
  installations will need to be upgraded with `--database.auto-upgrade true`. New geo
  indexes will now only report with the type `geo` instead of `geo1` or `geo2`.
  The index types `geo1` and `geo2` are now deprecated.
  Additionally we removed the deprecated flags `constraint` and `ignoreNull` from geo
  index definitions, these fields were initially deprecated in ArangoDB 2.5

* Add revision id to RocksDB values in primary indexes to speed up replication (~10x).

* PR #5238: Create a default pacing algorithm for arangoimport to avoid TimeoutErrors
  on VMs with limited disk throughput

* Starting a cluster with coordinators and DB servers using different storage engines
  is unsupported. Doing it anyway will now produce a warning on startup

* fixed issue #4919: C++ implementation of LIKE function now matches the old and correct
  behavior of the javascript implementation.

* added `--json` option to arangovpack, allowing to treat its input as plain JSON data
  make arangovpack work without any configuration file

* added experimental arangodb startup option `--javascript.enabled` to enable/disable the
  initialization of the V8 JavaScript engine. Only expected to work on single-servers and
  agency deployments

* pull request #5201: eliminate race scenario where handlePlanChange could run infinite times
  after an execution exceeded 7.4 second time span

* UI: fixed an unreasonable event bug within the modal view engine

* pull request #5114: detect shutdown more quickly on heartbeat thread of coordinator and
  DB servers

* fixed issue #3811: gharial api is now checking existence of `_from` and `_to` vertices
  during edge creation

* There is a new method `_profileQuery` on the database object to execute a query and
  print an explain with annotated runtime information.

* Query cursors can now be created with option `profile`, with a value of 0, 1 or 2.
  This will cause queries to include more statistics in their results and will allow tracing
  of queries.

* fixed internal issue #2147: fixed database filter in UI

* fixed internal issue #2149: number of documents in the UI is not adjusted after moving them

* fixed internal issue #2150: UI - loading a saved query does not update the list of bind
  parameters

* removed option `--cluster.my-local-info` in favor of persisted server UUIDs

  The option `--cluster.my-local-info` was deprecated since ArangoDB 3.3.

* added new collection property `cacheEnabled` which enables in-memory caching for
  documents and primary index entries. Available only when using RocksDB

* arangodump now supports `--threads` option to dump collections in parallel

* arangorestore now supports `--threads` option to restore collections in parallel

* Improvement: The AQL query planner in cluster is now a bit more clever and
  can prepare AQL queries with less network overhead.

  This should speed up simple queries in cluster mode, on complex queries it
  will most likely not show any performance effect.
  It will especially show effects on collections with a very high amount of Shards.

* removed remainders of dysfunctional `/_admin/cluster-test` and `/_admin/clusterCheckPort`
  API endpoints and removed them from documentation

* added new query option `stream` to enable streaming query execution via the
  `POST /_api/cursor` rest interface.

* fixed issue #4698: databases within the UI are now displayed in a sorted order.

* Behavior of permissions for databases and collections changed:
  The new fallback rule for databases for which an access level is not explicitly specified:
  Choose the higher access level of:
    * A wildcard database grant
    * A database grant on the `_system` database
  The new fallback rule for collections for which an access level is not explicitly specified:
  Choose the higher access level of:
    * Any wildcard access grant in the same database, or on "*/*"
    * The access level for the current database
    * The access level for the `_system` database

* fixed issue #4583: add AQL ASSERT and AQL WARN

* renamed startup option `--replication.automatic-failover` to
  `--replication.active-failover`
  using the old option name will still work in ArangoDB 3.4, but the old option
  will be removed afterwards

* index selectivity estimates for RocksDB engine are now eventually consistent

  This change addresses a previous issue where some index updates could be
  "lost" from the view of the internal selectivity estimate, leading to
  inaccurate estimates. The issue is solved now, but there can be up to a second
  or so delay before updates are reflected in the estimates.

* support `returnOld` and `returnNew` attributes for in the following HTTP REST
  APIs:

  * /_api/gharial/<graph>/vertex/<collection>
  * /_api/gharial/<graph>/edge/<collection>

  The exception from this is that the HTTP DELETE verb for these APIs does not
  support `returnOld` because that would make the existing API incompatible

* fixed internal issue #478: remove unused and undocumented REST API endpoints
  _admin/statistics/short and _admin/statistics/long

  These APIs were available in ArangoDB's REST API, but have not been called by
  ArangoDB itself nor have they been part of the documented API. They have been
  superseded by other REST APIs and were partially dysfunctional. Therefore
  these two endpoints have been removed entirely.

* fixed issue #1532: reload users on restore

* fixed internal issue #1475: when restoring a cluster dump to a single server
  ignore indexes of type primary and edge since we mustn't create them here.

* fixed internal issue #1439: improve performance of any-iterator for RocksDB

* issue #1190: added option `--create-database` for arangoimport

* UI: updated dygraph js library to version 2.1.0

* renamed arangoimp to arangoimport for consistency
  Release packages will still install arangoimp as a symlink so user scripts
  invoking arangoimp do not need to be changed

* UI: Shard distribution view now has an accordion view instead of displaying
  all shards of all collections at once.

* fixed issue #4393: broken handling of unix domain sockets in JS_Download

* added AQL function `IS_KEY`
  this function checks if the value passed to it can be used as a document key,
  i.e. as the value of the `_key` attribute

* added AQL functions `SORTED` and `SORTED_UNIQUE`

  `SORTED` will return a sorted version of the input array using AQL's internal
  comparison order
  `SORTED_UNIQUE` will do the same, but additionally removes duplicates.

* added C++ implementation for AQL functions `DATE_NOW`, `DATE_ISO8601`,
  `DATE_TIMESTAMP`, `IS_DATESTRING`, `DATE_DAYOFWEEK`, `DATE_YEAR`,
  `DATE_MONTH`, `DATE_DAY`, `DATE_HOUR`, `DATE_MINUTE`, `DATE_SECOND`,
  `DATE_MILLISECOND`, `DATE_DAYOFYEAR`, `DATE_ISOWEEK`, `DATE_LEAPYEAR`,
  `DATE_QUARTER`, `DATE_DAYS_IN_MONTH`, `DATE_ADD`, `DATE_SUBTRACT`,
  `DATE_DIFF`, `DATE_COMPARE`, `TRANSLATE` and `SHA512`

* fixed a bug where clusterinfo missed changes to plan after agency
  callback is registred for create collection

* Foxx manifest.json files can now contain a $schema key with the value
  of "http://json.schemastore.org/foxx-manifest" to improve tooling support.

* fixed agency restart from compaction without data

* fixed agency's log compaction for internal issue #2249

* only load Plan and Current from agency when actually needed


v3.3.21 (XXXX-XX-XX)
--------------------

* fixed TypeError being thrown instead of validation errors when Foxx manifest
  validation fails

* fixed issue #7586: a running query within the user interface was not shown
  if the active view was `Running Queries` or `Slow Query History`.

* improve Windows installer error messages, fix Windows installer backup routine
  and exit code handling

* make AQL REMOVE operations use less memory with the RocksDB storage engine

  the previous implementation of batch removals read everything to remove into
  memory first before carrying out the first remove operation. The new version
  will only read in about 1000 documents each time and then remove these. Queries
  such as

      FOR doc IN collection FILTER ... REMOVE doc IN collection

  will benefit from this change in terms of memory usage.


v3.3.20 (2018-11-28)
--------------------

* upgraded arangodb starter version to 0.13.9

* Added RocksDB option `--rocksdb.total-write-buffer-size` to limit total memory
  usage across all RocksDB in-memory write buffers

  The total amount of data to build up in all in-memory buffers (backed by log
  files). This option, together with the block cache size configuration option,
  can be used to limit memory usage. If set to 0, the memory usage is not limited.
  This is the default setting in 3.3. The default setting may be adjusted in
  future versions of ArangoDB.

  If set to a value greater than 0, this will cap the memory usage for write buffers,
  but may have an effect on write performance.

* Added RocksDB configuration option `--rocksdb.enforce-block-cache-size-limit`

  Whether or not the maximum size of the RocksDB block cache is strictly enforced.
  This option can be set to limit the memory usage of the block cache to at most the
  specified size. If then inserting a data block into the cache would exceed the
  cache's capacity, the data block will not be inserted. If the flag is not set,
  a data block may still get inserted into the cache. It is evicted later, but the
  cache may temporarily grow beyond its capacity limit.

* Export version and storage engine in cluster health

* Potential fix for issue #7407: arangorestore very slow converting from
  mmfiles to rocksdb

* Updated joi library (Web UI), improved foxx mount path validation

* fix internal issue #2786: improved confirmation dialog when clicking the
  Truncate button in the Web UI

* fix for supervision, which started failing servers using old transient store

* fixed Foxx queues not retrying jobs with infinite `maxFailures`

* Fixed a race condition in a coordinator, it could happen in rare cases and
  only with the maintainer mode enabled if the creation of a collection is in
  progress and at the same time a deletion is forced.

* disable startup warning for Linux kernel variable `vm.overcommit_memory` settings
  values of 0 or 1.
  Effectively `overcommit_memory` settings value of 0 or 1 fix two memory-allocation
  related issues with the default memory allocator used in ArangoDB release builds on
  64bit Linux.
  The issues will remain when running with an `overcommit_memory` settings value of 2,
  so this is now discouraged.
  Setting `overcommit_memory` to 0 or 1 (0 is the Linux kernel's default) fixes issues
  with increasing numbers of memory mappings for the arangod process (which may lead
  to an out-of-memory situation if the kernel's maximum number of mappings threshold
  is hit) and an increasing amount of memory that the kernel counts as "committed".
  With an `overcommit_memory` setting of 0 or 1, an arangod process may either be
  killed by the kernel's OOM killer or will die with a segfault when accessing memory
  it has allocated before but the kernel could not provide later on. This is still
  more acceptable than the kernel not providing any more memory to the process when
  there is still physical memory left, which may have occurred with an `overcommit_memory`
  setting of 2 after the arangod process had done lots of allocations.

  In summary, the recommendation for the `overcommit_memory` setting is now to set it
  to 0 or 1 (0 is kernel default) and not use 2.

* force connection timeout to be 7 seconds to allow libcurl time to retry lost DNS
  queries.

* increase maximum number of collections/shards in an AQL query from 256 to 2048

* don't rely on `_modules` collection being present and usable for arangod startup

* optimizes the web ui's routing which could possibly led to unwanted events.

* fixes some graph data parsing issues in the ui, e.g. cleaning up duplicate
  edges inside the graph viewer.

* in a cluster environment, the arangod process now exits if wrong credentials
  are used during the startup process.

* Fixed an AQL bug where the optimize-traversals rule was falsely applied to
  extensions with inline expressions and thereby ignoring them

* fix side-effects of sorting larger arrays (>= 16 members) of constant literal
  values in AQL, when the array was not used only for IN-value filtering but also
  later in the query.
  The array values were sorted so the IN-value lookup could use a binary search
  instead of a linear search, but this did not take into account that the array
  could have been used elsewhere in the query, e.g. as a return value. The fix
  will create a copy of the array and sort the copy, leaving the original array
  untouched.

* fixed a bug when cluster indexes were usable for queries, while
  still being built on db servers

* fix move leader shard: wait until all but the old leader are in sync.
  This fixes some unstable tests.

* cluster health features more elaborate agent records


v3.3.19 (2018-10-20)
--------------------

* fixes validation of allowed or not allowed foxx service mount paths within
  the Web UI

* The single database or single coordinator statistics in a cluster
  environment within the Web UI sometimes got called way too often.
  This caused artifacts in the graphs, which is now fixed.

* An aardvark statistics route could not collect and sum up the statistics of
  all coordinators if one of them was ahead and had more results than the others

* upgraded arangodb starter version to 0.13.6

* turn on intermediate commits in replication applier in order to decrease
  the size of transactional operations on replication (issue #6821)

* fixed issue #6770: document update: ignoreRevs parameter ignored

* when returning memory to the OS, use the same memory protection flags as
  when initializing the memory

  this prevents "hole punching" and keeps the OS from splitting one memory
  mapping into multiple mappings with different memory protection settings

* fix internal issue #2770: the Query Profiling modal dialog in the Web UI
  was slightly malformed.

* fix internal issue #2035: the Web UI now updates its indices view to check
  whether new indices exist or not.

* fix internal issue #6808: newly created databases within the Web UI did not
  appear when used Internet Explorer 11 as a browser.

* fix internal issue #2688: the Web UI's graph viewer created malformed node
  labels if a node was expanded multiple times.

* fix internal issue #2957: the Web UI was not able to display more than 1000
  documents, even when it was set to a higher amount.

* fix internal issue #2785: web ui's sort dialog sometimes got rendered, even
  if it should not.

* fix internal issue #2764: the waitForSync property of a satellite collection
  could not be changed via the Web UI

* improved logging in case of replication errors

* recover short server id from agency after a restart of a cluster node

  this fixes problems with short server ids being set to 0 after a node restart,
  which then prevented cursor result load-forwarding between multiple coordinators
  to work properly

  this should fix arangojs#573

* increased default timeouts in replication

  this decreases the chances of followers not getting in sync with leaders because
  of replication operations timing out

* fixed internal issue #1983: the Web UI was showing a deletion confirmation
  multiple times.

* fixed agents busy looping gossip

* handle missing `_frontend` collections gracefully

  the `_frontend` system collection is not required for normal ArangoDB operations,
  so if it is missing for whatever reason, ensure that normal operations can go
  on.


v3.3.18
-------

* not released


v3.3.17 (2018-10-04)
--------------------

* upgraded arangosync version to 0.6.0

* added several advanced options for configuring and debugging LDAP connections.
  Please note that some of the following options are platform-specific and may not
  work on all platforms or with all LDAP servers reliably:

  - `--ldap.serialized`: whether or not calls into the underlying LDAP library
    should be serialized.
    This option can be used to work around thread-unsafe LDAP library functionality.
  - `--ldap.serialize-timeout`: sets the timeout value that is used when waiting to
    enter the LDAP library call serialization lock. This is only meaningful when
    `--ldap.serialized` has been set to `true`.
  - `--ldap.retries`: number of tries to attempt a connection. Setting this to values
    greater than one will make ArangoDB retry to contact the LDAP server in case no
    connection can be made initially.
  - `--ldap.restart`: whether or not the LDAP library should implicitly restart
    connections
  - `--ldap.referrals`: whether or not the LDAP library should implicitly chase
    referrals
  - `--ldap.debug`: turn on internal OpenLDAP library output (warning: will print
    to stdout).
  - `--ldap.timeout`: timeout value (in seconds) for synchronous LDAP API calls
    (a value of 0 means default timeout).
  - `--ldap.network-timeout`: timeout value (in seconds) after which network operations
    following the initial connection return in case of no activity (a value of 0 means
    default timeout).
  - `--ldap.async-connect`: whether or not the connection to the LDAP library will
    be done asynchronously.

* fixed a shutdown race in ArangoDB's logger, which could have led to some buffered
  log messages being discarded on shutdown

* display shard synchronization progress for collections outside of the
  `_system` database

* fixed issue #6611: Properly display JSON properties of user defined foxx services
  configuration within the web UI

* fixed issue #6583: Agency node segfaults if sent an authenticated HTTP request is sent to its port

* when cleaning out a leader it could happen that it became follower instead of
  being removed completely

* make synchronous replication detect more error cases when followers cannot
  apply the changes from the leader

* fix some TLS errors that occurred when combining HTTPS/TLS transport with the
  VelocyStream protocol (VST)

  That combination could have led to spurious errors such as "TLS padding error"
  or "Tag mismatch" and connections being closed

* agency endpoint updates now go through RAFT


v3.3.16 (2018-09-19)
--------------------

* fix undefined behavior in AQL query result cache

* the query editor within the web ui is now catching http 501 responses
  properly

* fixed issue #6495 (Document not found when removing records)

* fixed undefined behavior in cluster plan-loading procedure that may have
  unintentionally modified a shared structure

* reduce overhead of function initialization in AQL COLLECT aggregate functions,
  for functions COUNT/LENGTH, SUM and AVG

  this optimization will only be noticable when the COLLECT produces many groups
  and the "hash" COLLECT variant is used

* fixed potential out-of-bounds access in admin log REST handler /_admin/log,
  which could have led to the server returning an HTTP 500 error

* catch more exceptions in replication and handle them appropriately


v3.3.15 (2018-09-10)
--------------------

* fixed an issue in the "sorted" AQL COLLECT variant, that may have led to producing
  an incorrect number of results

* upgraded arangodb starter version to 0.13.3

* fixed issue #5941 if using breadth-first search in traversals uniqueness checks
  on path (vertices and edges) have not been applied. In SmartGraphs the checks
  have been executed properly.

* added more detailed progress output to arangorestore, showing the percentage of
  how much data is restored for bigger collections plus a set of overview statistics
  after each processed collection

* added option `--rocksdb.use-file-logging` to enable writing of RocksDB's own
  informational LOG files into RocksDB's database directory.

  This option is turned off by default, but can be enabled for debugging RocksDB
  internals and performance.

* improved error messages when managing Foxx services

  Install/replace/upgrade will now provide additional information when an error
  is encountered during setup. Errors encountered during a `require` call will
  also include information about the underlying cause in the error message.

* fixed some Foxx script names being displayed incorrectly in web UI and Foxx CLI

* added startup option `--query.optimizer-max-plans value`

  This option allows limiting the number of query execution plans created by the
  AQL optimizer for any incoming queries. The default value is `128`.

  By adjusting this value it can be controlled how many different query execution
  plans the AQL query optimizer will generate at most for any given AQL query.
  Normally the AQL query optimizer will generate a single execution plan per AQL query,
  but there are some cases in which it creates multiple competing plans. More plans
  can lead to better optimized queries, however, plan creation has its costs. The
  more plans are created and shipped through the optimization pipeline, the more time
  will be spent in the optimizer.

  Lowering this option's value will make the optimizer stop creating additional plans
  when it has already created enough plans.

  Note that this setting controls the default maximum number of plans to create. The
  value can still be adjusted on a per-query basis by setting the *maxNumberOfPlans*
  attribute when running a query.

  This change also lowers the default maximum number of query plans from 192 to 128.

* bug fix: facilitate faster shutdown of coordinators and db servers

* cluster nodes should retry registering in agency until successful

* fixed some web ui action events related to Running Queries view and Slow
  Queries History view

* Create a default pacing algorithm for arangoimport to avoid TimeoutErrors
  on VMs with limited disk throughput

* backport PR 6150: establish unique function to indicate when
  application is terminating and therefore network retries should not occur

* backport PR #5201: eliminate race scenario where handlePlanChange
  could run infinite times after an execution exceeded 7.4 second time span


v3.3.14 (2018-08-15)
--------------------

* upgraded arangodb starter version to 0.13.1

* Foxx HTTP API errors now log stacktraces

* fixed issue #5736: Foxx HTTP API responds with 500 error when request body
  is too short

* fixed issue #5831: custom queries in the ui could not be loaded if the user
  only has read access to the _system database.

* fixed internal issue #2566: corrected web UI alignment of the nodes table

* fixed internal issue #2869: when attaching a follower with global applier to an
  authenticated leader already existing users have not been replicated, all users
  created/modified later are replicated.

* fixed internal issue #2865: dumping from an authenticated arangodb the users have
  not been included

* fixed issue #5943: misplaced database ui icon and wrong cursor type were used

* fixed issue #5354: updated the web UI JSON editor, improved usability

* fixed issue #5648: fixed error message when saving unsupported document types

* fixed issue #6076: Segmentation fault after AQL query

  This also fixes issues #6131 and #6174

* fixed issue #5884: Subquery nodes are no longer created on DBServers

* fixed issue #6031: Broken LIMIT in nested list iterations

* fixed internal issue #2812: Cluster fails to create many indexes in parallel

* intermediate commits in the RocksDB engine are now only enabled in standalone AQL
  queries (not within a JS transaction), standalone truncate as well as for the
  "import" API

* Bug fix: race condition could request data from Agency registry that did not
  exist yet.  This caused a throw that would end the Supervision thread.
  All registry query APIs no longer throw exceptions.


v3.3.13 (2018-07-26)
--------------------

* fixed internal issue #2567: the Web UI was showing the possibility to move a
  shard from a follower to the current leader

* fixed issue #5977: Unexpected execution plan when subquery contains COLLECT

* Bugfix: The AQL syntax variants `UPDATE/REPLACE k WITH d` now correctly take
  _rev from k instead of d (when ignoreRevs is false) and ignore d._rev.

* put an upper bound on the number of documents to be scanned when using
  `db.<collection>.any()` in the RocksDB storage engine

  previous versions of ArangoDB did a scan of a random amount of documents in
  the collection, up to the total number of documents available. this produced
  a random selection with a good quality, but needed to scan half the number
  of documents in the collection on average.

  The new version will only scan up to 500 documents, so it produces a less
  random result, but will be a lot faster especially for large collections.

  The implementation of `any()` for the MMFiles engine remains unchanged. The
  MMFiles engine will pick a random document from the entire range of the
  in-memory primary index without performing scans.

* return an empty result set instead of an "out of memory" exception when
  querying the geo index with invalid (out of range) coordinates

* added load balancer support and user-restriction to cursor API.

  If a cursor is accessed on a different coordinator than where it was created,
  the requests will be forwarded to the correct coordinator. If a cursor is
  accessed by a different user than the one who created it, the request will
  be denied.

* keep failed follower in followers list in Plan.

  This increases the changes of a failed follower getting back into sync if the
  follower comes back after a short time. In this case the follower can try to
  get in sync again, which normally takes less time than seeding a completely
  new follower.

* fix assertion failure and undefined behavior in Unix domain socket connections,
  introduced by 3.3.12

* added configuration option `--rocksdb.sync-interval`

  This option specifies interval (in milliseconds) that ArangoDB will use to
  automatically synchronize data in RocksDB's write-ahead log (WAL) files to
  disk. Automatic syncs will only be performed for not-yet synchronized data,
  and only for operations that have been executed without the *waitForSync*
  attribute.

  Automatic synchronization is performed by a background thread. The default
  sync interval is 0, meaning the automatic background syncing is turned off.
  Background syncing in 3.3 is opt-in, whereas in ArangoDB 3.4 the default sync
  interval will be 100 milliseconds.

  Note: this option is not supported on Windows platforms. Setting the sync
  interval to a value greater 0 will produce a startup warning.

* fixed graph creation sometimes failing with 'edge collection
  already used in edge def' when the edge definition contained multiple vertex
  collections, despite the edge definitions being identical

* inception could get caught in a trap, where agent configuration
  version and timeout multiplier lead to incapacitated agency

* fixed issue #5827: Batch request handling incompatible with .NET's default
  ContentType format

* fixed agency's log compaction for internal issue #2249

* inspector collects additionally disk data size and storage engine statistics


v3.3.12 (2018-07-12)
--------------------

* issue #5854: RocksDB engine would frequently request a new DelayToken.  This caused
  excessive write delay on the next Put() call.  Alternate approach taken.

* fixed graph creation under some circumstances failing with 'edge collection
  already used in edge def' despite the edge definitions being identical

* fixed issue #5727: Edge document with user provided key is inserted as many
  times as the number of shards, violating the primary index

* fixed internal issue #2658: AQL modification queries did not allow `_rev`
  checking. There is now a new option `ignoreRevs` which can be set to `false`
  in order to force AQL modification queries to match revision ids before
  doing any modifications

* fixed issue #5679: Replication applier restrictions will crash synchronisation
  after initial sync

* fixed potential issue in RETURN DISTINCT CollectBlock implementation
  that led to the block producing an empty result

* changed communication tasks to use boost strands instead of locks,
  this fixes a race condition with parallel VST communication over
  SSL

* fixed agency restart from compaction without data

* fixed for agent coming back to agency with changed endpoint and
  total data loss

* more patient agency tests to allow for ASAN tests to successfully finish


v3.3.11 (2018-06-26)
--------------------

* upgraded arangosync version to 0.5.3

* upgraded arangodb starter version to 0.12.0

* fixed internal issue #2559: "unexpected document key" error when custom
  shard keys are used and the "allowUserKeys" key generator option is set
  to false

* fixed AQL DOCUMENT lookup function for documents for sharded collections with
  more than a single shard and using a custom shard key (i.e. some shard
  key attribute other than `_key`).
  The previous implementation of DOCUMENT restricted to lookup to a single
  shard in all cases, though this restriction was invalid. That lead to
  `DOCUMENT` not finding documents in cases the wrong shard was contacted. The
  fixed implementation in 3.3.11 will reach out to all shards to find the
  document, meaning it will produce the correct result, but will cause more
  cluster-internal traffic. This increase in traffic may be high if the number
  of shards is also high, because each invocation of `DOCUMENT` will have to
  contact all shards.
  There will be no performance difference for non-sharded collections or
  collections that are sharded by `_key` or that only have a single shard.

* reimplemented replication view in web UI

* fixed internal issue #2256: ui, document id not showing up when deleting a document

* fixed internal issue #2163: wrong labels within foxx validation of service
  input parameters

* fixed internal issue #2160: fixed misplaced tooltips in indices view

* added new arangoinspect client tool, to help users and customers easily collect
  information of any ArangoDB server setup, and facilitate troubleshooting for the
  ArangoDB Support Team


v3.3.10 (2018-06-04)
--------------------

* make optimizer rule "remove-filter-covered-by-index" not stop after removing
  a sub-condition from a FILTER statement, but pass the optimized FILTER
  statement again into the optimizer rule for further optimizations.
  This allows optimizing away some more FILTER conditions than before.

* allow accessing /_admin/status URL on followers too in active failover setup

* fix cluster COLLECT optimization for attributes that were in "sorted" variant of
  COLLECT and that were provided by a sorted index on the collected attribute

* apply fulltext index optimization rule for multiple fulltext searches in
  the same query

  this fixes https://stackoverflow.com/questions/50496274/two-fulltext-searches-on-arangodb-cluster-v8-is-involved

* validate `_from` and `_to` values of edges on updates consistently

* fixed issue #5400: Unexpected AQL Result

* fixed issue #5429: Frequent 'updated local foxx repository' messages

* fixed issue #5252: Empty result if FULLTEXT() is used together with LIMIT offset

* fixed issue #5035: fixed a vulnerability issue within the web ui's index view

* inception was ignoring leader's configuration


v3.3.9 (2018-05-17)
-------------------

* added `/_admin/repair/distributeShardsLike` that repairs collections with
  distributeShardsLike where the shards aren't actually distributed like in the
  prototype collection, as could happen due to internal issue #1770

* fixed Foxx queues bug when queues are created in a request handler with an
  ArangoDB authentication header

* upgraded arangosync version to 0.5.1

* upgraded arangodb starter version to 0.11.3

* fix cluster upgrading issue introduced in 3.3.8

  the issue made arangod crash when starting a DB server with option
  `--database.auto-upgrade true`

* fix C++ implementation of AQL ZIP function to return each distinct attribute
  name only once. The previous implementation added non-unique attribute names
  multiple times, which led to follow-up issues.
  Now if an attribute name occurs multiple times in the input list of attribute
  names, it will only be incorporated once into the result object, with the
  value that corresponds to the first occurrence.
  This fix also changes the V8 implementation of the ZIP function, which now
  will always return the first value for non-unique attribute names and not the
  last occurring value.

* self heal during a Foxx service install, upgrade or replace no longer breaks
  the respective operation

* make /_api/index, /_api/database and /_api/user REST handlers use the scheduler's
  internal queue, so they do not run in an I/O handling thread

* fixed issue #4919: C++ implementation of LIKE function now matches the old and
  correct behavior of the JavaScript implementation.

* added REST API endpoint /_admin/server/availability for monitoring purposes

* UI: fixed an unreasonable event bug within the modal view engine

* fixed issue #3811: gharial api is now checking existence of _from and _to vertices
  during edge creation

* fixed internal issue #2149: number of documents in the UI is not adjusted after
  moving them

* fixed internal issue #2150: UI - loading a saved query does not update the list
  of bind parameters

* fixed internal issue #2147 - fixed database filter in UI

* fixed issue #4934: Wrong used GeoIndex depending on FILTER order

* added `query` and `aql.literal` helpers to `@arangodb` module.

* remove post-sort from GatherNode in cluster AQL queries that do use indexes
  for filtering but that do not require a sorted result

  This optimization can speed up gathering data from multiple shards, because
  it allows to remove a merge sort of the individual shards' results.

* extend the already existing "reduce-extraction-to-projection" AQL optimizer
  rule for RocksDB to provide projections of up to 5 document attributes. The
  previous implementation only supported a projection for a single document
  attribute. The new implementation will extract up to 5 document attributes from
  a document while scanning a collection via an EnumerateCollectionNode.
  Additionally the new version of the optimizer rule can also produce projections
  when scanning an index via an IndexNode.
  The optimization is benefial especially for huge documents because it will copy
  out only the projected attributes from the document instead of copying the entire
  document data from the storage engine.

  When applied, the explainer will show the projected attributes in a `projections`
  remark for an EnumerateCollectionNode or IndexNode. The optimization is limited
  to the RocksDB storage engine.

* added index-only optimization for AQL queries that can satisfy the retrieval of
  all required document attributes directly from an index.

  This optimization will be triggered for the RocksDB engine if an index is used
  that covers all required attributes of the document used later on in the query.
  If applied, it will save retrieving the actual document data (which would require
  an extra lookup in RocksDB), but will instead build the document data solely
  from the index values found. It will only be applied when using up to 5 attributes
  from the document, and only if the rest of the document data is not used later
  on in the query.

  The optimization is currently available for the RocksDB engine for the index types
  primary, edge, hash, skiplist and persistent.

  If the optimization is applied, it will show up as "index only" in an AQL
  query's execution plan for an IndexNode.

* added scan-only optimization for AQL queries that iterate over collections or
  indexes and that do not need to return the actual document values.

  Not fetching the document values from the storage engine will provide a
  considerable speedup when using the RocksDB engine, but may also help a bit
  in case of the MMFiles engine. The optimization will only be applied when
  full-scanning or index-scanning a collection without refering to any of its
  documents later on, and, for an IndexNode, if all filter conditions for the
  documents of the collection are covered by the index.

  If the optimization is applied, it will show up as "scan only" in an AQL
  query's execution plan for an EnumerateCollectionNode or an IndexNode.

* extend existing "collect-in-cluster" optimizer rule to run grouping, counting
  and deduplication on the DB servers in several cases, so that the coordinator
  will only need to sum up the potentially smaller results from the individual shards.

  The following types of COLLECT queries are covered now:
  - RETURN DISTINCT expr
  - COLLECT WITH COUNT INTO ...
  - COLLECT var1 = expr1, ..., varn = exprn (WITH COUNT INTO ...), without INTO or KEEP
  - COLLECT var1 = expr1, ..., varn = exprn AGGREGATE ..., without INTO or KEEP, for
    aggregate functions COUNT/LENGTH, SUM, MIN and MAX.

* honor specified COLLECT method in AQL COLLECT options

  for example, when the user explicitly asks for the COLLECT method
  to be `sorted`, the optimizer will now not produce an alternative
  version of the plan using the hash method.

  additionally, if the user explcitly asks for the COLLECT method to
  be `hash`, the optimizer will now change the existing plan to use
  the hash method if possible instead of just creating an alternative
  plan.

  `COLLECT ... OPTIONS { method: 'sorted' }` => always use sorted method
  `COLLECT ... OPTIONS { method: 'hash' }`   => use hash if this is technically possible
  `COLLECT ...` (no options)                 => create a plan using sorted, and another plan using hash method

* added bulk document lookups for MMFiles engine, which will improve the performance
  of document lookups from an inside an index in case the index lookup produces many
  documents


v3.3.8 (2018-04-24)
-------------------

* included version of ArangoDB Starter (`arangodb` binary) updated to v0.10.11,
  see [Starter changelog](https://github.com/arangodb-helper/arangodb/blob/master/CHANGELOG.md)

* added arangod startup option `--dump-options` to print all configuration parameters
  as a JSON object

* fixed: (Enterprise Edition only) If you restore a SmartGraph where the collections
  are still existing and are supposed to be dropped on restore we ended up in
  duplicate name error. This is now gone and the SmartGraph is correctly restored.

* fix lookups by `_id` in smart graph edge collections

* improve startup resilience in case there are datafile errors (MMFiles)

  also allow repairing broken VERSION files automatically on startup by
  specifying the option `--database.ignore-datafile-errors true`

* fix issue #4582: UI query editor now supports usage of empty string as bind parameter value

* fixed internal issue #2148: Number of documents found by filter is misleading in web UI

* added startup option `--database.required-directory-state`

  using this option it is possible to require the database directory to be
  in a specific state on startup. the options for this value are:

  - non-existing: database directory must not exist
  - existing: database directory must exist
  - empty: database directory must exist but be empty
  - populated: database directory must exist and contain specific files already
  - any: any state allowed

* field "$schema" in Foxx manifest.json files no longer produce warnings

* added `@arangodb/locals` module to expose the Foxx service context as an
  alternative to using `module.context` directly.

* `db._executeTransaction` now accepts collection objects as collections.

* supervision can be put into maintenance mode


v3.3.7 (2018-04-11)
-------------------

* added hidden option `--query.registry-ttl` to control the lifetime of cluster AQL
  query parts

* fixed internal issue #2237: AQL queries on collections with replicationFactor:
  "satellite" crashed arangod in single server mode

* fixed restore of satellite collections: replicationFactor was set to 1 during
  restore

* fixed dump and restore of smart graphs:
  a) The dump will not include the hidden shadow collections anymore, they were dumped
     accidentially and only contain duplicated data.
  b) Restore will now ignore hidden shadow collections as all data is contained
     in the smart-edge collection. You can manually include these collections from an
     old dump (3.3.5 or earlier) by using `--force`.
  c) Restore of a smart-graph will now create smart collections properly instead
     of getting into `TIMEOUT_IN_CLUSTER_OPERATION`

* fixed issue in AQL query optimizer rule "restrict-to-single-shard", which
  may have sent documents to a wrong shard in AQL INSERT queries that specified
  the value for `_key` using an expression (and not a constant value)
  Important: if you were affected by this bug in v3.3.5 it is required that you
  recreate your dataset in v3.3.6 (i.e. dumping and restoring) instead of doing
  a simple binary upgrade

* added /_admin/status HTTP API for debugging purposes

* added ArangoShell helper function for packaging all information about an
  AQL query so it can be run and analyzed elsewhere:

  query = "FOR doc IN mycollection FILTER doc.value > 42 RETURN doc";
  require("@arangodb/aql/explainer").debugDump("/tmp/query-debug-info", query);

  Entitled users can send the generated file to the ArangoDB support to facilitate
  reproduction and debugging.

* added hidden option `--server.ask-jwt-secret`. This is an internal option
  for debugging and should not be exposed to end-users.

* fix for internal issue #2215. supervision will now wait for agent to
  fully prepare before adding 10 second grace period after leadership change

* fixed internal issue #2215's FailedLeader timeout bug

v3.3.5 (2018-03-28)
-------------------

* fixed issue #4934: Wrong used GeoIndex depending on FILTER order

* make build id appear in startup log message alongside with other version info

* make AQL data modification operations that are sent to all shards and that are
  supposed to return values (i.e. `RETURN OLD` or `RETURN NEW`) not return fake
  empty result rows if the document to be updated/replaced/removed was not present
  on the target shard

* added AQL optimizer rule `restrict-to-single-shard`

  This rule will kick in if a collection operation (index lookup or data
  modification operation) will only affect a single shard, and the operation can be
  restricted to the single shard and is not applied for all shards. This optimization
  can be applied for queries that access a collection only once in the query, and that
  do not use traversals, shortest path queries and that do not access collection data
  dynamically using the `DOCUMENT`, `FULLTEXT`, `NEAR` or `WITHIN` AQL functions.
  Additionally, the optimizer will only pull off this optimization if can safely
  determine the values of all the collection's shard keys from the query, and when the
  shard keys are covered by a single index (this is always true if the shard key is
  the default `_key`)

* display missing attributes of GatherNodes in AQL explain output

* make AQL optimizer rule `undistribute-remove-after-enum-coll` fire in a few
  more cases in which it is possible

* slightly improve index selection for the RocksDB engine when there are multiple
  competing indexes with the same attribute prefixes, but different amount of
  attributes covered. In this case, the more specialized index will be preferred
  now

* fix issue #4924: removeFollower now prefers to remove the last follower(s)

* added "collect-in-cluster" optimizer rule to have COLLECT WITH COUNT queries
  without grouping being executed on the DB servers and the coordinator only summing
  up the counts from the individual shards

* fixed issue #4900: Nested FOR query uses index but ignores other filters

* properly exit v8::Context in one place where it was missing before

* added hidden option `--cluster.index-create-timeout` for controlling the
  default value of the index creation timeout in cluster
  under normal circumstances, this option does not need to be adjusted

* increase default timeout for index creation in cluster to 3600s

* fixed issue #4843: Query-Result has more Docs than the Collection itself

* fixed the behavior of ClusterInfo when waiting for current to catch
  up with plan in create collection.

* fixed issue #4827: COLLECT on edge _to field doesn't group distinct values as expected (MMFiles)


v3.3.4 (2018-03-01)
-------------------

* fix AQL `fullCount` result value in some cluster cases when it was off a bit

* fix issue #4651: Simple query taking forever until a request timeout error

* fix issue #4657: fixed incomplete content type header

* Vastly improved the Foxx Store UI

* fix issue #4677: AQL WITH with bind parameters results in "access after data-modification"
  for two independent UPSERTs

* remove unused startup option `--ldap.permissions-attribute-name`

* fix issue #4457: create /var/tmp/arangod with correct user in supervisor mode

* remove long disfunctional admin/long_echo handler

* fixed Foxx API:

  * PUT /_api/foxx/service: Respect force flag
  * PATCH /_api/foxx/service: Check whether a service under given mount exists

* internal issue #1726: supervision failed to remove multiple servers
  from health monitoring at once.

* more information from inception, why agent is activated

* fixed a bug where supervision tried to deal with shards of virtual collections

* fix internal issue #1770: collection creation using distributeShardsLike yields
  errors and did not distribute shards correctly in the following cases:
  1. If numberOfShards * replicationFactor % nrDBServers != 0
     (shards * replication is not divisible by DBServers).
  2. If there was failover / move shard case on the leading collection
     and creating the follower collection afterwards.

* fix timeout issues in replication client expiration

* added missing edge filter to neighbors-only traversals
  in case a filter condition was moved into the traverser and the traversal was
  executed in breadth-first mode and was returning each visited vertex exactly
  once, and there was a filter on the edges of the path and the resulting vertices
  and edges were not used later, the edge filter was not applied

* fixed issue #4160: Run arangod with "--database.auto-upgrade" option always crash silently without error log

* fix internal issue #1848: AQL optimizer was trying to resolve attribute accesses
  to attributes of constant object values at query compile time, but only did so far
  the very first attribute in each object

  this fixes https://stackoverflow.com/questions/48648737/beginner-bug-in-for-loops-from-objects

* fix inconvenience: If we want to start server with a non-existing
  --javascript.app-path it will now be created (if possible)

* fixed: REST API `POST _api/foxx` now returns HTTP code 201 on success, as documented.
         returned 200 before.

* fixed: REST API `PATCH _api/foxx/dependencies` now updates the existing dependencies
         instead of replacing them.

* fixed: Foxx upload of single javascript file. You now can upload via http-url pointing
         to a javascript file.

* fixed issue #4395: If your foxx app includes an `APP` folder it got
         accidently removed by selfhealing this is not the case anymore.

* fixed internal issue #1969 - command apt-get purge/remove arangodb3e was failing


v3.3.3 (2018-01-16)
-------------------

* fix issue #4272: VERSION file keeps disappearing

* fix internal issue #81: quotation marks disappeared when switching table/json
  editor in the query editor ui

* added option `--rocksdb.throttle` to control whether write-throttling is enabled
  Write-throttling is turned on by default, to reduce chances of compactions getting
  too far behind and blocking incoming writes.

* fixed issue #4308: Crash when getter for error.name throws an error (on Windows)

* UI: fixed a query editor caching and parsing issue

* Fixed internal issue #1683: fixes an UI issue where a collection name gets wrongly cached
  within the documents overview of a collection.

* Fixed an issue with the index estimates in RocksDB in the case a transaction is aborted.
  Former the index estimates were modified if the transaction commited or not.
  Now they will only be modified if the transaction commited successfully.

* UI: optimized login view for very small screen sizes

* Truncate in RocksDB will now do intermediate commits every 10.000 documents
  if truncate fails or the server crashes during this operation all deletes
  that have been commited so far are persisted.

* make the default value of `--rocksdb.block-cache-shard-bits` use the RocksDB
  default value. This will mostly mean the default number block cache shard
  bits is lower than before, allowing each shard to store more data and cause
  less evictions from block cache

* issue #4222: Permission error preventing AQL query import / export on webui

* UI: optimized error messages for invalid query bind parameter

* UI: upgraded swagger ui to version 3.9.0

* issue #3504: added option `--force-same-database` for arangorestore

  with this option set to true, it is possible to make any arangorestore attempt
  fail if the specified target database does not match the database name
  specified in the source dump's "dump.json" file. it can thus be used to
  prevent restoring data into the "wrong" database

  The option is set to `false` by default to ensure backwards-compatibility

* make the default value of `--rocksdb.block-cache-shard-bits` use the RocksDB
  default value. This will mostly mean the default number block cache shard
  bits is lower than before, allowing each shard to store more data and cause
  less evictions from block cache

* fixed issue #4255: AQL SORT consuming too much memory

* fixed incorrect persistence of RAFT vote and term


v3.3.2 (2018-01-04)
-------------------

* fixed issue #4199: Internal failure: JavaScript exception in file 'arangosh.js'
  at 98,7: ArangoError 4: Expecting type String

* fixed issue in agency supervision with a good server being left in
  failedServers

* distinguish isReady and allInSync in clusterInventory

* fixed issue #4197: AQL statement not working in 3.3.1 when upgraded from 3.2.10

* do not reuse collection ids when restoring collections from a dump, but assign new collection ids, this should prevent collection id conflicts


v3.3.1 (2017-12-28)
-------------------

* UI: displayed wrong wfs property for a collection when using RocksDB as
  storage engine

* added `--ignore-missing` option to arangoimp
  this option allows importing lines with less fields than specified in the CSV
  header line

* changed misleading error message from "no leader" to "not a leader"

* optimize usage of AQL FULLTEXT index function to a FOR loop with index
  usage in some cases
  When the optimization is applied, this especially speeds up fulltext index
  queries in the cluster

* UI: improved the behavior during collection creation in a cluster environment

* Agency lockup fixes for very small machines.

* Agency performance improvement by finer grained locking.

* Use steady_clock in agency whereever possible.

* Agency prevent Supervision thread crash.

* Fix agency integer overflow in timeout calculation.


v3.3.0 (2017-12-14)
-------------------

* release version

* added a missing try/catch block in the supervision thread


v3.3.rc8 (2017-12-12)
---------------------

* UI: fixed broken Foxx configuration keys. Some valid configuration values
  could not be edited via the ui.

* UI: pressing the return key inside a select2 box no longer triggers the modal's
  success function

* UI: coordinators and db servers are now in sorted order (ascending)


v3.3.rc7 (2017-12-07)
---------------------

* fixed issue #3741: fix terminal color output in Windows

* UI: fixed issue #3822: disabled name input field for system collections

* fixed issue #3640: limit in subquery

* fixed issue #3745: Invalid result when using OLD object with array attribute in UPSERT statement

* UI: edge collections were wrongly added to from and to vertices select box during graph creation

* UI: added not found views for documents and collections

* UI: using default user database api during database creation now

* UI: the graph viewer backend now picks one random start vertex of the
  first 1000 documents instead of calling any(). The implementation of
  "any" is known to scale bad on huge collections with RocksDB.

* UI: fixed disappearing of the navigation label in some case special case

* UI: the graph viewer now displays updated label values correctly.
  Additionally the included node/edge editor now closes automatically
  after a successful node/edge update.

* fixed issue #3917: traversals with high maximal depth take extremely long
  in planning phase.


v3.3.rc4 (2017-11-28)
---------------------

* minor bug-fixes


v3.3.rc3 (2017-11-24)
---------------------

* bug-fixes


v3.3.rc2 (2017-11-22)
---------------------

* UI: document/edge editor now remembering their modes (e.g. code or tree)

* UI: optimized error messages for invalid graph definitions. Also fixed a
  graph renderer cleanup error.

* UI: added a delay within the graph viewer while changing the colors of the
  graph. Necessary due different browser behavior.

* added options `--encryption.keyfile` and `--encryption.key-generator` to arangodump
  and arangorestore

* UI: the graph viewer now displays updated label values correctly.
  Additionally the included node/edge editor now closes automatically
  after a successful node/edge update.

* removed `--recycle-ids` option for arangorestore

  using that option could have led to problems on the restore, with potential
  id conflicts between the originating server (the source dump server) and the
  target server (the restore server)


v3.3.rc1 (2017-11-17)
---------------------

* add readonly mode REST API

* allow compilation of ArangoDB source code with g++ 7

* upgrade minimum required g++ compiler version to g++ 5.4
  That means ArangoDB source code will not compile with g++ 4.x or g++ < 5.4 anymore.

* AQL: during a traversal if a vertex is not found. It will not print an ERROR to the log and continue
  with a NULL value, but will register a warning at the query and continue with a NULL value.
  The situation is not desired as an ERROR as ArangoDB can store edges pointing to non-existing
  vertex which is perfectly valid, but it may be a n issue on the data model, so users
  can directly see it on the query now and do not "by accident" have to check the LOG output.

* introduce `enforceReplicationFactor` attribute for creating collections:
  this optional parameter controls if the coordinator should bail out during collection
  creation if there are not enough DBServers available for the desired `replicationFactor`.

* fixed issue #3516: Show execution time in arangosh

  this change adds more dynamic prompt components for arangosh
  The following components are now available for dynamic prompts,
  settable via the `--console.prompt` option in arangosh:

  - '%t': current time as timestamp
  - '%a': elpased time since ArangoShell start in seconds
  - '%p': duration of last command in seconds
  - '%d': name of current database
  - '%e': current endpoint
  - '%E': current endpoint without protocol
  - '%u': current user

  The time a command takes can be displayed easily by starting arangosh with `--console.prompt "%p> "`.

* make the ArangoShell refill its collection cache when a yet-unknown collection
  is first accessed. This fixes the following problem:

      arangosh1> db._collections();  // shell1 lists all collections
      arangosh2> db._create("test"); // shell2 now creates a new collection 'test'
      arangosh1> db.test.insert({}); // shell1 is not aware of the collection created
                                     // in shell2, so the insert will fail

* make AQL `DISTINCT` not change the order of the results it is applied on

* incremental transfer of initial collection data now can handle partial
  responses for a chunk, allowing the leader/master to send smaller chunks
  (in terms of HTTP response size) and limit memory usage

  this optimization is only active if client applications send the "offset" parameter
  in their requests to PUT `/_api/replication/keys/<id>?type=docs`

* initial creation of shards for cluster collections is now faster with
  `replicationFactor` values bigger than 1. this is achieved by an optimization
  for the case when the collection on the leader is still empty

* potential fix for issue #3517: several "filesystem full" errors in logs
  while there's a lot of disk space

* added C++ implementations for AQL function `SUBSTRING()`, `LEFT()`, `RIGHT()` and `TRIM()`

* show C++ function name of call site in ArangoDB log output

  this requires option `--log.line-number` to be set to *true*

* UI: added word wrapping to query editor

* UI: fixed wrong user attribute name validation, issue #3228

* make AQL return a proper error message in case of a unique key constraint
  violation. previously it only returned the generic "unique constraint violated"
  error message but omitted the details about which index caused the problem.

  This addresses https://stackoverflow.com/questions/46427126/arangodb-3-2-unique-constraint-violation-id-or-key

* added option `--server.local-authentication`

* UI: added user roles

* added config option `--log.color` to toggle colorful logging to terminal

* added config option `--log.thread-name` to additionally log thread names

* usernames must not start with `:role:`, added new options:
    --server.authentication-timeout
    --ldap.roles-attribute-name
    --ldap.roles-transformation
    --ldap.roles-search
    --ldap.superuser-role
    --ldap.roles-include
    --ldap.roles-exclude

* performance improvements for full collection scans and a few other operations
  in MMFiles engine

* added `--rocksdb.encryption-key-generator` for enterprise

* removed `--compat28` parameter from arangodump and replication API

  older ArangoDB versions will no longer be supported by these tools.

* increase the recommended value for `/proc/sys/vm/max_map_count` to a value
  eight times as high as the previous recommended value. Increasing the
  values helps to prevent an ArangoDB server from running out of memory mappings.

  The raised minimum recommended value may lead to ArangoDB showing some startup
  warnings as follows:

      WARNING {memory} maximum number of memory mappings per process is 65530, which seems too low. it is recommended to set it to at least 512000
      WARNING {memory} execute 'sudo sysctl -w "vm.max_map_count=512000"'

* Foxx now warns about malformed configuration/dependency names and aliases in the manifest.


v3.2.17 (XXXX-XX-XX)
--------------------

* added missing virtual destructor for MMFiles transaction data context object

* make synchronous replication detect more error cases when followers cannot
  apply the changes from the leader

* fixed undefined behavior in cluster plan-loading procedure that may have
  unintentionally modified a shared structure

* cluster nodes should retry registering in agency until successful

* fixed issue #5354: updated the ui json editor, improved usability

* fixed issue #5648: fixed error message when saving unsupported document
  types

* fixed issue #5943: misplaced database ui icon and wrong cursor type were used


v3.2.16 (2018-07-12)
--------------------

* upgraded arangodb starter version to 0.12.0

* make edge cache initialization and invalidation more portable by avoiding memset
  on non-POD types

* fixed internal issue #2256: ui, document id not showing up when deleting a document

* fixed issue #5400: Unexpected AQL Result

* Fixed issue #5035: fixed a vulnerability issue within the web ui's index view

* issue one HTTP call less per cluster AQL query

* self heal during a Foxx service install, upgrade or replace no longer breaks
  the respective operation

* inception was ignoring leader's configuration

* inception could get caught in a trap, where agent configuration
  version and timeout multiplier lead to incapacitated agency

* more patient agency tests to allow for ASAN tests to successfully finish

* fixed for agent coming back to agency with changed endpoint and
  total data loss

* fixed agency restart from compaction without data


v3.2.15 (2018-05-13)
--------------------

* upgraded arangodb starter version to 0.11.2

* make /_api/index and /_api/database REST handlers use the scheduler's internal
  queue, so they do not run in an I/O handling thread

* fixed issue #3811: gharial api is now checking existence of _from and _to vertices
  during edge creation


v3.2.14 (2018-04-20)
--------------------

* field "$schema" in Foxx manifest.json files no longer produce warnings

* added `@arangodb/locals` module to expose the Foxx service context as an
  alternative to using `module.context` directly.

* the internal implementation of REST API `/_api/simple/by-example` now uses
  C++ instead of JavaScript

* supervision can be switched to maintenance mode f.e. for rolling upgrades


v3.2.13 (2018-04-13)
--------------------

* improve startup resilience in case there are datafile errors (MMFiles)

  also allow repairing broken VERSION files automatically on startup by
  specifying the option `--database.ignore-datafile-errors true`

* fix issue #4582: UI query editor now supports usage of empty string as bind parameter value

* fix issue #4924: removeFollower now prefers to remove the last follower(s)

* fixed issue #4934: Wrong used GeoIndex depending on FILTER order

* fixed the behavior of clusterinfo when waiting for current to catch
  up with plan in create collection.

* fix for internal issue #2215. supervision will now wait for agent to
  fully prepare before adding 10 second grace period after leadership change

* fixed interal issue #2215 FailedLeader timeout bug


v3.2.12 (2018-02-27)
--------------------

* remove long disfunctional admin/long_echo handler

* fixed Foxx API:

  * PUT /_api/foxx/service: Respect force flag
  * PATCH /_api/foxx/service: Check whether a service under given mount exists

* fix issue #4457: create /var/tmp/arangod with correct user in supervisor mode

* fix internal issue #1848

  AQL optimizer was trying to resolve attribute accesses
  to attributes of constant object values at query compile time, but only did so far
  the very first attribute in each object

  this fixes https://stackoverflow.com/questions/48648737/beginner-bug-in-for-loops-from-objects

* fix inconvenience: If we want to start server with a non-existing
  --javascript.app-path it will now be created (if possible)

* fixed: REST API `POST _api/foxx` now returns HTTP code 201 on success, as documented.
         returned 200 before.

* fixed: REST API `PATCH _api/foxx/dependencies` now updates the existing dependencies
         instead of replacing them.

* fixed: Foxx upload of single javascript file. You now can upload via http-url pointing
         to a javascript file.

* fixed issue #4395: If your foxx app includes an `APP` folder it got accidently removed by selfhealing
         this is not the case anymore.

* fix internal issue 1770: collection creation using distributeShardsLike yields
  errors and did not distribute shards correctly in the following cases:
  1. If numberOfShards * replicationFactor % nrDBServers != 0
     (shards * replication is not divisible by DBServers).
  2. If there was failover / move shard case on the leading collection
     and creating the follower collection afterwards.

* fix timeout issues in replication client expiration

+ fix some inconsistencies in replication for RocksDB engine that could have led
  to some operations not being shipped from master to slave servers

* fix issue #4272: VERSION file keeps disappearing

* fix internal issue #81: quotation marks disappeared when switching table/json
  editor in the query editor ui

* make the default value of `--rocksdb.block-cache-shard-bits` use the RocksDB
  default value. This will mostly mean the default number block cache shard
  bits is lower than before, allowing each shard to store more data and cause
  less evictions from block cache

* fix issue #4393: broken handling of unix domain sockets in
  JS_Download

* fix internal bug #1726: supervision failed to remove multiple
  removed servers from health UI

* fixed internal issue #1969 - command apt-get purge/remove arangodb3e was failing

* fixed a bug where supervision tried to deal with shards of virtual collections


v3.2.11 (2018-01-17)
--------------------

* Fixed an issue with the index estimates in RocksDB in the case a transaction is aborted.
  Former the index estimates were modified if the transaction commited or not.
  Now they will only be modified if the transaction commited successfully.

* Truncate in RocksDB will now do intermediate commits every 10.000 documents
  if truncate fails or the server crashes during this operation all deletes
  that have been commited so far are persisted.

* fixed issue #4308: Crash when getter for error.name throws an error (on Windows)

* UI: fixed a query editor caching and parsing issue for arrays and objects

* Fixed internal issue #1684: Web UI: saving arrays/objects as bind parameters faulty

* Fixed internal issue #1683: fixes an UI issue where a collection name gets wrongly cached
  within the documents overview of a collection.

* issue #4222: Permission error preventing AQL query import / export on webui

* UI: optimized login view for very small screen sizes

* UI: Shard distribution view now has an accordion view instead of displaying
  all shards of all collections at once.

* UI: optimized error messages for invalid query bind parameter

* fixed missing transaction events in RocksDB asynchronous replication

* fixed issue #4255: AQL SORT consuming too much memory

* fixed issue #4199: Internal failure: JavaScript exception in file 'arangosh.js'
  at 98,7: ArangoError 4: Expecting type String

* fixed issue #3818: Foxx configuration keys cannot contain spaces (will not save)

* UI: displayed wrong "waitForSync" property for a collection when
  using RocksDB as storage engine

* prevent binding to the same combination of IP and port on Windows

* fixed incorrect persistence of RAFT vote and term


v3.2.10 (2017-12-22)
--------------------

* replication: more robust initial sync

* fixed a bug in the RocksDB engine that would prevent recalculated
  collection counts to be actually stored

* fixed issue #4095: Inconsistent query execution plan

* fixed issue #4056: Executing empty query causes crash

* fixed issue #4045: Out of memory in `arangorestore` when no access
  rights to dump files

* fixed issue #3031: New Graph: Edge definitions with edges in
  fromCollections and toCollections

* fixed issue #2668: UI: when following wrong link from edge to vertex in
  nonexisting collection misleading error is printed

* UI: improved the behavior during collection creation in a cluster environment

* UI: the graph viewer backend now picks one random start vertex of the
  first 1000 documents instead of calling any(). The implementation of
  any is known to scale bad on huge collections with rocksdb.

* fixed snapshots becoming potentially invalid after intermediate commits in
  the RocksDB engine

* backport agency inquire API changes

* fixed issue #3822: Field validation error in ArangoDB UI - Minor

* UI: fixed disappearing of the navigation label in some cases

* UI: fixed broken foxx configuration keys. Some valid configuration values
  could not be edited via the ui.

* fixed issue #3640: limit in subquery

* UI: edge collections were wrongly added to from and to vertices select
  box during graph creation

* fixed issue #3741: fix terminal color output in Windows

* fixed issue #3917: traversals with high maximal depth take extremely long
  in planning phase.

* fix equality comparison for MMFiles documents in AQL functions UNIQUE
  and UNION_DISTINCT


v3.2.9 (2017-12-04)
-------------------

* under certain conditions, replication could stop. Now fixed by adding an
  equality check for requireFromPresent tick value

* fixed locking for replication context info in RocksDB engine
  this fixes undefined behavior when parallel requests are made to the
  same replication context

* UI: added not found views for documents and collections

* fixed issue #3858: Foxx queues stuck in 'progress' status

* allow compilation of ArangoDB source code with g++ 7

* fixed issue #3224: Issue in the Foxx microservices examples

* fixed a deadlock in user privilege/permission change routine

* fixed a deadlock on server shutdown

* fixed some collection locking issues in MMFiles engine

* properly report commit errors in AQL write queries to the caller for the
  RocksDB engine

* UI: optimized error messages for invalid graph definitions. Also fixed a
  graph renderer cleanrenderer cleanup error.

* UI: document/edge editor now remembering their modes (e.g. code or tree)

* UI: added a delay within the graph viewer while changing the colors of the
  graph. Necessary due different browser behavior.

* fix removal of failed cluster nodes via web interface

* back port of ClusterComm::wait fix in devel
  among other things this fixes too eager dropping of other followers in case
  one of the followers does not respond in time

* transact interface in agency should not be inquired as of now

* inquiry tests and blocking of inquiry on AgencyGeneralTransaction

v3.2.8 (2017-11-18)
-------------------

* fixed a race condition occuring when upgrading via linux package manager

* fixed authentication issue during replication


v3.2.7 (2017-11-13)
-------------------

* Cluster customers, which have upgraded from 3.1 to 3.2 need to upgrade
  to 3.2.7. The cluster supervision is otherwise not operational.

* Fixed issue #3597: AQL with path filters returns unexpected results
  In some cases breadth first search in combination with vertex filters
  yields wrong result, the filter was not applied correctly.

* fixed some undefined behavior in some internal value caches for AQL GatherNodes
  and SortNodes, which could have led to sorted results being effectively not
  correctly sorted.

* make the replication applier for the RocksDB engine start automatically after a
  restart of the server if the applier was configured with its `autoStart` property
  set to `true`. previously the replication appliers were only automatically restarted
  at server start for the MMFiles engine.

* fixed arangodump batch size adaptivity in cluster mode and upped default batch size
  for arangodump

  these changes speed up arangodump in cluster context

* smart graphs now return a proper inventory in response to replication inventory
  requests

* fixed issue #3618: Inconsistent behavior of OR statement with object bind parameters

* only users with read/write rights on the "_system" database can now execute
  "_admin/shutdown" as well as modify properties of the write-ahead log (WAL)

* increase default maximum number of V8 contexts to at least 16 if not explicitly
  configured otherwise.
  the procedure for determining the actual maximum value of V8 contexts is unchanged
  apart from the value `16` and works as follows:
  - if explicitly set, the value of the configuration option `--javascript.v8-contexts`
    is used as the maximum number of V8 contexts
  - when the option is not set, the maximum number of V8 contexts is determined
    by the configuration option `--server.threads` if that option is set. if
    `--server.threads` is not set, then the maximum number of V8 contexts is the
    server's reported hardware concurrency (number of processors visible
    to the arangod process). if that would result in a maximum value of less than 16
    in any of these two cases, then the maximum value will be increased to 16.

* fixed issue #3447: ArangoError 1202: AQL: NotFound: (while executing) when
  updating collection

* potential fix for issue #3581: Unexpected "rocksdb unique constraint
  violated" with unique hash index

* fixed geo index optimizer rule for geo indexes with a single (array of coordinates)
  attribute.

* improved the speed of the shards overview in cluster (API endpoint /_api/cluster/shardDistribution API)
  It is now guaranteed to return after ~2 seconds even if the entire cluster is unresponsive.

* fix agency precondition check for complex objects
  this fixes issues with several CAS operations in the agency

* several fixes for agency restart and shutdown

* the cluster-internal representation of planned collection objects is now more
  lightweight than before, using less memory and not allocating any cache for indexes
  etc.

* fixed issue #3403: How to kill long running AQL queries with the browser console's
  AQL (display issue)

* fixed issue #3549: server reading ENGINE config file fails on common standard
  newline character

* UI: fixed error notifications for collection modifications

* several improvements for the truncate operation on collections:

  * the timeout for the truncate operation was increased in cluster mode in
    order to prevent too frequent "could not truncate collection" errors

  * after a truncate operation, collections in MMFiles still used disk space.
    to reclaim disk space used by truncated collection, the truncate actions
    in the web interface and from the ArangoShell now issue an extra WAL flush
    command (in cluster mode, this command is also propagated to all servers).
    the WAL flush allows all servers to write out any pending operations into the
    datafiles of the truncated collection. afterwards, a final journal rotate
    command is sent, which enables the compaction to entirely remove all datafiles
    and journals for the truncated collection, so that all disk space can be
    reclaimed

  * for MMFiles a special method will be called after a truncate operation so that
    all indexes of the collection can free most of their memory. previously some
    indexes (hash and skiplist indexes) partially kept already allocated memory
    in order to avoid future memory allocations

  * after a truncate operation in the RocksDB engine, an additional compaction
    will be triggered for the truncated collection. this compaction removes all
    deletions from the key space so that follow-up scans over the collection's key
    range do not have to filter out lots of already-removed values

  These changes make truncate operations potentially more time-consuming than before,
  but allow for memory/disk space savings afterwards.

* enable JEMalloc background threads for purging and returning unused memory
  back to the operating system (Linux only)

  JEMalloc will create its background threads on demand. The number of background
  threads is capped by the number of CPUs or active arenas. The background threads run
  periodically and purge unused memory pages, allowing memory to be returned to the
  operating system.

  This change will make the arangod process create several additional threads.
  It is accompanied by an increased `TasksMax` value in the systemd service configuration
  file for the arangodb3 service.

* upgraded bundled V8 engine to bugfix version v5.7.492.77

  this upgrade fixes a memory leak in upstream V8 described in
  https://bugs.chromium.org/p/v8/issues/detail?id=5945 that will result in memory
  chunks only getting uncommitted but not unmapped


v3.2.6 (2017-10-26)
-------------------

* UI: fixed event cleanup in cluster shards view

* UI: reduced cluster dashboard api calls

* fixed a permission problem that prevented collection contents to be displayed
  in the web interface

* removed posix_fadvise call from RocksDB's PosixSequentialFile::Read(). This is
  consistent with Facebook PR 2573 (#3505)

  this fix should improve the performance of the replication with the RocksDB
  storage engine

* allow changing of collection replication factor for existing collections

* UI: replicationFactor of a collection is now changeable in a cluster
  environment

* several fixes for the cluster agency

* fixed undefined behavior in the RocksDB-based geo index

* fixed Foxxmaster failover

* purging or removing the Debian/Ubuntu arangodb3 packages now properly stops
  the arangod instance before actuallying purging or removing


v3.2.5 (2017-10-16)
-------------------

* general-graph module and _api/gharial now accept cluster options
  for collection creation. It is now possible to set replicationFactor and
  numberOfShards for all collections created via this graph object.
  So adding a new collection will not result in a singleShard and
  no replication anymore.

* fixed issue #3408: Hard crash in query for pagination

* minimum number of V8 contexts in console mode must be 2, not 1. this is
  required to ensure the console gets one dedicated V8 context and all other
  operations have at least one extra context. This requirement was not enforced
  anymore.

* fixed issue #3395: AQL: cannot instantiate CollectBlock with undetermined
  aggregation method

* UI: fixed wrong user attribute name validation, issue #3228

* fix potential overflow in CRC marker check when a corrupted CRC marker
  is found at the very beginning of an MMFiles datafile

* UI: fixed unresponsive events in cluster shards view

* Add statistics about the V8 context counts and number of available/active/busy
  threads we expose through the server statistics interface.


v3.2.4 (2017-09-26)
-------------------

* UI: no default index selected during index creation

* UI: added replicationFactor option during SmartGraph creation

* make the MMFiles compactor perform less writes during normal compaction
  operation

  This partially fixes issue #3144

* make the MMFiles compactor configurable

  The following options have been added:

* `--compaction.db-sleep-time`: sleep interval between two compaction runs
    (in s)
  * `--compaction.min-interval"`: minimum sleep time between two compaction
     runs (in s)
  * `--compaction.min-small-data-file-size`: minimal filesize threshold
    original datafiles have to be below for a compaction
  * `--compaction.dead-documents-threshold`: minimum unused count of documents
    in a datafile
  * `--compaction.dead-size-threshold`: how many bytes of the source data file
    are allowed to be unused at most
  * `--compaction.dead-size-percent-threshold`: how many percent of the source
    datafile should be unused at least
  * `--compaction.max-files`: Maximum number of files to merge to one file
  * `--compaction.max-result-file-size`: how large may the compaction result
    file become (in bytes)
  * `--compaction.max-file-size-factor`: how large the resulting file may
    be in comparison to the collection's `--database.maximal-journal-size' setting`

* fix downwards-incompatibility in /_api/explain REST handler

* fix Windows implementation for fs.getTempPath() to also create a
  sub-directory as we do on linux

* fixed a multi-threading issue in cluster-internal communication

* performance improvements for traversals and edge lookups

* removed internal memory zone handling code. the memory zones were a leftover
  from the early ArangoDB days and did not provide any value in the current
  implementation.

* (Enterprise Edition only) added `skipInaccessibleCollections` option for AQL queries:
  if set, AQL queries (especially graph traversals) will treat collections to
  which a user has no access rights to as if these collections were empty.

* adjusted scheduler thread handling to start and stop less threads in
  normal operations

* leader-follower replication catchup code has been rewritten in C++

* early stage AQL optimization now also uses the C++ implementations of
  AQL functions if present. Previously it always referred to the JavaScript
  implementations and ignored the C++ implementations. This change gives
  more flexibility to the AQL optimizer.

* ArangoDB tty log output is now colored for log messages with levels
  FATAL, ERR and WARN.

* changed the return values of AQL functions `REGEX_TEST` and `REGEX_REPLACE`
  to `null` when the input regex is invalid. Previous versions of ArangoDB
  partly returned `false` for invalid regexes and partly `null`.

* added `--log.role` option for arangod

  When set to `true`, this option will make the ArangoDB logger print a single
  character with the server's role into each logged message. The roles are:

  - U: undefined/unclear (used at startup)
  - S: single server
  - C: coordinator
  - P: primary
  - A: agent

  The default value for this option is `false`, so no roles will be logged.


v3.2.3 (2017-09-07)
-------------------

* fixed issue #3106: orphan collections could not be registered in general-graph module

* fixed wrong selection of the database inside the internal cluster js api

* added startup option `--server.check-max-memory-mappings` to make arangod check
  the number of memory mappings currently used by the process and compare it with
  the maximum number of allowed mappings as determined by /proc/sys/vm/max_map_count

  The default value is `true`, so the checks will be performed. When the current
  number of mappings exceeds 90% of the maximum number of mappings, the creation
  of further V8 contexts will be deferred.

  Note that this option is effective on Linux systems only.

* arangoimp now has a `--remove-attribute` option

* added V8 context lifetime control options
  `--javascript.v8-contexts-max-invocations` and `--javascript.v8-contexts-max-age`

  These options allow specifying after how many invocations a used V8 context is
  disposed, or after what time a V8 context is disposed automatically after its
  creation. If either of the two thresholds is reached, an idl V8 context will be
  disposed.

  The default value of `--javascript.v8-contexts-max-invocations` is 0, meaning that
  the maximum number of invocations per context is unlimited. The default value
  for `--javascript.v8-contexts-max-age` is 60 seconds.

* fixed wrong UI cluster health information

* fixed issue #3070: Add index in _jobs collection

* fixed issue #3125: HTTP Foxx API JSON parsing

* fixed issue #3120: Foxx queue: job isn't running when server.authentication = true

* fixed supervision failure detection and handling, which happened with simultaneous
  agency leadership change


v3.2.2 (2017-08-23)
-------------------

* make "Rebalance shards" button work in selected database only, and not make
  it rebalance the shards of all databases

* fixed issue #2847: adjust the response of the DELETE `/_api/users/database/*` calls

* fixed issue #3075: Error when upgrading arangoDB on linux ubuntu 16.04

* fixed a buffer overrun in linenoise console input library for long input strings

* increase size of the linenoise input buffer to 8 KB

* abort compilation if the detected GCC or CLANG isn't in the range of compilers
  we support

* fixed spurious cluster hangups by always sending AQL-query related requests
  to the correct servers, even after failover or when a follower drops

  The problem with the previous shard-based approach was that responsibilities
  for shards may change from one server to another at runtime, after the query
  was already instanciated. The coordinator and other parts of the query then
  sent further requests for the query to the servers now responsible for the
  shards.
  However, an AQL query must send all further requests to the same servers on
  which the query was originally instanciated, even in case of failover.
  Otherwise this would potentially send requests to servers that do not know
  about the query, and would also send query shutdown requests to the wrong
  servers, leading to abandoned queries piling up and using resources until
  they automatically time out.

* fixed issue with RocksDB engine acquiring the collection count values too
  early, leading to the collection count values potentially being slightly off
  even in exclusive transactions (for which the exclusive access should provide
  an always-correct count value)

* fixed some issues in leader-follower catch-up code, specifically for the
  RocksDB engine

* make V8 log fatal errors to syslog before it terminates the process.
  This change is effective on Linux only.

* fixed issue with MMFiles engine creating superfluous collection journals
  on shutdown

* fixed issue #3067: Upgrade from 3.2 to 3.2.1 reset autoincrement keys

* fixed issue #3044: ArangoDB server shutdown unexpectedly

* fixed issue #3039: Incorrect filter interpretation

* fixed issue #3037: Foxx, internal server error when I try to add a new service

* improved MMFiles fulltext index document removal performance
  and fulltext index query performance for bigger result sets

* ui: fixed a display bug within the slow and running queries view

* ui: fixed a bug when success event triggers twice in a modal

* ui: fixed the appearance of the documents filter

* ui: graph vertex collections not restricted to 10 anymore

* fixed issue #2835: UI detection of JWT token in case of server restart or upgrade

* upgrade jemalloc version to 5.0.1

  This fixes problems with the memory allocator returing "out of memory" when
  calling munmap to free memory in order to return it to the OS.

  It seems that calling munmap on Linux can increase the number of mappings, at least
  when a region is partially unmapped. This can lead to the process exceeding its
  maximum number of mappings, and munmap and future calls to mmap returning errors.

  jemalloc version 5.0.1 does not have the `--enable-munmap` configure option anymore,
  so the problem is avoided. To return memory to the OS eventually, jemalloc 5's
  background purge threads are used on Linux.

* fixed issue #2978: log something more obvious when you log a Buffer

* fixed issue #2982: AQL parse error?

* fixed issue #3125: HTTP Foxx API Json parsing

v3.2.1 (2017-08-09)
-------------------

* added C++ implementations for AQL functions `LEFT()`, `RIGHT()` and `TRIM()`

* fixed docs for issue #2968: Collection _key autoincrement value increases on error

* fixed issue #3011: Optimizer rule reduce-extraction-to-projection breaks queries

* Now allowing to restore users in a sharded environment as well
  It is still not possible to restore collections that are sharded
  differently than by _key.

* fixed an issue with restoring of system collections and user rights.
  It was not possible to restore users into an authenticated server.

* fixed issue #2977: Documentation for db._createDatabase is wrong

* ui: added bind parameters to slow query history view

* fixed issue #1751: Slow Query API should provide bind parameters, webui should display them

* ui: fixed a bug when moving multiple documents was not possible

* fixed docs for issue #2968: Collection _key autoincrement value increases on error

* AQL CHAR_LENGTH(null) returns now 0. Since AQL TO_STRING(null) is '' (string of length 0)

* ui: now supports single js file upload for Foxx services in addition to zip files

* fixed a multi-threading issue in the agency when callElection was called
  while the Supervision was calling updateSnapshot

* added startup option `--query.tracking-with-bindvars`

  This option controls whether the list of currently running queries
  and the list of slow queries should contain the bind variables used
  in the queries or not.

  The option can be changed at runtime using the commands

      // enables tracking of bind variables
      // set to false to turn tracking of bind variables off
      var value = true;
      require("@arangodb/aql/queries").properties({
        trackBindVars: value
      });

* index selectivity estimates are now available in the cluster as well

* fixed issue #2943: loadIndexesIntoMemory not returning the same structure
  as the rest of the collection APIs

* fixed issue #2949: ArangoError 1208: illegal name

* fixed issue #2874: Collection properties do not return `isVolatile`
  attribute

* potential fix for issue #2939: Segmentation fault when starting
  coordinator node

* fixed issue #2810: out of memory error when running UPDATE/REPLACE
  on medium-size collection

* fix potential deadlock errors in collector thread

* disallow the usage of volatile collections in the RocksDB engine
  by throwing an error when a collection is created with attribute
  `isVolatile` set to `true`.
  Volatile collections are unsupported by the RocksDB engine, so
  creating them should not succeed and silently create a non-volatile
  collection

* prevent V8 from issuing SIGILL instructions when it runs out of memory

  Now arangod will attempt to log a FATAL error into its logfile in case V8
  runs out of memory. In case V8 runs out of memory, it will still terminate the
  entire process. But at least there should be something in the ArangoDB logs
  indicating what the problem was. Apart from that, the arangod process should
  now be exited with SIGABRT rather than SIGILL as it shouldn't return into the
  V8 code that aborted the process with `__builtin_trap`.

  this potentially fixes issue #2920: DBServer crashing automatically post upgrade to 3.2

* Foxx queues and tasks now ensure that the scripts in them run with the same
  permissions as the Foxx code who started the task / queue

* fixed issue #2928: Offset problems

* fixed issue #2876: wrong skiplist index usage in edge collection

* fixed issue #2868: cname missing from logger-follow results in rocksdb

* fixed issue #2889: Traversal query using incorrect collection id

* fixed issue #2884: AQL traversal uniqueness constraints "propagating" to other traversals? Weird results

* arangoexport: added `--query` option for passing an AQL query to export the result

* fixed issue #2879: No result when querying for the last record of a query

* ui: allows now to edit default access level for collections in database
  _system for all users except the root user.

* The _users collection is no longer accessible outside the arngod process, _queues is always read-only

* added new option "--rocksdb.max-background-jobs"

* removed options "--rocksdb.max-background-compactions", "--rocksdb.base-background-compactions" and "--rocksdb.max-background-flushes"

* option "--rocksdb.compaction-read-ahead-size" now defaults to 2MB

* change Windows build so that RocksDB doesn't enforce AVX optimizations by default
  This fixes startup crashes on servers that do not have AVX CPU extensions

* speed up RocksDB secondary index creation and dropping

* removed RocksDB note in Geo index docs


v3.2.0 (2017-07-20)
-------------------

* fixed UI issues

* fixed multi-threading issues in Pregel

* fixed Foxx resilience

* added command-line option `--javascript.allow-admin-execute`

  This option can be used to control whether user-defined JavaScript code
  is allowed to be executed on server by sending via HTTP to the API endpoint
  `/_admin/execute`  with an authenticated user account.
  The default value is `false`, which disables the execution of user-defined
  code. This is also the recommended setting for production. In test environments,
  it may be convenient to turn the option on in order to send arbitrary setup
  or teardown commands for execution on the server.


v3.2.beta6 (2017-07-18)
-----------------------

* various bugfixes


v3.2.beta5 (2017-07-16)
-----------------------

* numerous bugfixes


v3.2.beta4 (2017-07-04)
-----------------------

* ui: fixed document view _from and _to linking issue for special characters

* added function `db._parse(query)` for parsing an AQL query and returning information about it

* fixed one medium priority and two low priority security user interface
  issues found by owasp zap.

* ui: added index deduplicate options

* ui: fixed renaming of collections for the rocksdb storage engine

* documentation and js fixes for secondaries

* RocksDB storage format was changed, users of the previous beta/alpha versions
  must delete the database directory and re-import their data

* enabled permissions on database and collection level

* added and changed some user related REST APIs
    * added `PUT /_api/user/{user}/database/{database}/{collection}` to change collection permission
    * added `GET /_api/user/{user}/database/{database}/{collection}`
    * added optional `full` parameter to the `GET /_api/user/{user}/database/` REST call

* added user functions in the arangoshell `@arangodb/users` module
    * added `grantCollection` and `revokeCollection` functions
    * added `permission(user, database, collection)` to retrieve collection specific rights

* added "deduplicate" attribute for array indexes, which controls whether inserting
  duplicate index values from the same document into a unique array index will lead to
  an error or not:

      // with deduplicate = true, which is the default value:
      db._create("test");
      db.test.ensureIndex({ type: "hash", fields: ["tags[*]"], deduplicate: true });
      db.test.insert({ tags: ["a", "b"] });
      db.test.insert({ tags: ["c", "d", "c"] }); // will work, because deduplicate = true
      db.test.insert({ tags: ["a"] }); // will fail

      // with deduplicate = false
      db._create("test");
      db.test.ensureIndex({ type: "hash", fields: ["tags[*]"], deduplicate: false });
      db.test.insert({ tags: ["a", "b"] });
      db.test.insert({ tags: ["c", "d", "c"] }); // will not work, because deduplicate = false
      db.test.insert({ tags: ["a"] }); // will fail

  The "deduplicate" attribute is now also accepted by the index creation HTTP
  API endpoint POST /_api/index and is returned by GET /_api/index.

* added optimizer rule "remove-filters-covered-by-traversal"

* Debian/Ubuntu installer: make messages about future package upgrades more clear

* fix a hangup in VST

  The problem happened when the two first chunks of a VST message arrived
  together on a connection that was newly switched to VST.

* fix deletion of outdated WAL files in RocksDB engine

* make use of selectivity estimates in hash, skiplist and persistent indexes
  in RocksDB engine

* changed VM overcommit recommendation for user-friendliness

* fix a shutdown bug in the cluster: a destroyed query could still be active

* do not terminate the entire server process if a temp file cannot be created
  (Windows only)

* fix log output in the front-end, it stopped in case of too many messages


v3.2.beta3 (2017-06-27)
-----------------------

* numerous bugfixes


v3.2.beta2 (2017-06-20)
-----------------------

* potentially fixed issue #2559: Duplicate _key generated on insertion

* fix invalid results (too many) when a skipping LIMIT was used for a
  traversal. `LIMIT x` or `LIMIT 0, x` were not affected, but `LIMIT s, x`
  may have returned too many results

* fix races in SSL communication code

* fix invalid locking in JWT authentication cache, which could have
  crashed the server

* fix invalid first group results for sorted AQL COLLECT when LIMIT
  was used

* fix potential race, which could make arangod hang on startup

* removed `exception` field from transaction error result; users should throw
  explicit `Error` instances to return custom exceptions (addresses issue #2561)

* fixed issue #2613: Reduce log level when Foxx manager tries to self heal missing database

* add a read only mode for users and collection level authorization

* removed `exception` field from transaction error result; users should throw
  explicit `Error` instances to return custom exceptions (addresses issue #2561)

* fixed issue #2677: Foxx disabling development mode creates non-deterministic service bundle

* fixed issue #2684: Legacy service UI not working


v3.2.beta1 (2017-06-12)
-----------------------

* provide more context for index errors (addresses issue #342)

* arangod now validates several OS/environment settings on startup and warns if
  the settings are non-ideal. Most of the checks are executed on Linux systems only.

* fixed issue #2515: The replace-or-with-in optimization rule might prevent use of indexes

* added `REGEX_REPLACE` AQL function

* the RocksDB storage format was changed, users of the previous alpha versions
  must delete the database directory and re-import their data

* added server startup option `--query.fail-on-warning`

  setting this option to `true` will abort any AQL query with an exception if
  it causes a warning at runtime. The value can be overridden per query by
  setting the `failOnWarning` attribute in a query's options.

* added --rocksdb.num-uncompressed-levels to adjust number of non-compressed levels

* added checks for memory managment and warn (i. e. if hugepages are enabled)

* set default SSL cipher suite string to "HIGH:!EXPORT:!aNULL@STRENGTH"

* fixed issue #2469: Authentication = true does not protect foxx-routes

* fixed issue #2459: compile success but can not run with rocksdb

* `--server.maximal-queue-size` is now an absolute maximum. If the queue is
  full, then 503 is returned. Setting it to 0 means "no limit".

* (Enterprise Edition only) added authentication against an LDAP server

* fixed issue #2083: Foxx services aren't distributed to all coordinators

* fixed issue #2384: new coordinators don't pick up existing Foxx services

* fixed issue #2408: Foxx service validation causes unintended side-effects

* extended HTTP API with routes for managing Foxx services

* added distinction between hasUser and authorized within Foxx
  (cluster internal requests are authorized requests but don't have a user)

* arangoimp now has a `--threads` option to enable parallel imports of data

* PR #2514: Foxx services that can't be fixed by self-healing now serve a 503 error

* added `time` function to `@arangodb` module


v3.2.alpha4 (2017-04-25)
------------------------

* fixed issue #2450: Bad optimization plan on simple query

* fixed issue #2448: ArangoDB Web UI takes no action when Delete button is clicked

* fixed issue #2442: Frontend shows already deleted databases during login

* added 'x-content-type-options: nosniff' to avoid MSIE bug

* set default value for `--ssl.protocol` from TLSv1 to TLSv1.2.

* AQL breaking change in cluster:
  The SHORTEST_PATH statement using edge-collection names instead
  of a graph name now requires to explicitly name the vertex-collection names
  within the AQL query in the cluster. It can be done by adding `WITH <name>`
  at the beginning of the query.

  Example:
  ```
  FOR v,e IN OUTBOUND SHORTEST_PATH @start TO @target edges [...]
  ```

  Now has to be:

  ```
  WITH vertices
  FOR v,e IN OUTBOUND SHORTEST_PATH @start TO @target edges [...]
  ```

  This change is due to avoid dead-lock sitations in clustered case.
  An error stating the above is included.

* add implicit use of geo indexes when using SORT/FILTER in AQL, without
  the need to use the special-purpose geo AQL functions `NEAR` or `WITHIN`.

  the special purpose `NEAR` AQL function can now be substituted with the
  following AQL (provided there is a geo index present on the `doc.latitude`
  and `doc.longitude` attributes):

      FOR doc in geoSort
        SORT DISTANCE(doc.latitude, doc.longitude, 0, 0)
        LIMIT 5
        RETURN doc

  `WITHIN` can be substituted with the following AQL:

      FOR doc in geoFilter
        FILTER DISTANCE(doc.latitude, doc.longitude, 0, 0) < 2000
        RETURN doc

  Compared to using the special purpose AQL functions this approach has the
  advantage that it is more composable, and will also honor any `LIMIT` values
  used in the AQL query.

* potential fix for shutdown hangs on OSX

* added KB, MB, GB prefix for integer parameters, % for integer parameters
  with a base value

* added JEMALLOC 4.5.0

* added `--vm.resident-limit` and `--vm.path` for file-backed memory mapping
  after reaching a configurable maximum RAM size

* try recommended limit for file descriptors in case of unlimited
  hard limit

* issue #2413: improve logging in case of lock timeout and deadlocks

* added log topic attribute to /_admin/log api

* removed internal build option `USE_DEV_TIMERS`

  Enabling this option activated some proprietary timers for only selected
  events in arangod. Instead better use `perf` to gather timings.


v3.2.alpha3 (2017-03-22)
------------------------

* increase default collection lock timeout from 30 to 900 seconds

* added function `db._engine()` for retrieval of storage engine information at
  server runtime

  There is also an HTTP REST handler at GET /_api/engine that returns engine
  information.

* require at least cmake 3.2 for building ArangoDB

* make arangod start with less V8 JavaScript contexts

  This speeds up the server start (a little bit) and makes it use less memory.
  Whenever a V8 context is needed by a Foxx action or some other operation and
  there is no usable V8 context, a new one will be created dynamically now.

  Up to `--javascript.v8-contexts` V8 contexts will be created, so this option
  will change its meaning. Previously as many V8 contexts as specified by this
  option were created at server start, and the number of V8 contexts did not
  change at runtime. Now up to this number of V8 contexts will be in use at the
  same time, but the actual number of V8 contexts is dynamic.

  The garbage collector thread will automatically delete unused V8 contexts after
  a while. The number of spare contexts will go down to as few as configured in
  the new option `--javascript.v8-contexts-minimum`. Actually that many V8 contexts
  are also created at server start.

  The first few requests in new V8 contexts will take longer than in contexts
  that have been there already. Performance may therefore suffer a bit for the
  initial requests sent to ArangoDB or when there are only few but performance-
  critical situations in which new V8 contexts will be created. If this is a
  concern, it can easily be fixed by setting `--javascipt.v8-contexts-minimum`
  and `--javascript.v8-contexts` to a relatively high value, which will guarantee
  that many number of V8 contexts to be created at startup and kept around even
  when unused.

  Waiting for an unused V8 context will now also abort if no V8 context can be
  acquired/created after 120 seconds.

* improved diagnostic messages written to logfiles by supervisor process

* fixed issue #2367

* added "bindVars" to attributes of currently running and slow queries

* added "jsonl" as input file type for arangoimp

* upgraded version of bundled zlib library from 1.2.8 to 1.2.11

* added input file type `auto` for arangoimp so it can automatically detect the
  type of the input file from the filename extension

* fixed variables parsing in GraphQL

* added `--translate` option for arangoimp to translate attribute names from
  the input files to attriubte names expected by ArangoDB

  The `--translate` option can be specified multiple times (once per translation
  to be executed). The following example renames the "id" column from the input
  file to "_key", and the "from" column to "_from", and the "to" column to "_to":

      arangoimp --type csv --file data.csv --translate "id=_key" --translate "from=_from" --translate "to=_to"

  `--translate` works for CSV and TSV inputs only.

* changed default value for `--server.max-packet-size` from 128 MB to 256 MB

* fixed issue #2350

* fixed issue #2349

* fixed issue #2346

* fixed issue #2342

* change default string truncation length from 80 characters to 256 characters for
  `print`/`printShell` functions in ArangoShell and arangod. This will emit longer
  prefixes of string values before truncating them with `...`, which is helpful
  for debugging.

* always validate incoming JSON HTTP requests for duplicate attribute names

  Incoming JSON data with duplicate attribute names will now be rejected as
  invalid. Previous versions of ArangoDB only validated the uniqueness of
  attribute names inside incoming JSON for some API endpoints, but not
  consistently for all APIs.

* don't let read-only transactions block the WAL collector

* allow passing own `graphql-sync` module instance to Foxx GraphQL router

* arangoexport can now export to csv format

* arangoimp: fixed issue #2214

* Foxx: automatically add CORS response headers

* added "OPTIONS" to CORS `access-control-allow-methods` header

* Foxx: Fix arangoUser sometimes not being set correctly

* fixed issue #1974


v3.2.alpha2 (2017-02-20)
------------------------

* ui: fixed issue #2065

* ui: fixed a dashboard related memory issue

* Internal javascript rest actions will now hide their stack traces to the client
  unless maintainer mode is activated. Instead they will always log to the logfile

* Removed undocumented internal HTTP API:
  * PUT _api/edges

  The documented GET _api/edges and the undocumented POST _api/edges remains unmodified.

* updated V8 version to 5.7.0.0

* change undocumented behavior in case of invalid revision ids in
  If-Match and If-None-Match headers from 400 (BAD) to 412 (PRECONDITION
  FAILED).

* change undocumented behavior in case of invalid revision ids in
  JavaScript document operations from 1239 ("illegal document revision")
  to 1200 ("conflict").

* added data export tool, arangoexport.

  arangoexport can be used to export collections to json, jsonl or xml
  and export a graph or collections to xgmml.

* fixed a race condition when closing a connection

* raised default hard limit on threads for very small to 64

* fixed negative counting of http connection in UI


v3.2.alpha1 (2017-02-05)
------------------------

* added figure `httpRequests` to AQL query statistics

* removed revisions cache intermediate layer implementation

* obsoleted startup options `--database.revision-cache-chunk-size` and
  `--database.revision-cache-target-size`

* fix potential port number over-/underruns

* added startup option `--log.shorten-filenames` for controlling whether filenames
  in log messages should be shortened to just the filename with the absolute path

* removed IndexThreadFeature, made `--database.index-threads` option obsolete

* changed index filling to make it more parallel, dispatch tasks to boost::asio

* more detailed stacktraces in Foxx apps

* generated Foxx services now use swagger tags


v3.1.24 (XXXX-XX-XX)
--------------------

* fixed one more LIMIT issue in traversals


v3.1.23 (2017-06-19)
--------------------

* potentially fixed issue #2559: Duplicate _key generated on insertion

* fix races in SSL communication code

* fix invalid results (too many) when a skipping LIMIT was used for a
  traversal. `LIMIT x` or `LIMIT 0, x` were not affected, but `LIMIT s, x`
  may have returned too many results

* fix invalid first group results for sorted AQL COLLECT when LIMIT
  was used

* fix invalid locking in JWT authentication cache, which could have
  crashed the server

* fix undefined behavior in traverser when traversals were used inside
  a FOR loop


v3.1.22 (2017-06-07)
--------------------

* fixed issue #2505: Problem with export + report of a bug

* documented changed behavior of WITH

* fixed ui glitch in aardvark

* avoid agency compaction bug

* fixed issue #2283: disabled proxy communication internally


v3.1.21 (2017-05-22)
--------------------

* fixed issue #2488:  AQL operator IN error when data use base64 chars

* more randomness in seeding RNG

v3.1.20 (2016-05-16)
--------------------

* fixed incorrect sorting for distributeShardsLike

* improve reliability of AgencyComm communication with Agency

* fixed shard numbering bug, where ids were erouneously incremented by 1

* remove an unnecessary precondition in createCollectionCoordinator

* funny fail rotation fix

* fix in SimpleHttpClient for correct advancement of readBufferOffset

* forward SIG_HUP in supervisor process to the server process to fix logrotaion
  You need to stop the remaining arangod server process manually for the upgrade to work.


v3.1.19 (2017-04-28)
--------------------

* Fixed a StackOverflow issue in Traversal and ShortestPath. Occured if many (>1000) input
  values in a row do not return any result. Fixes issue: #2445

* fixed issue #2448

* fixed issue #2442

* added 'x-content-type-options: nosniff' to avoid MSIE bug

* fixed issue #2441

* fixed issue #2440

* Fixed a StackOverflow issue in Traversal and ShortestPath. Occured if many (>1000) input
  values in a row do not return any result. Fixes issue: #2445

* fix occasional hanging shutdowns on OS X


v3.1.18 (2017-04-18)
--------------------

* fixed error in continuous synchronization of collections

* fixed spurious hangs on server shutdown

* better error messages during restore collection

* completely overhaul supervision. More detailed tests

* Fixed a dead-lock situation in cluster traversers, it could happen in
  rare cases if the computation on one DBServer could be completed much earlier
  than the other server. It could also be restricted to SmartGraphs only.

* (Enterprise Edition only) Fixed a bug in SmartGraph DepthFirstSearch. In some
  more complicated queries, the maxDepth limit of 1 was not considered strictly
  enough, causing the traverser to do unlimited depth searches.

* fixed issue #2415

* fixed issue #2422

* fixed issue #1974


v3.1.17 (2017-04-04)
--------------------

* (Enterprise Edition only) fixed a bug where replicationFactor was not correctly
  forwarded in SmartGraph creation.

* fixed issue #2404

* fixed issue #2397

* ui - fixed smart graph option not appearing

* fixed issue #2389

* fixed issue #2400


v3.1.16 (2017-03-27)
--------------------

* fixed issue #2392

* try to raise file descriptors to at least 8192, warn otherwise

* ui - aql editor improvements + updated ace editor version (memory leak)

* fixed lost HTTP requests

* ui - fixed some event issues

* avoid name resolution when given connection string is a valid ip address

* helps with issue #1842, bug in COLLECT statement in connection with LIMIT.

* fix locking bug in cluster traversals

* increase lock timeout defaults

* increase various cluster timeouts

* limit default target size for revision cache to 1GB, which is better for
  tight RAM situations (used to be 40% of (totalRAM - 1GB), use
  --database.revision-cache-target-size <VALUEINBYTES> to get back the
  old behavior

* fixed a bug with restarted servers indicating status as "STARTUP"
  rather that "SERVING" in Nodes UI.


v3.1.15 (2017-03-20)
--------------------

* add logrotate configuration as requested in #2355

* fixed issue #2376

* ui - changed document api due a chrome bug

* ui - fixed a submenu bug

* added endpoint /_api/cluster/endpoints in cluster case to get all
  coordinator endpoints

* fix documentation of /_api/endpoint, declaring this API obsolete.

* Foxx response objects now have a `type` method for manipulating the content-type header

* Foxx tests now support `xunit` and `tap` reporters


v3.1.14 (2017-03-13)
--------------------

* ui - added feature request (multiple start nodes within graph viewer) #2317

* added missing locks to authentication cache methods

* ui - added feature request (multiple start nodes within graph viewer) #2317

* ui - fixed wrong merge of statistics infor
